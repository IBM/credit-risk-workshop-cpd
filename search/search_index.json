{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WARNING: This repository is no longer maintained \u00b6 This repository does not have active maintainers. Pull requests for fixes and enhancements will still be accepted, but no active work will be done on this workshop. This Workshop uses Cloud Pak for Data version 3.5 Analyzing Credit Risk with Cloud Pak for Data on OpenShift \u00b6 Welcome to our workshop! In this workshop we'll be using the Cloud Pak for Data platform to Collect Data, Organize Data, Analyze Data, and Infuse AI into our applications. The goals of this workshop are: Collect and virtualize data Visualize data with Data Refinery Create and deploy a machine learning model Monitor the model Create a Python app to use the model About this workshop \u00b6 Agenda Compatability About Cloud Pak for Data Credits About the data set \u00b6 In this workshop we will be using a credit risk / lending scenario. In this scenario, lenders respond to an increased pressure to expand lending to larger and more diverse audiences, by using different approaches to risk modeling. This means going beyond traditional credit data sources to alternative credit sources (i.e. mobile phone plan payment histories, education, etc), which may introduce risk of bias or other unexpected correlations. The credit risk model that we are exploring in this workshop uses a training data set that contains 20 attributes about each loan applicant. The scenario and model use synthetic data based on the UCI German Credit dataset . The data is split into three CSV files and are located in the data directory of the GitHub repository you will download in the pre-work section. Applicant Financial Data \u00b6 This file has the following attributes: CUSTOMERID (hex number, used as Primary Key) CHECKINGSTATUS CREDITHISTORY EXISTINGSAVINGS INSTALLMENTPLANS EXISTINGCREDITSCOUNT Applicant Loan Data \u00b6 This file has the following attributes: CUSTOMERID LOANDURATION LOANPURPOSE LOANAMOUNT INSTALLMENTPERCENT OTHERSONLOAN RISK Applicant Personal Data \u00b6 This file has the following attributes: CUSTOMERID EMPLOYMENTDURATION SEX CURRENTRESIDENCEDURATION OWNSPROPERTY AGE HOUSING JOB DEPENDENTS TELEPHONE FOREIGNWORKER FIRSTNAME LASTNAME EMAIL STREETADDRESS CITY STATE POSTALCODE Agenda \u00b6 00:05 Welcome Welcome to the Cloud Pak for Data workshop 00:20 Lecture - Intro and Overview Introduction to Cloud Pak for Data and an Overview of this workshop 00:20 Lab - Pre-work Clone or Download the repo, create a project, create a deployment space 00:10 Walkthrough - Pre-work Clone or Download the repo, create a project, create a deployment space 00:20 Lecture - Data Refinery and Data Virtualization Data Refinery and Data Virtualization 00:30 Lab - Data Connection and Virtualization and importing the data into the project Creating a new connection, virtualizing the data, importing the data into the project 00:10 Walkthrough - Data Connection and Virtualization Creating a new connection, virtualizing the data, importing the data into the project 00:15 Lab - Import Data into Project Importing data in your projects 00:05 Walkthrough - Import Data into Project Importing data in your projects 00:15 Lab - Data Visualization with Data Refinery Refining the data, visualizing and profiling the data 00:10 Walkthrough - Data Visualization with Data Refinery Refining the data, visualizing and profiling the data 00:15 Lecture - Watson Knowledge Catalog Enterprise governance with Watson Knowledge Catalog 00:20 Lab - Enterprise data governance for Viewers using Watson Knowledge Catalog Use and Enterprise data catalog to search, manage, and protect data 00:05 Walkthrough - Enterprise data governance for Viewers using Watson Knowledge Catalog Use and Enterprise data catalog to search, manage, and protect data 00:20 Lab - Enterprise data governance for Admins using Watson Knowledge Catalog Create new Categories, Business terms, Policies and Rules in Watson Knowledge Catalog 00:05 Walkthrough - Enterprise data governance for Admins using Watson Knowledge Catalog Create new Categories, Business terms, Policies and Rules in Watson Knowledge Catalog 00:15 Lecture - Machine Learning Machine Learning and Deep Learning concepts 00:20 Lab - Machine Learning with Jupyter Building a model with Spark, deploying the model with Watson Machine Learning, testing the model with a Python Flask app 00:10 Walkthrough - Machine Learning with Jupyter Building a model with Spark, deploying the model with Watson Machine Learning, testing the model with a Python Flask app 00:20 Lab - AutoAI - Machine Learning with AutoAI Use AutoAi to quickly generate a Machine Learning pipeline and model 00:10 Walkthrough - Machine Learning with AutoAI Use AutoAi to quickly generate a Machine Learning pipeline and model 00:30 Lab - Deploy and Test Machine Learning Models Deploy and machine learning models using several approaches 00:10 Walkthrough - Deploy and Test Machine Learning Models Deploy and machine learning models using several approaches 00:15 Lab - Monitoring models with OpenScale GUI (Auto setup Monitoring) Quickly deploy an OpenScale demo with Auto setup 00:10 Walkthrough - Monitoring models with OpenScale GUI (Auto setup Monitoring) Quickly deploy an OpenScale demo with Auto setup 00:30 Lab - Monitoring models with OpenScale (Notebook) See the OpenScale APIs in a Jupyter notebook and manually configure the monitors 00:10 Walkthrough - Monitoring models with OpenScale (Notebook) See the OpenScale APIs in a Jupyter notebook and manually configure the monitors 00:10 Closing Other capabilities, review, and next steps Compatability \u00b6 This workshop has been tested on the following platforms: macOS : Mojave (10.14), Catalina (10.15) Google Chrome version 81 Microsoft : Windows 10 Google Chrome, Microsoft Edge","title":"About the workshop"},{"location":"#warning-this-repository-is-no-longer-maintained","text":"This repository does not have active maintainers. Pull requests for fixes and enhancements will still be accepted, but no active work will be done on this workshop. This Workshop uses Cloud Pak for Data version 3.5","title":"WARNING: This repository is no longer maintained"},{"location":"#analyzing-credit-risk-with-cloud-pak-for-data-on-openshift","text":"Welcome to our workshop! In this workshop we'll be using the Cloud Pak for Data platform to Collect Data, Organize Data, Analyze Data, and Infuse AI into our applications. The goals of this workshop are: Collect and virtualize data Visualize data with Data Refinery Create and deploy a machine learning model Monitor the model Create a Python app to use the model","title":"Analyzing Credit Risk with Cloud Pak for Data on OpenShift"},{"location":"#about-this-workshop","text":"Agenda Compatability About Cloud Pak for Data Credits","title":"About this workshop"},{"location":"#about-the-data-set","text":"In this workshop we will be using a credit risk / lending scenario. In this scenario, lenders respond to an increased pressure to expand lending to larger and more diverse audiences, by using different approaches to risk modeling. This means going beyond traditional credit data sources to alternative credit sources (i.e. mobile phone plan payment histories, education, etc), which may introduce risk of bias or other unexpected correlations. The credit risk model that we are exploring in this workshop uses a training data set that contains 20 attributes about each loan applicant. The scenario and model use synthetic data based on the UCI German Credit dataset . The data is split into three CSV files and are located in the data directory of the GitHub repository you will download in the pre-work section.","title":"About the data set"},{"location":"#applicant-financial-data","text":"This file has the following attributes: CUSTOMERID (hex number, used as Primary Key) CHECKINGSTATUS CREDITHISTORY EXISTINGSAVINGS INSTALLMENTPLANS EXISTINGCREDITSCOUNT","title":"Applicant Financial Data"},{"location":"#applicant-loan-data","text":"This file has the following attributes: CUSTOMERID LOANDURATION LOANPURPOSE LOANAMOUNT INSTALLMENTPERCENT OTHERSONLOAN RISK","title":"Applicant Loan Data"},{"location":"#applicant-personal-data","text":"This file has the following attributes: CUSTOMERID EMPLOYMENTDURATION SEX CURRENTRESIDENCEDURATION OWNSPROPERTY AGE HOUSING JOB DEPENDENTS TELEPHONE FOREIGNWORKER FIRSTNAME LASTNAME EMAIL STREETADDRESS CITY STATE POSTALCODE","title":"Applicant Personal Data"},{"location":"#agenda","text":"00:05 Welcome Welcome to the Cloud Pak for Data workshop 00:20 Lecture - Intro and Overview Introduction to Cloud Pak for Data and an Overview of this workshop 00:20 Lab - Pre-work Clone or Download the repo, create a project, create a deployment space 00:10 Walkthrough - Pre-work Clone or Download the repo, create a project, create a deployment space 00:20 Lecture - Data Refinery and Data Virtualization Data Refinery and Data Virtualization 00:30 Lab - Data Connection and Virtualization and importing the data into the project Creating a new connection, virtualizing the data, importing the data into the project 00:10 Walkthrough - Data Connection and Virtualization Creating a new connection, virtualizing the data, importing the data into the project 00:15 Lab - Import Data into Project Importing data in your projects 00:05 Walkthrough - Import Data into Project Importing data in your projects 00:15 Lab - Data Visualization with Data Refinery Refining the data, visualizing and profiling the data 00:10 Walkthrough - Data Visualization with Data Refinery Refining the data, visualizing and profiling the data 00:15 Lecture - Watson Knowledge Catalog Enterprise governance with Watson Knowledge Catalog 00:20 Lab - Enterprise data governance for Viewers using Watson Knowledge Catalog Use and Enterprise data catalog to search, manage, and protect data 00:05 Walkthrough - Enterprise data governance for Viewers using Watson Knowledge Catalog Use and Enterprise data catalog to search, manage, and protect data 00:20 Lab - Enterprise data governance for Admins using Watson Knowledge Catalog Create new Categories, Business terms, Policies and Rules in Watson Knowledge Catalog 00:05 Walkthrough - Enterprise data governance for Admins using Watson Knowledge Catalog Create new Categories, Business terms, Policies and Rules in Watson Knowledge Catalog 00:15 Lecture - Machine Learning Machine Learning and Deep Learning concepts 00:20 Lab - Machine Learning with Jupyter Building a model with Spark, deploying the model with Watson Machine Learning, testing the model with a Python Flask app 00:10 Walkthrough - Machine Learning with Jupyter Building a model with Spark, deploying the model with Watson Machine Learning, testing the model with a Python Flask app 00:20 Lab - AutoAI - Machine Learning with AutoAI Use AutoAi to quickly generate a Machine Learning pipeline and model 00:10 Walkthrough - Machine Learning with AutoAI Use AutoAi to quickly generate a Machine Learning pipeline and model 00:30 Lab - Deploy and Test Machine Learning Models Deploy and machine learning models using several approaches 00:10 Walkthrough - Deploy and Test Machine Learning Models Deploy and machine learning models using several approaches 00:15 Lab - Monitoring models with OpenScale GUI (Auto setup Monitoring) Quickly deploy an OpenScale demo with Auto setup 00:10 Walkthrough - Monitoring models with OpenScale GUI (Auto setup Monitoring) Quickly deploy an OpenScale demo with Auto setup 00:30 Lab - Monitoring models with OpenScale (Notebook) See the OpenScale APIs in a Jupyter notebook and manually configure the monitors 00:10 Walkthrough - Monitoring models with OpenScale (Notebook) See the OpenScale APIs in a Jupyter notebook and manually configure the monitors 00:10 Closing Other capabilities, review, and next steps","title":"Agenda"},{"location":"#compatability","text":"This workshop has been tested on the following platforms: macOS : Mojave (10.14), Catalina (10.15) Google Chrome version 81 Microsoft : Windows 10 Google Chrome, Microsoft Edge","title":"Compatability"},{"location":"SUMMARY/","text":"WARNING: This repository is no longer maintained \u00b6 This repository does not have active maintainers. Pull requests for fixes and enhancements will still be accepted, but no active work will be done on this workshop. This Workshop uses Cloud Pak for Data version 3.5 Summary \u00b6 Getting Started \u00b6 Pre-work Credit Risk Workshop \u00b6 Data Connection and Virtualization Import Data to Project Data Visualization with Data Refinery Enterprise data governance for Viewers using Watson Knowledge Catalog Enterprise data governance for Admins using Watson Knowledge Catalog Machine Learning with Jupyter Machine Learning with AutoAI Deploy and Test Machine Learning Models Monitoring models with OpenScale GUI (Auto setup Monitoring) Monitoring models with OpenScale (Notebook) Workshop Resources \u00b6 FAQs / Tips Resources \u00b6 IBM Cloud Pak for Data - Information and Trial IBM Cloud Pak for Data - Knowledge Center IBM Cloud Pak for Data - Platform API IBM Cloud Pak for Data - Community Watson Knowledge Catalog Watson Knowledge Catalog Learning Center IBM Developer IBM Developer - Cloud Pak for Data IBM Garage Architecture - Data","title":"WARNING: This repository is no longer maintained :warning:"},{"location":"SUMMARY/#warning-this-repository-is-no-longer-maintained","text":"This repository does not have active maintainers. Pull requests for fixes and enhancements will still be accepted, but no active work will be done on this workshop. This Workshop uses Cloud Pak for Data version 3.5","title":"WARNING: This repository is no longer maintained"},{"location":"SUMMARY/#summary","text":"","title":"Summary"},{"location":"SUMMARY/#getting-started","text":"Pre-work","title":"Getting Started"},{"location":"SUMMARY/#credit-risk-workshop","text":"Data Connection and Virtualization Import Data to Project Data Visualization with Data Refinery Enterprise data governance for Viewers using Watson Knowledge Catalog Enterprise data governance for Admins using Watson Knowledge Catalog Machine Learning with Jupyter Machine Learning with AutoAI Deploy and Test Machine Learning Models Monitoring models with OpenScale GUI (Auto setup Monitoring) Monitoring models with OpenScale (Notebook)","title":"Credit Risk Workshop"},{"location":"SUMMARY/#workshop-resources","text":"FAQs / Tips","title":"Workshop Resources"},{"location":"SUMMARY/#resources","text":"IBM Cloud Pak for Data - Information and Trial IBM Cloud Pak for Data - Knowledge Center IBM Cloud Pak for Data - Platform API IBM Cloud Pak for Data - Community Watson Knowledge Catalog Watson Knowledge Catalog Learning Center IBM Developer IBM Developer - Cloud Pak for Data IBM Garage Architecture - Data","title":"Resources"},{"location":"addData/","text":"Importing data in your projects \u00b6 There are many ways to bring your data into your project, in this section we'll cover using from the following data sources: Using virtualized data Note: The lab instructions below assume you have your project already available. If not, follow the instructions in the pre-work section to create a project. Importing Virtualized Data \u00b6 For this section we'll explore the data that is available from the virtualized views that have been created in Data Virtualization. This data may come from various data sources or tables within a single source, but will appear as a single data asset. We will add this data to an analytics project so that it can be used in subsequent modules of this workshop (i.e for data analysis and to build ML models). Assign the data to your project \u00b6 To launch the data virtualization tool, go the (\u2630) navigation menu and under the Data section click on Data virtualization . From the data virtualization sub-menu at the top left of the page, click on the menu drop down list and choose My virtualized data . Here you should see the data you have virtualized or that you have been given access to (or that the administrator has assigned to you). Select the checkbox next to the data sets you want to use in your project and click the Assign button to start importing it to your project. Note: The name of the data assets to select may vary based on names chosen during data virtualization. The default names to select are: LOANS, APPLICANTFINANCIALDATA, APPLICANTPERSONALDATA, APPLICANTFINANCIALPERSONALDATA AND APPLICANTFINANCIALPERSONALLOANDATA In the 'Assign virtual objects' screen, choose your analytics project from the drop down list. Then click the Assign button to add the data to your project. In the Publish virtual assets to catalog pop up panel, select the Publish button to publish these assets to the catalog. In the pop up panel, you will receive a confirmation that the objects have been assigned to your project. Click the Go to project button. In the project page, clicking on the Assets tab will show the virtualized tables and joined tables that are now in your project (along with other assets that are in the project). Do not go to the next section until you see the data assets in your project. Conclusion \u00b6 This lab shows just one of the ways to gather data for your analytics projects in Cloud Pak for Data. In this case you used data that was previously virtualized. Other ways might include: browsing the catalogs, importing flat files, or importing data from connections directly in the project.","title":"Import Data to Project"},{"location":"addData/#importing-data-in-your-projects","text":"There are many ways to bring your data into your project, in this section we'll cover using from the following data sources: Using virtualized data Note: The lab instructions below assume you have your project already available. If not, follow the instructions in the pre-work section to create a project.","title":"Importing data in your projects"},{"location":"addData/#importing-virtualized-data","text":"For this section we'll explore the data that is available from the virtualized views that have been created in Data Virtualization. This data may come from various data sources or tables within a single source, but will appear as a single data asset. We will add this data to an analytics project so that it can be used in subsequent modules of this workshop (i.e for data analysis and to build ML models).","title":"Importing Virtualized Data"},{"location":"addData/#assign-the-data-to-your-project","text":"To launch the data virtualization tool, go the (\u2630) navigation menu and under the Data section click on Data virtualization . From the data virtualization sub-menu at the top left of the page, click on the menu drop down list and choose My virtualized data . Here you should see the data you have virtualized or that you have been given access to (or that the administrator has assigned to you). Select the checkbox next to the data sets you want to use in your project and click the Assign button to start importing it to your project. Note: The name of the data assets to select may vary based on names chosen during data virtualization. The default names to select are: LOANS, APPLICANTFINANCIALDATA, APPLICANTPERSONALDATA, APPLICANTFINANCIALPERSONALDATA AND APPLICANTFINANCIALPERSONALLOANDATA In the 'Assign virtual objects' screen, choose your analytics project from the drop down list. Then click the Assign button to add the data to your project. In the Publish virtual assets to catalog pop up panel, select the Publish button to publish these assets to the catalog. In the pop up panel, you will receive a confirmation that the objects have been assigned to your project. Click the Go to project button. In the project page, clicking on the Assets tab will show the virtualized tables and joined tables that are now in your project (along with other assets that are in the project). Do not go to the next section until you see the data assets in your project.","title":"Assign the data to your project"},{"location":"addData/#conclusion","text":"This lab shows just one of the ways to gather data for your analytics projects in Cloud Pak for Data. In this case you used data that was previously virtualized. Other ways might include: browsing the catalogs, importing flat files, or importing data from connections directly in the project.","title":"Conclusion"},{"location":"data-connection-and-virtualization/","text":"Data Connection and Virtualization \u00b6 This section will cover aspects of collecting data in Cloud Pak for Data. Specifically we will be connecting to different data sources and creating views against those data sources to create a single unified set of data assets that can be used in other modules of this workshop. Note: To complete this section, an Admin or Data Engineer role needs to be assigned to your user account. The workshop instructor will assign this role as appropriate. Virtualizing Data \u00b6 In this section, we will gather data from several tables across data sources. We will use data virtualization to access these tables and then create joined views against those virtualized tables. Create Virtualized Tables \u00b6 To launch the data virtualization tool, go the (\u2630) navigation menu and under the Data section click on Data virtualization . From the Data virtualization sub-menu at the top left of the page, click on the menu drop down list and choose Virtualize . Several tables names will be displayed across any of the data sources that are included in the data virtualization server. You will notice that on the top of the panel, we can filter the tables being displayed by selecting the database type. To simplify the search for tables you will use, click on the Schemas column header to sort the tables by Schema. Then find the tables we will be using for this workshop: APPLICANTFINANCIALDATA , APPLICANTPERSONALDATA and LOANS , which are under the CP4DCREDIT schema. Select the checkboxes next to these three tables, and then click on Add to cart followed by the View Cart button. Note: You may need to page through the available tables by clicking on the right arrow at the bottom of the tables view. Or changing the number of Items per page to a larger value at the bottom left of the tables view. The next panel prompts you to select where to assign the virtualized tables. Select the My virtualized data radio button and click the Virtualize button to add the virtualized tables to your data (we left the default values, so the tables will be virtualized under your own user schema with the same table names as the original tables). A pop up dialog panel will indicate that the virtual tables have been created. Let's see the new virtualized tables by clicking the View my virtualized data button. Note: You may receive a notification at the top of the page that the virtual assets were published to the catalog. Feel free to dismiss the notification by clicking on the X Create Joined Virtual Views \u00b6 Now we're going to join the tables we previously virtualized, so we have a final merged set of data. It will be easier to do it here rather than in a notebook where we'd have to write code to handle three different data sets. From the 'My virtualized data' page, Click on two of the virtualized tables ( APPLICANTPERSONALDATA and APPLICANTFINANCIALDATA ) and click the Join button. To join the tables we need to pick a key that is common to both data sets. Here we choose to map CustomerID from the first table to CustomerID on the second table. Do this by clicking on one and dragging it to another. We will leave the default of having all the column names selected. When the line is drawn, click on the Next button. In the next panel, although we could change the names of our columns, we will accept the existing names for our columns. Click the Next button to continue. In the next panel we'll give our joined data view a unique name (to be consistent with SQL standards, pick an all uppercase name), choose someething like: XXXAPPLICANTFINANCIALPERSONALDATA (where XXX is your initials in all upper case ). Also select the My virtualized data radio button and then click the Create view button to add the virtualized aggregate view to your data. A pop up dialog panel will indicate that the join view creation has succeeded! Click on View my virutalized data button. Repeat the same steps as above, but this time choose to join the new joined view you just created ( XXXAPPLICANTFINANCIALPERSONALDATA ) and the last virtualized table ( LOANS ), to create a new joined view that has all three tables. Click the Join button. Again join the two tables by selecting/mapping the CustomerID from the first table to CustomerID on the second table. Do this by clicking on one and dragging it to another. We will leave the default of having all the column names selected. When the line is drawn, click on the Next button. In the next panel, although we could change the names of our columns, we will accept the existing names for our columns. Click the Next button to continue. In the next panel we'll give our joined data view a unique name (to be consistent with SQL standards, pick an all uppercase name), choose someething like: XXXAPPLICANTFINANCIALPERSONALLOANSDATA (where XXX is your initials in all upper case ). Also select the My virtualized data radio button and then click the Create view button to add the virtualized aggregate view to your data. A pop up dialog panel will indicate that the join view creation has succeeded! Click on View my virtualized data button. From the My virtualized data page you should now see all three virtualized tables and two joined tables. Do not go to the next section until you have all the tables. Conclusion \u00b6 In this section we learned how to make connection to databases that contain our data, how to virtualize them, and how to use the virtualized data. Remember that you can add data from different databases and servers if you need to. Moreover, you can virtualized these data from different sources together as well! The goal is to take care of bringing the data to the platform early on so all the data scientists can use it without reinventing the wheel while you keep full control of who has access to what data.","title":"Data Connection and Virtualization"},{"location":"data-connection-and-virtualization/#data-connection-and-virtualization","text":"This section will cover aspects of collecting data in Cloud Pak for Data. Specifically we will be connecting to different data sources and creating views against those data sources to create a single unified set of data assets that can be used in other modules of this workshop. Note: To complete this section, an Admin or Data Engineer role needs to be assigned to your user account. The workshop instructor will assign this role as appropriate.","title":"Data Connection and Virtualization"},{"location":"data-connection-and-virtualization/#virtualizing-data","text":"In this section, we will gather data from several tables across data sources. We will use data virtualization to access these tables and then create joined views against those virtualized tables.","title":"Virtualizing Data"},{"location":"data-connection-and-virtualization/#create-virtualized-tables","text":"To launch the data virtualization tool, go the (\u2630) navigation menu and under the Data section click on Data virtualization . From the Data virtualization sub-menu at the top left of the page, click on the menu drop down list and choose Virtualize . Several tables names will be displayed across any of the data sources that are included in the data virtualization server. You will notice that on the top of the panel, we can filter the tables being displayed by selecting the database type. To simplify the search for tables you will use, click on the Schemas column header to sort the tables by Schema. Then find the tables we will be using for this workshop: APPLICANTFINANCIALDATA , APPLICANTPERSONALDATA and LOANS , which are under the CP4DCREDIT schema. Select the checkboxes next to these three tables, and then click on Add to cart followed by the View Cart button. Note: You may need to page through the available tables by clicking on the right arrow at the bottom of the tables view. Or changing the number of Items per page to a larger value at the bottom left of the tables view. The next panel prompts you to select where to assign the virtualized tables. Select the My virtualized data radio button and click the Virtualize button to add the virtualized tables to your data (we left the default values, so the tables will be virtualized under your own user schema with the same table names as the original tables). A pop up dialog panel will indicate that the virtual tables have been created. Let's see the new virtualized tables by clicking the View my virtualized data button. Note: You may receive a notification at the top of the page that the virtual assets were published to the catalog. Feel free to dismiss the notification by clicking on the X","title":"Create Virtualized Tables"},{"location":"data-connection-and-virtualization/#create-joined-virtual-views","text":"Now we're going to join the tables we previously virtualized, so we have a final merged set of data. It will be easier to do it here rather than in a notebook where we'd have to write code to handle three different data sets. From the 'My virtualized data' page, Click on two of the virtualized tables ( APPLICANTPERSONALDATA and APPLICANTFINANCIALDATA ) and click the Join button. To join the tables we need to pick a key that is common to both data sets. Here we choose to map CustomerID from the first table to CustomerID on the second table. Do this by clicking on one and dragging it to another. We will leave the default of having all the column names selected. When the line is drawn, click on the Next button. In the next panel, although we could change the names of our columns, we will accept the existing names for our columns. Click the Next button to continue. In the next panel we'll give our joined data view a unique name (to be consistent with SQL standards, pick an all uppercase name), choose someething like: XXXAPPLICANTFINANCIALPERSONALDATA (where XXX is your initials in all upper case ). Also select the My virtualized data radio button and then click the Create view button to add the virtualized aggregate view to your data. A pop up dialog panel will indicate that the join view creation has succeeded! Click on View my virutalized data button. Repeat the same steps as above, but this time choose to join the new joined view you just created ( XXXAPPLICANTFINANCIALPERSONALDATA ) and the last virtualized table ( LOANS ), to create a new joined view that has all three tables. Click the Join button. Again join the two tables by selecting/mapping the CustomerID from the first table to CustomerID on the second table. Do this by clicking on one and dragging it to another. We will leave the default of having all the column names selected. When the line is drawn, click on the Next button. In the next panel, although we could change the names of our columns, we will accept the existing names for our columns. Click the Next button to continue. In the next panel we'll give our joined data view a unique name (to be consistent with SQL standards, pick an all uppercase name), choose someething like: XXXAPPLICANTFINANCIALPERSONALLOANSDATA (where XXX is your initials in all upper case ). Also select the My virtualized data radio button and then click the Create view button to add the virtualized aggregate view to your data. A pop up dialog panel will indicate that the join view creation has succeeded! Click on View my virtualized data button. From the My virtualized data page you should now see all three virtualized tables and two joined tables. Do not go to the next section until you have all the tables.","title":"Create Joined Virtual Views"},{"location":"data-connection-and-virtualization/#conclusion","text":"In this section we learned how to make connection to databases that contain our data, how to virtualize them, and how to use the virtualized data. Remember that you can add data from different databases and servers if you need to. Moreover, you can virtualized these data from different sources together as well! The goal is to take care of bringing the data to the platform early on so all the data scientists can use it without reinventing the wheel while you keep full control of who has access to what data.","title":"Conclusion"},{"location":"data-grant-access/","text":"Grant Access to Virtualized Data \u00b6 This section will continue exploring aspects of collecting data on Cloud Pak for Data. Specifically, there will be scenarios where a platform user (i.e a Data Engineer) will want to make data available to other users of the platform. This can be for any number of reasons. For this workshop, we will assume the scenario that one set of users know/understand the underlying data sources and will be responsible for creating virtualized views of the data needed. While a broader set of users will need to access the data (to assess, refine, etc). In this lab, we will make the data (views/tables/joins) that was virtualized in the Data Virtualization module, available to other users. In order for other users to have access to the data, you need to grant them access. Note: This section only needs to be completed if there are non-Admin or non-Data Engineer users you are working in a group with. The instructors would have indicated that it needs to be completed to give those users access to the data you have virtualized above. Grant access to the virtualized data \u00b6 To launch the data virtualization tool, go the (\u2630) navigation menu and under the Data section click on Data virtualization . From the Data virtualization sub-menu at the top left of the page, click on the menu drop down list and choose My virtualized data . For one of the virtualized data assets you've created, click the 3 vertical dots on the right and choose Manage access . Click the Specific users button and click the Add user button. In the popup dialog window, click the checkbox next to the user (or multiple users) you wish to grant access to and then click the Add users button. Repeat the above steps to give access to the remaining virtualized tables and views (all five that you created). Conclusion \u00b6 In this section we learned how to allow users to collaborate and make use of virtualized data, without needing to go through the virtualization process themselves.","title":"Grant Access to Virtualized Data"},{"location":"data-grant-access/#grant-access-to-virtualized-data","text":"This section will continue exploring aspects of collecting data on Cloud Pak for Data. Specifically, there will be scenarios where a platform user (i.e a Data Engineer) will want to make data available to other users of the platform. This can be for any number of reasons. For this workshop, we will assume the scenario that one set of users know/understand the underlying data sources and will be responsible for creating virtualized views of the data needed. While a broader set of users will need to access the data (to assess, refine, etc). In this lab, we will make the data (views/tables/joins) that was virtualized in the Data Virtualization module, available to other users. In order for other users to have access to the data, you need to grant them access. Note: This section only needs to be completed if there are non-Admin or non-Data Engineer users you are working in a group with. The instructors would have indicated that it needs to be completed to give those users access to the data you have virtualized above.","title":"Grant Access to Virtualized Data"},{"location":"data-grant-access/#grant-access-to-the-virtualized-data","text":"To launch the data virtualization tool, go the (\u2630) navigation menu and under the Data section click on Data virtualization . From the Data virtualization sub-menu at the top left of the page, click on the menu drop down list and choose My virtualized data . For one of the virtualized data assets you've created, click the 3 vertical dots on the right and choose Manage access . Click the Specific users button and click the Add user button. In the popup dialog window, click the checkbox next to the user (or multiple users) you wish to grant access to and then click the Add users button. Repeat the above steps to give access to the remaining virtualized tables and views (all five that you created).","title":"Grant access to the virtualized data"},{"location":"data-grant-access/#conclusion","text":"In this section we learned how to allow users to collaborate and make use of virtualized data, without needing to go through the virtualization process themselves.","title":"Conclusion"},{"location":"data-visualization-and-refinery/","text":"Data Visualization and Data Refinery \u00b6 Let's take a quick detour to the Data Refinery tool. Data Refinery can quickly filter and mutate data, create quick visualizations, and do other data cleansing tasks from an easy to use user interface. This section is broken up into the following steps: Load the data Refine the data Profile the data Visualize the data Note: The lab instructions below assume you have a project already and have data you will refine. If not, follow the instructions in the pre-work and import data to project sections to create a project and assign data to your project. 1. Load Data \u00b6 Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click the project name you created in the pre-work section. From the Project home, under the Assets tab, ensure the Data assets section is expanded or click on the arrow to toggle it and open up the list of data assets. Click the merged data data asset XXXAPPLICANTFINANCIALPERSONALLOANDATA (the name of the file may vary, XXX may be your initials or the initials of the person who granted you data access) to open it. If this is the first time you are opening the data asset, you will be asked to unlock the connection with your personal credentials. Click the Use your Cloud Pak for Data credentials to authenticate to the data source checkbox and then click the Connect button. Once the preview of the data asset opens, click on the Refine button on the top right of the table. Data Refinery will launch and open to the Data tab. It will also display the information panel with details of the data refinery flow and where the output of the flow will be placed. Go ahead and click the X to the right of the Information panel to close it. 2. Refine Data \u00b6 We'll start out in the Data tab where we wrangle, shape and refine our data. As you refine your data, IBM Data Refinery keeps track of the steps in your data flow. You can modify them and even select a step to return to a particular moment in your data\u2019s transformation. Create Transformation Flow \u00b6 With Data Refinery, we can transform our data by directly entering operations in R syntax or interactively by selecting operations from the menu. For example, start typing filter on the Command line and observe that the list of operations displayed will get updated. Click on the filter operation. A filter operation syntax will be displayed in the Command line. Clicking on the operation name within the Command line will give hints on the syntax and how to use the command. For instance, to filter for customers who have paid credits up to date, build the expression shown below. To inact the filter, you would Apply the expression. filter ( `CreditHistory` == 'credits_paid_to_date' ) We can remove this custom filter by clicking on the trash icon on the Custom code step of our data workflow. We will use the UI to explore and transform the data. Click the +Operation button. Let's use the Filter operation to check some values. Click on Filter in the left panel. We want to make sure that there are no empty values in the StreetAddress column. Select the StreetAddress column from the Column drop down list, select Is empty from the Operator drop down list, and then click the Apply button. Note: If there are records where the selected column is empty, they will be displayed after clicking the apply button. If there are no records for this filter, it means that the rows being sampled do not have any empty values for the selected column. Now, click on the counter-clockwise \"back\" arrow to remove the filter. Alternately, we can also remove the filter by clicking the trash icon for the Filter step in the Steps panel on the right. We can remove these records with empty values. Click the +Operation again and this time select the Remove empty rows operation. Select the StreetAddress column, then click the Next button and finally the Apply button. Let's say we've decide that there are columns that we don't want to leave in our dataset ( maybe because they might not be usefule features in our Machine Learning model, or because we don't want to make those data attributes accessible to others, or any other reason). We'll remove the FirstName , LastName , Email , StreetAddress , City , State , PostalCode columns. For each columnn to be removed: Click the +Operation button, then select the Remove operation. Click the Change column selection option. In the Select column drop down, choose one of the columns to remove (i.e FirstName ). Click the Next button and then the Apply button. The columns will be removed. Repeat for each of the above columns. At this point, you have a data transformation flow with 8 steps. As we saw in the last section, we keep track of each of the steps and we can even undo (or redo) an action using the circular arrows. To see the steps in the data flow that you have performed, click the Steps button. The operations that you have performed on the data will be shown. You can modify these steps in real time and save for future use. Schedule Jobs \u00b6 Data Refinery allows you to run jobs at scheduled times, and save the output. In this way, you can regularly refine new data as it is updated. Click on the \"jobs\" icon and then Save and create job option from the menu. Give the job a name and optional description, then click the Next button. The job will configure a default input and output data asset, as well as the runtime environment. Click the Next button. We can set the job to run on a schedule. For now, leave the schedule off and click the Next button. Click the Create and Run button to save and run this job. This refinery flow will be saved to your project in the Data Refinery flows section of the project overview page. From that section you could revisit the flow to edit the steps or even see any execution jobs you have run. For now, we will move on to exploring our data. 3. Profile Data \u00b6 Back on the top level of the data refinery view, click on the Profile tab to bring up a view of several statistics and histograms for the attributes in your data. Once the data profile loads, you can get insight into the data from the views and statistics: The median age of the applicants is 36, with the bulk under 49. About as many people had credits_paid_to_date as prior_payments_delayed. Few had no_credits. The median was 3 years for duration at current residence. Range was 1-6 years. 4. Visualize Data \u00b6 Let's do some visual exploration of our data using charts and graphs. Note that this is an exploratory phase and we're looking for insights in out data. We can accomplish this in Data Refinery interactively without coding. Choose the Visualizations tab to bring up the page where you can select columns that you want to visualize. Select LoanAmount from the \"Columns to visualize\" drop down list as the first column and click Add another column to add another column. Next add LoanDuration and click the Visualize data button. The system will pick a suggested plot for you based on your data and show more suggested plot types at the top. Remember that we are most interested in knowing how these features impact a loan being at the risk. So, let's add the Risk as a color on top of our current scatter plot. That should help us visually see if there's something of interest here. From the left panel, click the Color Map drop down and select Risk . Also, to see the full data, drag the right side of the data selector at the bottom all the way to the right, in order to show all the data inside your plot. We notice that there are more blue (risk) on this plot towards the top right, than there is on the bottom left. This is a good start as it shows that there is probably a relationship between the riskiness of a loan and its duration and amount. It appears that the higher the amount and duration, the riskier the loan. Interesting, let's dig in further in how the loan duration could play into the riskiness of a loan. Note: The colors used in your visualization may be different. Be sure to look at chart legend for clarification Let's plot a histogram of the LoanDuration to see if we can notice anything. First, select Histogram from the Chart Type . On the left, select LoanDuration for the 'X-axis', select Risk in the 'Split By' section, check the Stacked option, uncheck the Show kde curve toggle, uncheck the Show distribution curve toggle. You should see a chart that looks like the following image. It looks like the longer the duration the larger the blue bar (risky loan count) become and the smaller the dark blue bars (non risky loan count) become. That indicate loans with longer duration are in general more likely to be risky. However, we need more information. We next explore if there is some insight in terms of the riskiness of a loan based on its duration when broken down by the loan purpose. To do so, let's create a Heat Map plot. At the top of the page, in the Chart Type section, open the arrows on the right, select Heat Map . Next, select Risk in the column section and LoanPurpose for the Row section. Additionally, to see the effects of the loan duration, select Mean in the summary section, and select LoanDuration in the Value section. You can now see that the least risky loans are those taken out for purchasing a new car and they are on average 10 years long. To the left of that cell we see that loans taken out for the same purpose that average around 15 years for term length seem to be more risky. So one could conclude the longer the loan term is, the more likely it will be risky. In contrast, we can see that both risky and non-risky loans for the other category seem to have the same average term length, so one could conclude that there's little, if any, relationship between loan length and its riskiness for the loans of type other. In general, for each row, the bigger the color difference between the right and left column, the more likely that loan duration plays a role for the riskiness of the loan category. Now let's look into customizing our plot. Under the Actions panel, notice that you can perform tasks such as Start over , Download chart details , Download chart image , or set Global visualization preferences (Note: Hover over the icons to see the names) . Click on the drop down arrow next to Action . Then click on the Global visualization preferences option from the menu. We see that we can do things in the Global visualization preferences for Titles , Tools , Theme , and Notifications . Click on the Theme tab and update the color scheme to Dark . Then click the Apply button, now the colors for all of our charts will reflect this. Play around with various Themes and find one that you like. Conclusion \u00b6 We've seen a some of the capabilities of the Data Refinery. We saw how we can transform data using R code, as well as using various operations on the columns such as changing the data type, removing empty rows, or deleting the column altogether. We next saw that all the steps in our Data Flow are recorded, so we can remove steps, repeat them, or edit an individual step. We were able to quickly profile the data, to see histograms and statistics for each column. And finally we created more in-depth Visualizations, creating a scatter plot, histogram, and heatmap to explore the relationship between the riskiness of a loan and its duration, and purpose.","title":"Data Visualization with Data Refinery"},{"location":"data-visualization-and-refinery/#data-visualization-and-data-refinery","text":"Let's take a quick detour to the Data Refinery tool. Data Refinery can quickly filter and mutate data, create quick visualizations, and do other data cleansing tasks from an easy to use user interface. This section is broken up into the following steps: Load the data Refine the data Profile the data Visualize the data Note: The lab instructions below assume you have a project already and have data you will refine. If not, follow the instructions in the pre-work and import data to project sections to create a project and assign data to your project.","title":"Data Visualization and Data Refinery"},{"location":"data-visualization-and-refinery/#1-load-data","text":"Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click the project name you created in the pre-work section. From the Project home, under the Assets tab, ensure the Data assets section is expanded or click on the arrow to toggle it and open up the list of data assets. Click the merged data data asset XXXAPPLICANTFINANCIALPERSONALLOANDATA (the name of the file may vary, XXX may be your initials or the initials of the person who granted you data access) to open it. If this is the first time you are opening the data asset, you will be asked to unlock the connection with your personal credentials. Click the Use your Cloud Pak for Data credentials to authenticate to the data source checkbox and then click the Connect button. Once the preview of the data asset opens, click on the Refine button on the top right of the table. Data Refinery will launch and open to the Data tab. It will also display the information panel with details of the data refinery flow and where the output of the flow will be placed. Go ahead and click the X to the right of the Information panel to close it.","title":"1. Load Data"},{"location":"data-visualization-and-refinery/#2-refine-data","text":"We'll start out in the Data tab where we wrangle, shape and refine our data. As you refine your data, IBM Data Refinery keeps track of the steps in your data flow. You can modify them and even select a step to return to a particular moment in your data\u2019s transformation.","title":"2. Refine Data"},{"location":"data-visualization-and-refinery/#create-transformation-flow","text":"With Data Refinery, we can transform our data by directly entering operations in R syntax or interactively by selecting operations from the menu. For example, start typing filter on the Command line and observe that the list of operations displayed will get updated. Click on the filter operation. A filter operation syntax will be displayed in the Command line. Clicking on the operation name within the Command line will give hints on the syntax and how to use the command. For instance, to filter for customers who have paid credits up to date, build the expression shown below. To inact the filter, you would Apply the expression. filter ( `CreditHistory` == 'credits_paid_to_date' ) We can remove this custom filter by clicking on the trash icon on the Custom code step of our data workflow. We will use the UI to explore and transform the data. Click the +Operation button. Let's use the Filter operation to check some values. Click on Filter in the left panel. We want to make sure that there are no empty values in the StreetAddress column. Select the StreetAddress column from the Column drop down list, select Is empty from the Operator drop down list, and then click the Apply button. Note: If there are records where the selected column is empty, they will be displayed after clicking the apply button. If there are no records for this filter, it means that the rows being sampled do not have any empty values for the selected column. Now, click on the counter-clockwise \"back\" arrow to remove the filter. Alternately, we can also remove the filter by clicking the trash icon for the Filter step in the Steps panel on the right. We can remove these records with empty values. Click the +Operation again and this time select the Remove empty rows operation. Select the StreetAddress column, then click the Next button and finally the Apply button. Let's say we've decide that there are columns that we don't want to leave in our dataset ( maybe because they might not be usefule features in our Machine Learning model, or because we don't want to make those data attributes accessible to others, or any other reason). We'll remove the FirstName , LastName , Email , StreetAddress , City , State , PostalCode columns. For each columnn to be removed: Click the +Operation button, then select the Remove operation. Click the Change column selection option. In the Select column drop down, choose one of the columns to remove (i.e FirstName ). Click the Next button and then the Apply button. The columns will be removed. Repeat for each of the above columns. At this point, you have a data transformation flow with 8 steps. As we saw in the last section, we keep track of each of the steps and we can even undo (or redo) an action using the circular arrows. To see the steps in the data flow that you have performed, click the Steps button. The operations that you have performed on the data will be shown. You can modify these steps in real time and save for future use.","title":"Create Transformation Flow"},{"location":"data-visualization-and-refinery/#schedule-jobs","text":"Data Refinery allows you to run jobs at scheduled times, and save the output. In this way, you can regularly refine new data as it is updated. Click on the \"jobs\" icon and then Save and create job option from the menu. Give the job a name and optional description, then click the Next button. The job will configure a default input and output data asset, as well as the runtime environment. Click the Next button. We can set the job to run on a schedule. For now, leave the schedule off and click the Next button. Click the Create and Run button to save and run this job. This refinery flow will be saved to your project in the Data Refinery flows section of the project overview page. From that section you could revisit the flow to edit the steps or even see any execution jobs you have run. For now, we will move on to exploring our data.","title":"Schedule Jobs"},{"location":"data-visualization-and-refinery/#3-profile-data","text":"Back on the top level of the data refinery view, click on the Profile tab to bring up a view of several statistics and histograms for the attributes in your data. Once the data profile loads, you can get insight into the data from the views and statistics: The median age of the applicants is 36, with the bulk under 49. About as many people had credits_paid_to_date as prior_payments_delayed. Few had no_credits. The median was 3 years for duration at current residence. Range was 1-6 years.","title":"3. Profile Data"},{"location":"data-visualization-and-refinery/#4-visualize-data","text":"Let's do some visual exploration of our data using charts and graphs. Note that this is an exploratory phase and we're looking for insights in out data. We can accomplish this in Data Refinery interactively without coding. Choose the Visualizations tab to bring up the page where you can select columns that you want to visualize. Select LoanAmount from the \"Columns to visualize\" drop down list as the first column and click Add another column to add another column. Next add LoanDuration and click the Visualize data button. The system will pick a suggested plot for you based on your data and show more suggested plot types at the top. Remember that we are most interested in knowing how these features impact a loan being at the risk. So, let's add the Risk as a color on top of our current scatter plot. That should help us visually see if there's something of interest here. From the left panel, click the Color Map drop down and select Risk . Also, to see the full data, drag the right side of the data selector at the bottom all the way to the right, in order to show all the data inside your plot. We notice that there are more blue (risk) on this plot towards the top right, than there is on the bottom left. This is a good start as it shows that there is probably a relationship between the riskiness of a loan and its duration and amount. It appears that the higher the amount and duration, the riskier the loan. Interesting, let's dig in further in how the loan duration could play into the riskiness of a loan. Note: The colors used in your visualization may be different. Be sure to look at chart legend for clarification Let's plot a histogram of the LoanDuration to see if we can notice anything. First, select Histogram from the Chart Type . On the left, select LoanDuration for the 'X-axis', select Risk in the 'Split By' section, check the Stacked option, uncheck the Show kde curve toggle, uncheck the Show distribution curve toggle. You should see a chart that looks like the following image. It looks like the longer the duration the larger the blue bar (risky loan count) become and the smaller the dark blue bars (non risky loan count) become. That indicate loans with longer duration are in general more likely to be risky. However, we need more information. We next explore if there is some insight in terms of the riskiness of a loan based on its duration when broken down by the loan purpose. To do so, let's create a Heat Map plot. At the top of the page, in the Chart Type section, open the arrows on the right, select Heat Map . Next, select Risk in the column section and LoanPurpose for the Row section. Additionally, to see the effects of the loan duration, select Mean in the summary section, and select LoanDuration in the Value section. You can now see that the least risky loans are those taken out for purchasing a new car and they are on average 10 years long. To the left of that cell we see that loans taken out for the same purpose that average around 15 years for term length seem to be more risky. So one could conclude the longer the loan term is, the more likely it will be risky. In contrast, we can see that both risky and non-risky loans for the other category seem to have the same average term length, so one could conclude that there's little, if any, relationship between loan length and its riskiness for the loans of type other. In general, for each row, the bigger the color difference between the right and left column, the more likely that loan duration plays a role for the riskiness of the loan category. Now let's look into customizing our plot. Under the Actions panel, notice that you can perform tasks such as Start over , Download chart details , Download chart image , or set Global visualization preferences (Note: Hover over the icons to see the names) . Click on the drop down arrow next to Action . Then click on the Global visualization preferences option from the menu. We see that we can do things in the Global visualization preferences for Titles , Tools , Theme , and Notifications . Click on the Theme tab and update the color scheme to Dark . Then click the Apply button, now the colors for all of our charts will reflect this. Play around with various Themes and find one that you like.","title":"4. Visualize Data"},{"location":"data-visualization-and-refinery/#conclusion","text":"We've seen a some of the capabilities of the Data Refinery. We saw how we can transform data using R code, as well as using various operations on the columns such as changing the data type, removing empty rows, or deleting the column altogether. We next saw that all the steps in our Data Flow are recorded, so we can remove steps, repeat them, or edit an individual step. We were able to quickly profile the data, to see histograms and statistics for each column. And finally we created more in-depth Visualizations, creating a scatter plot, histogram, and heatmap to explore the relationship between the riskiness of a loan and its duration, and purpose.","title":"Conclusion"},{"location":"faq/","text":"Frequently Asked Questions & Helpful Tips / Tricks \u00b6 I dont have the Jupyter notebooks in my project. How can I import them ? How can I see what the Jupyter notebooks output should be? Manually Importing Jupyter Notebooks \u00b6 During the pre-work section of this workshop, you create a project based on an existing project file. If, for some reason, you are not using the project zip file to create your project then you will not have all the assets (Jupyter Notebooks, CSV files, etc) necessary for the labs. You can manually import these assets into an existing or empty project. Use the following steps to import the Jupyter notebook files manually. At your project overview page, click the Add to project button, and choose the Notebook option. On the next panel: select the From URL tab, give your notebook a name, provide the notebook URL, and leave the default Python environment: Note: The URL and name shown in the screenshot may not match your scenario. Use the table below to determine what URL to use. Use the following table to find the URL to use for each of the Workshop notebooks: Module Suggested Name URL ML Spark Model machinelearning-creditrisk-sparkmlmodel https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/machinelearning-creditrisk-sparkmlmodel.ipynb ML Model Deployment machinelearning-creditrisk-batchscoring https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/machinelearning-creditrisk-batchscoring.ipynb OpenScale Full Config openscale-full-configuration https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-full-configuration.ipynb OpenScale Manual Config - Initial Setup openscale-initial-setup https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-initial-setup.ipynb OpenScale Manual Config - Fairness / Explainability openscale-fairness-explainability https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-fairness-explainability.ipynb OpenScale Manual Config - Quality openscale-quality-feedback https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-quality-feedback.ipynb OpenScale Manual Config - Drift openscale-drift-config https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-drift-config.ipynb OpenScale Manual Config - Historic Data openscale-historic-data https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-historic-data.ipynb Click the Create notebook button. The Jupyter notebook will be loaded and the kernel is started. Once the kernel is ready you can go back to the workshop instructions. Jupyter Notebooks with Output \u00b6 The Jupyter notebooks in the project file you import have been cleared of output. For reference, you can see the notebooks with output at the links listed below. Note: These links are not meant to be used for importing the notebook into the project. Module Notebook Name Notebook With Output URL ML Spark Model machinelearning-creditrisk-sparkmlmodel https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/machinelearning-creditrisk-sparkmlmodel-with-output.ipynb ML Model Deployment machinelearning-creditrisk-batchscoring https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/machinelearning-creditrisk-batchscoring-with-output.ipynb OpenScale Full Config openscale-full-configuration https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-full-configuration-output.ipynb OpenScale Manual Config - Initial Setup openscale-initial-setup https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-initial-setup-with-output.ipynb OpenScale Manual Config - Fairness / Explainability openscale-fairness-explainability https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-fairness-explainability-with-output.ipynb OpenScale Manual Config - Quality openscale-quality-feedback https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-quality-feedback-with-output.ipynb OpenScale Manual Config - Drift openscale-drift-config https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-drift-config-with-output.ipynb OpenScale Manual Config - Historic Data openscale-historic-data https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-historic-data-with-output.ipynb","title":"FAQs / Tips"},{"location":"faq/#frequently-asked-questions-helpful-tips-tricks","text":"I dont have the Jupyter notebooks in my project. How can I import them ? How can I see what the Jupyter notebooks output should be?","title":"Frequently Asked Questions &amp; Helpful Tips / Tricks"},{"location":"faq/#manually-importing-jupyter-notebooks","text":"During the pre-work section of this workshop, you create a project based on an existing project file. If, for some reason, you are not using the project zip file to create your project then you will not have all the assets (Jupyter Notebooks, CSV files, etc) necessary for the labs. You can manually import these assets into an existing or empty project. Use the following steps to import the Jupyter notebook files manually. At your project overview page, click the Add to project button, and choose the Notebook option. On the next panel: select the From URL tab, give your notebook a name, provide the notebook URL, and leave the default Python environment: Note: The URL and name shown in the screenshot may not match your scenario. Use the table below to determine what URL to use. Use the following table to find the URL to use for each of the Workshop notebooks: Module Suggested Name URL ML Spark Model machinelearning-creditrisk-sparkmlmodel https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/machinelearning-creditrisk-sparkmlmodel.ipynb ML Model Deployment machinelearning-creditrisk-batchscoring https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/machinelearning-creditrisk-batchscoring.ipynb OpenScale Full Config openscale-full-configuration https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-full-configuration.ipynb OpenScale Manual Config - Initial Setup openscale-initial-setup https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-initial-setup.ipynb OpenScale Manual Config - Fairness / Explainability openscale-fairness-explainability https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-fairness-explainability.ipynb OpenScale Manual Config - Quality openscale-quality-feedback https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-quality-feedback.ipynb OpenScale Manual Config - Drift openscale-drift-config https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-drift-config.ipynb OpenScale Manual Config - Historic Data openscale-historic-data https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-historic-data.ipynb Click the Create notebook button. The Jupyter notebook will be loaded and the kernel is started. Once the kernel is ready you can go back to the workshop instructions.","title":"Manually Importing Jupyter Notebooks"},{"location":"faq/#jupyter-notebooks-with-output","text":"The Jupyter notebooks in the project file you import have been cleared of output. For reference, you can see the notebooks with output at the links listed below. Note: These links are not meant to be used for importing the notebook into the project. Module Notebook Name Notebook With Output URL ML Spark Model machinelearning-creditrisk-sparkmlmodel https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/machinelearning-creditrisk-sparkmlmodel-with-output.ipynb ML Model Deployment machinelearning-creditrisk-batchscoring https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/machinelearning-creditrisk-batchscoring-with-output.ipynb OpenScale Full Config openscale-full-configuration https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-full-configuration-output.ipynb OpenScale Manual Config - Initial Setup openscale-initial-setup https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-initial-setup-with-output.ipynb OpenScale Manual Config - Fairness / Explainability openscale-fairness-explainability https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-fairness-explainability-with-output.ipynb OpenScale Manual Config - Quality openscale-quality-feedback https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-quality-feedback-with-output.ipynb OpenScale Manual Config - Drift openscale-drift-config https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-drift-config-with-output.ipynb OpenScale Manual Config - Historic Data openscale-historic-data https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-historic-data-with-output.ipynb","title":"Jupyter Notebooks with Output"},{"location":"gitIntegration/","text":"Git Repository Integration \u00b6 IMPORTANT NOTE: This module has been archived and is no longer being actively maintained. The readme will be left below for reference only. A Cloud Pak for Data project can be integrated with a git repository. The Git integration must be done at project creation time. Generate GitLab Token \u00b6 To create a token for Gitlab, login to GitLab , click on your user account in the top right and choose Settings : From the left navigation bar select Access tokens and fill in the name , expiration date , and check the boxes for read_repository and write_repository . Finally, click Create personal access token button: On the resulting page, you'll see your personal access token. Copy this. NOTE: This token gives access to your git repository. Do not share with anyone. Create Analytics Project with Git Integration \u00b6 Go the (\u2630) navigation menu and click on the Projects link. Click on the New project button on the top right. Select the Analytics project radio button and click the Next button. We are going to create an empty project. Select the Create an empty project option. Give the project a name and Click the box for Integrate this project with git . Then click the New Token link on the right. In the Git Integration panel: select the Gitlab option from the Platform drop down list, Paste in your access token from the previous section, Enter your GitLab username, provide a name to the token. Then click the Continue button. Now select the token you just created in the Token drop down list. Put in the repository URL, Select a branch, and click Create . Note: We are assuming you have a project already created in the repository that you will synch to. If not, go ahead and create an empty project in GitLab Create Asset and Push to GitLab \u00b6 Now whenever we create an asset in the project, we will be able to push it to the GitLab repository. From the project overview, either click the Add to project + button, and choose Notebook . On the 'New notebook' panel, give the notebook a name and optional description. Leave the default runtime. Click the Create notebook button. We can start making changes to the notebook but first lets sync with the repository. Go back to the top-level project page by clicking on the project name in the navigation hierarchy in the top left of the page. Click the \"circular arrow\" sync-icon and choose the Pull and Push option from the menu. On the Confirm Sync page, select your token, check the box next to your notebook, and click the Sync button. A window will come up asking you to verify \"Did you remove credentials from assets?\". After confirming this, click the Continue export button. After syncing, the window will show Success. You can click the Back to project button. You can check your GitLab project to see the notebook has been added under the 'assets' directory. Conclusion \u00b6 In this section we covered how to enable the Git integration to your projects in Cloud Pak for Data. You can integrate git into your workflow in your usual way, syncing with teammates via 'git pull' and using 'git push' to upload your changes to the git remote repository.","title":"Git Repository Integration"},{"location":"gitIntegration/#git-repository-integration","text":"IMPORTANT NOTE: This module has been archived and is no longer being actively maintained. The readme will be left below for reference only. A Cloud Pak for Data project can be integrated with a git repository. The Git integration must be done at project creation time.","title":"Git Repository Integration"},{"location":"gitIntegration/#generate-gitlab-token","text":"To create a token for Gitlab, login to GitLab , click on your user account in the top right and choose Settings : From the left navigation bar select Access tokens and fill in the name , expiration date , and check the boxes for read_repository and write_repository . Finally, click Create personal access token button: On the resulting page, you'll see your personal access token. Copy this. NOTE: This token gives access to your git repository. Do not share with anyone.","title":"Generate GitLab Token"},{"location":"gitIntegration/#create-analytics-project-with-git-integration","text":"Go the (\u2630) navigation menu and click on the Projects link. Click on the New project button on the top right. Select the Analytics project radio button and click the Next button. We are going to create an empty project. Select the Create an empty project option. Give the project a name and Click the box for Integrate this project with git . Then click the New Token link on the right. In the Git Integration panel: select the Gitlab option from the Platform drop down list, Paste in your access token from the previous section, Enter your GitLab username, provide a name to the token. Then click the Continue button. Now select the token you just created in the Token drop down list. Put in the repository URL, Select a branch, and click Create . Note: We are assuming you have a project already created in the repository that you will synch to. If not, go ahead and create an empty project in GitLab","title":"Create Analytics Project with Git Integration"},{"location":"gitIntegration/#create-asset-and-push-to-gitlab","text":"Now whenever we create an asset in the project, we will be able to push it to the GitLab repository. From the project overview, either click the Add to project + button, and choose Notebook . On the 'New notebook' panel, give the notebook a name and optional description. Leave the default runtime. Click the Create notebook button. We can start making changes to the notebook but first lets sync with the repository. Go back to the top-level project page by clicking on the project name in the navigation hierarchy in the top left of the page. Click the \"circular arrow\" sync-icon and choose the Pull and Push option from the menu. On the Confirm Sync page, select your token, check the box next to your notebook, and click the Sync button. A window will come up asking you to verify \"Did you remove credentials from assets?\". After confirming this, click the Continue export button. After syncing, the window will show Success. You can click the Back to project button. You can check your GitLab project to see the notebook has been added under the 'assets' directory.","title":"Create Asset and Push to GitLab"},{"location":"gitIntegration/#conclusion","text":"In this section we covered how to enable the Git integration to your projects in Cloud Pak for Data. You can integrate git into your workflow in your usual way, syncing with teammates via 'git pull' and using 'git push' to upload your changes to the git remote repository.","title":"Conclusion"},{"location":"machine-learning-autoai/","text":"Automate model building with AutoAI \u00b6 For this part of the workshop, we'll learn how to use AutoAI . AutoAI is a capability that automates various tasks to ease the workflow for data scientists that are creating machine learning models. It automates steps such as preparing your data for modeling, chooses the best algorithm/estimator for your problem, experiments with pipelines and parameters for the trained models. This section is broken up into the following steps: Run AutoAI Experiment Save AutoAI Model Save AutoAI Notebook Promote the Model Note: The lab instructions below assume you have a project already with the assets necessary to build a model. If not, follow the instructions in the pre-work section to create a project. 1. Run AutoAI Experiment \u00b6 Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. To start the AutoAI experiment, click the Add to Project button from the top of the page and select the AutoAI experiment option. Name your AutoAI experiment asset and leave the default compute configuration option listed in the drop-down menu. Then click the Create button. To configure the experiment, we must first give it the dataset that will be used to train the machine learning model. We will be using one of the CSV file datasets we have preloaded into the project. Click on the Select from project option. In the dialog, select the german_credit_data.csv file and click the Select asset button. Once the dataset is read in, we will need to indicate what we want the model to predict. Under Select prediction column panel, find and click on the Risk row. AutoAI will set up defaults values for the experiment based on the dataset and the column selected for the prediction. This includes the type of model to build, the metrics to optimize against, the test/train split, etc. To view/change these values, click the Experiment settings button. On the Data source settings panel, in the Select columns to include section, deselect the checkbox for the CustomerID column name. This will remove the customer ID column from being used as a feature for the model. Although we could change other aspects of the experiment, we will accept the remaining default values and click the Save settings button. To start the experiment, click on the Run experiment button. The AutoAI experiment will now run. AutoAI will run through steps to prepare the dataset, split the dataset into training / evaluation groups and then find the best performing algorithms / estimators for the type of model. It will then build the following series of candidate pipelines for each of the top N performing algorithms (where N is a number chosen in the configuration which defaults to 2): Baseline model (Pipeline 1) Hyperparameter optimization (Pipeline 2) Automated feature engineering (Pipeline 3) Hyperparameter optimization on top of engineered features(Pipeline 4) The UI will show progress as different algorithms/evaluators are selected and as different pipelines are created & evaluated. You can view the performance of the pipelines that have completed by expanding each pipeline section in the leaderboard. The experiment can take several minutes to run. Upon completion you will see a message that the pipelines have been created. Do not proceed to the next section until the experiment completes. 2. Save AutoAI Model \u00b6 Once the experiment completes, you can explore the various pipelines and options in the UI. Some of the options available are to see a comparison of the pipelines, to change the ranking based on a different performance metric, to see a log of the experiment, or to see the ranked listing of the pipelines (ranking based on the optimization metric in your experiment, in this case accuracy.) Scroll down to see the Pipeline leaderboard . The top performing pipeline is in the first rank. The next step is to select the model that gives the best result and view its performance. In this case, Pipeline 4 gave the best result for our experiment. You can view the detailed results by clicking the corresponding pipeline name from the leaderboard: The model evaluation page will show metrics for the experiment, confusion matrix, feature transformations that were performed (if any), which features contribute to the model, and more details of the pipeline. Optionally, feel free to click through these views of the pipeline details. In order to deploy this model, click on the Save as button. On the next scren, select the Model option. Keep the default name or change it, add an optional description and tags, and click Create to save it. You receive a notification to indicate that your model is saved to your project. You can return to your project using the notification by clicking View in project , or go back to your project main page by clicking on the project name on the navigator on the top left. You will see the new model under Models section of the Assets page: 3. Save AutoAI notebook \u00b6 To save the AutoAI experiment as a notebook, go back to the window for the pipeline you have chosen, and click Save as . Note: You can get back to the pipeline window by going to your project overview page, clicking on the AutoAI experiment and clicking the pipeline from the leaderboard Choose the Notebook tile, accept the default name or change it if you like. Add optional description or tags, and click Create . You will receive a notification to indicate that your notebook is saved to your project. Close the pipeline details window to expose the path back to the project at the top of the screen. Click on your project name to navigate to the project overview page. The notebook will be saved to your project, and can be examined in detail, changed and modified, and used to create a new model. See the documentations for Modifying and running an AutoAI generated notebook for details. 4. Promote the model \u00b6 Now that we have saved our model, we will next need to make the model available in our deployment space so it can be deployed. Under the Models section of the Assets page, click the name of your saved model. To make the model available to be deployed, we need to make it available in the deployment space we previously set up. Click on the Promote to deployment space : ***Note: This is assuming you have already created a deployment space in the pre-work section of the workshop. Add an optional description or tags if you'd like. Click on the Promote button. You will see a notification that the model was promoted to the deployment space succesfully. Conclusion \u00b6 In this section we covered one approach to building machine learning models on Cloud Pak for Data. We have seen how AutoAI helps find an optimal model by automating tasks such as: Data Wrangling Model Algorithm Evaluation & Selection Feature Engineering Hyperparameter Optimization.","title":"Machine Learning with AutoAI"},{"location":"machine-learning-autoai/#automate-model-building-with-autoai","text":"For this part of the workshop, we'll learn how to use AutoAI . AutoAI is a capability that automates various tasks to ease the workflow for data scientists that are creating machine learning models. It automates steps such as preparing your data for modeling, chooses the best algorithm/estimator for your problem, experiments with pipelines and parameters for the trained models. This section is broken up into the following steps: Run AutoAI Experiment Save AutoAI Model Save AutoAI Notebook Promote the Model Note: The lab instructions below assume you have a project already with the assets necessary to build a model. If not, follow the instructions in the pre-work section to create a project.","title":"Automate model building with AutoAI"},{"location":"machine-learning-autoai/#1-run-autoai-experiment","text":"Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. To start the AutoAI experiment, click the Add to Project button from the top of the page and select the AutoAI experiment option. Name your AutoAI experiment asset and leave the default compute configuration option listed in the drop-down menu. Then click the Create button. To configure the experiment, we must first give it the dataset that will be used to train the machine learning model. We will be using one of the CSV file datasets we have preloaded into the project. Click on the Select from project option. In the dialog, select the german_credit_data.csv file and click the Select asset button. Once the dataset is read in, we will need to indicate what we want the model to predict. Under Select prediction column panel, find and click on the Risk row. AutoAI will set up defaults values for the experiment based on the dataset and the column selected for the prediction. This includes the type of model to build, the metrics to optimize against, the test/train split, etc. To view/change these values, click the Experiment settings button. On the Data source settings panel, in the Select columns to include section, deselect the checkbox for the CustomerID column name. This will remove the customer ID column from being used as a feature for the model. Although we could change other aspects of the experiment, we will accept the remaining default values and click the Save settings button. To start the experiment, click on the Run experiment button. The AutoAI experiment will now run. AutoAI will run through steps to prepare the dataset, split the dataset into training / evaluation groups and then find the best performing algorithms / estimators for the type of model. It will then build the following series of candidate pipelines for each of the top N performing algorithms (where N is a number chosen in the configuration which defaults to 2): Baseline model (Pipeline 1) Hyperparameter optimization (Pipeline 2) Automated feature engineering (Pipeline 3) Hyperparameter optimization on top of engineered features(Pipeline 4) The UI will show progress as different algorithms/evaluators are selected and as different pipelines are created & evaluated. You can view the performance of the pipelines that have completed by expanding each pipeline section in the leaderboard. The experiment can take several minutes to run. Upon completion you will see a message that the pipelines have been created. Do not proceed to the next section until the experiment completes.","title":"1. Run AutoAI Experiment"},{"location":"machine-learning-autoai/#2-save-autoai-model","text":"Once the experiment completes, you can explore the various pipelines and options in the UI. Some of the options available are to see a comparison of the pipelines, to change the ranking based on a different performance metric, to see a log of the experiment, or to see the ranked listing of the pipelines (ranking based on the optimization metric in your experiment, in this case accuracy.) Scroll down to see the Pipeline leaderboard . The top performing pipeline is in the first rank. The next step is to select the model that gives the best result and view its performance. In this case, Pipeline 4 gave the best result for our experiment. You can view the detailed results by clicking the corresponding pipeline name from the leaderboard: The model evaluation page will show metrics for the experiment, confusion matrix, feature transformations that were performed (if any), which features contribute to the model, and more details of the pipeline. Optionally, feel free to click through these views of the pipeline details. In order to deploy this model, click on the Save as button. On the next scren, select the Model option. Keep the default name or change it, add an optional description and tags, and click Create to save it. You receive a notification to indicate that your model is saved to your project. You can return to your project using the notification by clicking View in project , or go back to your project main page by clicking on the project name on the navigator on the top left. You will see the new model under Models section of the Assets page:","title":"2. Save AutoAI Model"},{"location":"machine-learning-autoai/#3-save-autoai-notebook","text":"To save the AutoAI experiment as a notebook, go back to the window for the pipeline you have chosen, and click Save as . Note: You can get back to the pipeline window by going to your project overview page, clicking on the AutoAI experiment and clicking the pipeline from the leaderboard Choose the Notebook tile, accept the default name or change it if you like. Add optional description or tags, and click Create . You will receive a notification to indicate that your notebook is saved to your project. Close the pipeline details window to expose the path back to the project at the top of the screen. Click on your project name to navigate to the project overview page. The notebook will be saved to your project, and can be examined in detail, changed and modified, and used to create a new model. See the documentations for Modifying and running an AutoAI generated notebook for details.","title":"3. Save AutoAI notebook"},{"location":"machine-learning-autoai/#4-promote-the-model","text":"Now that we have saved our model, we will next need to make the model available in our deployment space so it can be deployed. Under the Models section of the Assets page, click the name of your saved model. To make the model available to be deployed, we need to make it available in the deployment space we previously set up. Click on the Promote to deployment space : ***Note: This is assuming you have already created a deployment space in the pre-work section of the workshop. Add an optional description or tags if you'd like. Click on the Promote button. You will see a notification that the model was promoted to the deployment space succesfully.","title":"4. Promote the model"},{"location":"machine-learning-autoai/#conclusion","text":"In this section we covered one approach to building machine learning models on Cloud Pak for Data. We have seen how AutoAI helps find an optimal model by automating tasks such as: Data Wrangling Model Algorithm Evaluation & Selection Feature Engineering Hyperparameter Optimization.","title":"Conclusion"},{"location":"machine-learning-autoai/running-autoai-notebook/","text":"Modifying and Running an AutoAI generated notebook \u00b6 Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ([*]) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17]). The notebook generated is pre filled with Python code and is divided into the following steps: 1.0 Setup 2.0 Pipeline inspection 3.0 Pipeline refinery and testing 3.1 Modify source code 3.2 Use lale library for semiautomated data science 4.0 Deploy and score as web service using WML instance 1.0 Setup \u00b6 In the cell after Connection to WML you will enter the Cloud Pak for Data cluster URL to access your instance of Watson Machine Learning. This URL will look like: https://zen-cpd-zen.myname-cpd3-os45-v2-2bef1f4b4393021da9302000c44fc3b2-0000.us-south.containers.appdomain.cloud/ You can get this from your browser window where you are viewing this notebook, dropping everything in the URL after appdomain.cloud/ (or whatever ends the first section of the URL). This section also contains credentials to Cloud Object Storage through which the current AutoAI pipeline is retrieved. The cell contains code prefilled to extract the training data used to create the pipeline and the pipeline results. Also this section contains the metadata of the current pipelines that were used to run the experiment. 2.0 Pipeline inspection \u00b6 Within this section of the notebook, there is code that extracts the current model and prints it as Python code. This section also contains code to visualize the stages within the model as a graph using Watson Machine Learning's AutoAI APIs. 3.0 Pipeline refinery and testing \u00b6 This optional section walks us through two different ways of editing and retraining the model generated from the chosen pipeline. Here, you could modify parameters or change inputs to the methods such as algorithm choices, cross validation, or maximum evaluations. For this lab, just run the cells as the are, but feel free to come back later and modify things for your own learning and experimentation. Convert Raw NBConvert cells to Code Cells - Since this section of the notebook is optional, these cells cannot be run by default. To execute these series of cells, highlight the cell, and click on the Format dropdown from the menu and change the selection from Raw NBConvert cell to Code . 3.1 Modify source code \u00b6 By running the pretty-print function, a code cell is inserted as shown below. This new cell contains the source code of the pipeline. This code can now be modified and re-trained to produce a new model. 3.2 Use lale library for semiautomated data science \u00b6 By using the hyperopt optimizer provided by the lale framework, we see how an estimator modified as a graph can be trained and tested. Since the framework is compatible with scikit-learn the output pipeline can be exported as a scikit-learn model using the export_to_sklearn_pipeline method. 4.0 Deploy and score as web service using WML instance \u00b6 This section of the notebook contains code that deploys the pipeline model as a web service using Watson Machine Learning. This section requires users to enter credentials to be able to identify the deployment space used for this model. target_space_id \u00b6 You'll need the target_space_id from your deployment space, created in the pre-work (../pre-work#create-a-deployment-space) part of this workshop. Click on the hamburger menu on the top-left corner of the Cloud Pak for Data home page. Click on Deployments from the list and select the Spaces tab. Click on the name of your Deployment space, click on the Settings tab, and then copy the Space ID . Acquire the target_space_id as shown in the steps above and paste within the Create deployment section. The Watson Machine Learning API uses the wml_credentials and the target_space_id to deploy the machine learning model as a web service. Once the cells are executed, the model is promoted to the deployment space and is now available as a web service and can be verified from within the UI, or using the Python SDK as demonstrated in the notebook. Score web service \u00b6 For details on how to test the deployed model using the UI, see the steps in the Machine Learning Deployment and Scoring Lab For this lab, you can test this deployment using the Python SDK by running the final cell in the notebook: predictions = service . score ( payload = test_df ) predictions Conclusion \u00b6 In this part of the lab, we examined and ran a Jupyter notebook that was generated as the result of an AutoAI experiment. Feel free to modify and re-run the notebook, making any changes that you are comfortable with.","title":"Modifying and Running an AutoAI generated notebook"},{"location":"machine-learning-autoai/running-autoai-notebook/#modifying-and-running-an-autoai-generated-notebook","text":"Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ([*]) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17]). The notebook generated is pre filled with Python code and is divided into the following steps: 1.0 Setup 2.0 Pipeline inspection 3.0 Pipeline refinery and testing 3.1 Modify source code 3.2 Use lale library for semiautomated data science 4.0 Deploy and score as web service using WML instance","title":"Modifying and Running an AutoAI generated notebook"},{"location":"machine-learning-autoai/running-autoai-notebook/#10-setup","text":"In the cell after Connection to WML you will enter the Cloud Pak for Data cluster URL to access your instance of Watson Machine Learning. This URL will look like: https://zen-cpd-zen.myname-cpd3-os45-v2-2bef1f4b4393021da9302000c44fc3b2-0000.us-south.containers.appdomain.cloud/ You can get this from your browser window where you are viewing this notebook, dropping everything in the URL after appdomain.cloud/ (or whatever ends the first section of the URL). This section also contains credentials to Cloud Object Storage through which the current AutoAI pipeline is retrieved. The cell contains code prefilled to extract the training data used to create the pipeline and the pipeline results. Also this section contains the metadata of the current pipelines that were used to run the experiment.","title":"1.0 Setup"},{"location":"machine-learning-autoai/running-autoai-notebook/#20-pipeline-inspection","text":"Within this section of the notebook, there is code that extracts the current model and prints it as Python code. This section also contains code to visualize the stages within the model as a graph using Watson Machine Learning's AutoAI APIs.","title":"2.0 Pipeline inspection"},{"location":"machine-learning-autoai/running-autoai-notebook/#30-pipeline-refinery-and-testing","text":"This optional section walks us through two different ways of editing and retraining the model generated from the chosen pipeline. Here, you could modify parameters or change inputs to the methods such as algorithm choices, cross validation, or maximum evaluations. For this lab, just run the cells as the are, but feel free to come back later and modify things for your own learning and experimentation. Convert Raw NBConvert cells to Code Cells - Since this section of the notebook is optional, these cells cannot be run by default. To execute these series of cells, highlight the cell, and click on the Format dropdown from the menu and change the selection from Raw NBConvert cell to Code .","title":"3.0 Pipeline refinery and testing"},{"location":"machine-learning-autoai/running-autoai-notebook/#31-modify-source-code","text":"By running the pretty-print function, a code cell is inserted as shown below. This new cell contains the source code of the pipeline. This code can now be modified and re-trained to produce a new model.","title":"3.1 Modify source code"},{"location":"machine-learning-autoai/running-autoai-notebook/#32-use-lale-library-for-semiautomated-data-science","text":"By using the hyperopt optimizer provided by the lale framework, we see how an estimator modified as a graph can be trained and tested. Since the framework is compatible with scikit-learn the output pipeline can be exported as a scikit-learn model using the export_to_sklearn_pipeline method.","title":"3.2 Use lale library for semiautomated data science"},{"location":"machine-learning-autoai/running-autoai-notebook/#40-deploy-and-score-as-web-service-using-wml-instance","text":"This section of the notebook contains code that deploys the pipeline model as a web service using Watson Machine Learning. This section requires users to enter credentials to be able to identify the deployment space used for this model.","title":"4.0 Deploy and score as web service using WML instance"},{"location":"machine-learning-autoai/running-autoai-notebook/#target_space_id","text":"You'll need the target_space_id from your deployment space, created in the pre-work (../pre-work#create-a-deployment-space) part of this workshop. Click on the hamburger menu on the top-left corner of the Cloud Pak for Data home page. Click on Deployments from the list and select the Spaces tab. Click on the name of your Deployment space, click on the Settings tab, and then copy the Space ID . Acquire the target_space_id as shown in the steps above and paste within the Create deployment section. The Watson Machine Learning API uses the wml_credentials and the target_space_id to deploy the machine learning model as a web service. Once the cells are executed, the model is promoted to the deployment space and is now available as a web service and can be verified from within the UI, or using the Python SDK as demonstrated in the notebook.","title":"target_space_id"},{"location":"machine-learning-autoai/running-autoai-notebook/#score-web-service","text":"For details on how to test the deployed model using the UI, see the steps in the Machine Learning Deployment and Scoring Lab For this lab, you can test this deployment using the Python SDK by running the final cell in the notebook: predictions = service . score ( payload = test_df ) predictions","title":"Score web service"},{"location":"machine-learning-autoai/running-autoai-notebook/#conclusion","text":"In this part of the lab, we examined and ran a Jupyter notebook that was generated as the result of an AutoAI experiment. Feel free to modify and re-run the notebook, making any changes that you are comfortable with.","title":"Conclusion"},{"location":"machine-learning-deployment-scoring/","text":"Machine Learning Model Deployment and Scoring \u00b6 In this module, we will go through the process of deploying a machine learning model so it can be used by others. Deploying a model allows us to put a model into production, so that data can be passed to it to return a prediction. The deployment will result in an endpoint that makes the model available for wider use in applications and to make business decisions. There are several types of deployments available ( depending on the model framework used ), of which we will explore: Online Deployments - Creates an endpoint to generate a score or prediction in real time. Batch Deployments - Creates an endpoint to schedule the processing of bulk data to return predictions. This module is broken up into several sections that explore the different model deployment options as well as the different ways to invoke or consume the model. The first section of this lab will build an online deployment and test the model endpoint using both the built in testing tool as well as external testing tools. The remaining sections are optional, they build and test the batch deployment, followed by using the model from a python application. Online Deployment for a Model Create Online Deployment Test model using Cloud Pak for Data tooling (Optional) Test model using cURL (Optional) Batch Deployment for a Model Create Batch Deployment Create and Schedule a Job (Optional) Integrate Model to an External Application Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks. Note: It is also assumed that you have completed one of the machine learning modules to promote a model to the deployment space. If not, follow the instructions in one of the machine learning modules to create and promote a machine learning model. Online Model Deployment \u00b6 After a model has been created and saved / promoted to our deployment space, we will want to deploy the model so it can be used by others. For this section, we will be creating an online deployment. This type of deployment will make an instance of the model available to make predictions in real time via an API. Although we will use the Cloud Pak for Data UI to deploy the model, the same can be done programmatically. Navigate to the left-hand (\u2630) hamburger menu and click on Deployments . Click on the Spaces tab and then choose the deployment space you setup previously by clicking on the name of your space. From your deployment space overview, in the table, find the model name for the model you previously built and now want to create a deployment against. Use your mouse to hover over the right side of that table row and click the Deploy rocket icon (the icons are not visible by default until you hover over them). Note: There may be more than one model listed in the 'Models' section. This can happen if you have run the Jupyter notebook more than once or if you have run through both the Jupyter notebook and AutoAI modules to create models. Although you could select any of the models you see listed in the page, the recommendation is to start with whichever model is available that is using a spark-mllib_2.4 software specification. On the 'Create a deployment' screen, choose Online for the Deployment Type , give the Deployment a name and optional description and click the Create button. Click on the Deployments tab. The online deployment will show as In progress and then switch to Deployed when done. Test Online Model Deployment \u00b6 Cloud Pak for Data offers tools to quickly test out Watson Machine Learning models. We begin with the built-in tooling. From the Model deployment page, once the deployment status shows as Deployed , click on the name of your deployment. The deployment API reference tab shows how to use the model using cURL , Java , Javascript , Python , and Scala . To get to the built-in test tool, click on the Test tab and then click on the Provide input data as JSON icon. Copy and paste the following data objects into the Body panel (replace the text that was in the input panel). Note: Click the tab appropriate for the model you are testing (either an AutoAI model or one built using the Jupyter notebook). Also make sure the input below is the only content in the field. Do not append it to the default content { \"input_data\": [] } that may already be in the test input panel. Jupyter Spark Model { \"input_data\" : [{ \"fields\" : [ \"CheckingStatus\" , \"LoanDuration\" , \"CreditHistory\" , \"LoanPurpose\" , \"LoanAmount\" , \"ExistingSavings\" , \"EmploymentDuration\" , \"InstallmentPercent\" , \"Sex\" , \"OthersOnLoan\" , \"CurrentResidenceDuration\" , \"OwnsProperty\" , \"Age\" , \"InstallmentPlans\" , \"Housing\" , \"ExistingCreditsCount\" , \"Job\" , \"Dependents\" , \"Telephone\" , \"ForeignWorker\" ], \"values\" : [[ \"no_checking\" , 13 , \"credits_paid_to_date\" , \"car_new\" , 1343 , \"100_to_500\" , \"1_to_4\" , 2 , \"female\" , \"none\" , 3 , \"savings_insurance\" , 46 , \"none\" , \"own\" , 2 , \"skilled\" , 1 , \"none\" , \"yes\" ]] }]} AutoAI Model { \"input_data\" : [{ \"fields\" : [ \"CustomerId\" , \"CheckingStatus\" , \"LoanDuration\" , \"CreditHistory\" , \"LoanPurpose\" , \"LoanAmount\" , \"ExistingSavings\" , \"EmploymentDuration\" , \"InstallmentPercent\" , \"Sex\" , \"OthersOnLoan\" , \"CurrentResidenceDuration\" , \"OwnsProperty\" , \"Age\" , \"InstallmentPlans\" , \"Housing\" , \"ExistingCreditsCount\" , \"Job\" , \"Dependents\" , \"Telephone\" , \"ForeignWorker\" ], \"values\" : [[ \"\" , \"no_checking\" , 13 , \"credits_paid_to_date\" , \"car_new\" , 1343 , \"100_to_500\" , \"1_to_4\" , 2 , \"female\" , \"none\" , 3 , \"savings_insurance\" , 46 , \"none\" , \"own\" , 2 , \"skilled\" , 1 , \"none\" , \"yes\" ]] }]} Click the Predict button. The model will be called with the input data and the results will display in the Result window. Scroll down to the bottom of the result to see the prediction (i.e \"Risk\" or \"No Risk\"): Note: For some deployed models (for example AutoAI based models), you can provide the request payload using a generated form by clicking on the Provide input using form icon and providing values for the input fields of the form. If the form is not available for the model you deployed, the icon will not be displayed. (Optional) Test Online Model Deployment using cURL \u00b6 Now that the model is deployed, we can also test it from external applications. One way to invoke the model API is using the cURL command. NOTE: Windows users will need the cURL command. It's recommended to download gitbash for this, as you'll also have other tools and you'll be able to easily use the shell environment variables in the following steps. Also note that if you are not using gitbash, you may need to change export commands to set commands. In a terminal window (or command prompt in Windows), run the following command to get a token to access the API. Replace <username> and <password> with the username and password you used to log into the Cloud pak for data cluster. Replace <cluster-url> with just the hostname of the cloud pak for data cluster (i.e the url from your web browser address bar) curl -k -X GET https://<cluster-url>/v1/preauth/validateAuth -u <username>:<password> A json string will be returned with a value for \"accessToken\" that will look similar to this: { \"username\" : \"scottda\" , \"role\" : \"Admin\" , \"permissions\" :[ \"access_catalog\" , \"administrator\" , \"manage_catalog\" , \"can_provision\" ], \"sub\" : \"scottda\" , \"iss\" : \"KNOXSSO\" , \"aud\" : \"DSX\" , \"uid\" : \"1000331002\" , \"authenticator\" : \"default\" , \"accessToken\" : \"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6InNjb3R0ZGEiLCJyb2xlIjoiQWRtaW4iLCJwZXJtaXNzaW9ucyI6WyJhY2Nlc3NfY2F0YWxvZyIsImFkbWluaXN0cmF0b3IiLCJtYW5hZ2VfY2F0YWxvZyIsImNhbl9wcm92aXNpb24iXSwic3ViIjoic2NvdHRkYSIsImlzcyI6IktOT1hTU08iLCJhdWQiOiJEU1giLCJ1aWQiOiIxMDAwMzMxMDAyIiwiYXV0aGVudGljYXRvciI6ImRlZmF1bHQiLCJpYXQiOjE1NzM3NjM4NzYsImV4cCI6MTU3MzgwNzA3Nn0.vs90XYeKmLe0Efi5_3QV8F9UK1tjZmYIqmyCX575I7HY1QoH4DBhon2fa4cSzWLOM7OQ5Xm32hNUpxPH3xIi1PcxAntP9jBuM8Sue6JU4grTnphkmToSlN5jZvJOSa4RqqhjzgNKFoiqfl4D0t1X6uofwXgYmZESP3tla4f4dbhVz86RZ8ad1gS1_UNI-w8dfdmr-Q6e3UMDUaahh8JaAEiSZ_o1VTMdVPMWnRdD1_F0YnDPkdttwBFYcM9iSXHFt3gyJDCLLPdJkoyZFUa40iRB8Xf5-iA1sxGCkhK-NVHh-VTS2XmKAA0UYPGYXmouCTOUQHdGq2WXF7PkWQK0EA\" , \"_messageCode_\" : \"success\" , \"message\" : \"success\" } You will save this access token to a temporary environment variable in your terminal. Copy the access token value (without the quotes) in the terminal and then use the following export command to save the \"accessToken\" to a variable called WML_AUTH_TOKEN . export WML_AUTH_TOKEN = <value-of-access-token> Back on the model deployment page, gather the URL to invoke the deployed model from the API reference by copying the Endpoint . Now save that endpoint to a variable named URL in your terminal by exporting it. export URL = <value-of-endpoint> Now run this curl command from the terminal to invoke the model with the same payload we used previousy: curl -k -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' --header \"Authorization: Bearer $WML_AUTH_TOKEN \" -d '{\"input_data\": [{\"fields\": [ \"CheckingStatus\", \"LoanDuration\", \"CreditHistory\", \"LoanPurpose\", \"LoanAmount\", \"ExistingSavings\", \"EmploymentDuration\", \"InstallmentPercent\", \"Sex\", \"OthersOnLoan\", \"CurrentResidenceDuration\", \"OwnsProperty\", \"Age\", \"InstallmentPlans\", \"Housing\", \"ExistingCreditsCount\", \"Job\", \"Dependents\", \"Telephone\", \"ForeignWorker\"],\"values\": [[ \"no_checking\", 13, \"credits_paid_to_date\", \"car_new\", 1343, \"100_to_500\", \"1_to_4\", 2, \"female\", \"none\", 3, \"savings_insurance\", 46, \"none\", \"own\", 2, \"skilled\", 1, \"none\", \"yes\"]]}]}' $URL A json string will be returned with the response, including a prediction from the model (i.e a \"Risk\" or \"No Risk\" at the end indicating the prediction of this loan representing risk). (Optional) Batch Model Deployment \u00b6 Another approach to expose the model to be consumed by other users/applications is to create a batch deployment. This type of deployment will make an instance of the model available to make predictions against data assets or groups of records. The model prediction requests are scheduled as jobs, which are exected asynchronously. For the lab, we will break this into two steps: Creating the deployment (which we will do using the UI). Creating and scheduling a job with values (which we will do using a Jupyter Notebook). Lets start by creating the deployment: Navigate to the left-hand (\u2630) hamburger menu and click on Deployments . Click on the Spaces tab and then choose the deployment space you setup previously by clicking on the name of your space. From your deployment space overview, in the table, find the model name for the model you previously built and now want to create a deployment against. Use your mouse to hover over the right side of that table row and click the Deploy rocket icon (the icons are not visible by default until you hover over them). Note: There may be more than one model listed in them 'Models' section. This can happen if you have run the Jupyter notebook more than once or if you have run through both the Jupyter notebook and AutoAI modules to create models. Although you could select any of the models you see listed in the page, the recommendation is to start with whicever model is available that is using a spark-mllib_2.4 software specification. On the 'Create a deployment' screen: choose Batch for the Deployment Type , give the deployment a name and optional description. From the 'Hardware definition' drop down, select the smallest option ( 1 standard CPU, 4GB RAM in this case though for large or frequent batch jobs, you might choose to scale the hardware up). Click the Create button. Once the status shows as Deployed you will be able to start submitting jobs to the deployment. Create and Schedule a Job \u00b6 Next we can schedule a job to run against our batch deployment. We could create a job, with specific input data (or data asset) and schedule, either programmatically or through the UI. For this lab, we are going to do this programmatically using the Python client SDK. For this part of the exercise we're going to use a Jupyter notebook to create and submit a batch job to our model deployment. Note: The batch job input is impacted by the machine learning framework used to build the model. Currently, SparkML based model batch jobs require inline payload to be used. For other frameworks, we can use data assets (i.e CSV files) as the input payload. Run the Batch Notebook \u00b6 The Jupyter notebook is already included as an asset in the project you imported earlier. Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click the project name you created in the pre-work section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the machinelearning-creditrisk-sparkmlmodel notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section. Notebook sections \u00b6 With the notebook open, spend a minute looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Please note that some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. Section 1.0 Install required packages will install some of the libraries we are going to use in the notebook (many libraries come pre-installed on Cloud Pak for Data). Note that we upgrade the installed version of Watson Machine Learning Python Client. Ensure the output of the first code cell is that the python packages were successfully installed. Section 2.0 Create Batch Deployment Job will create a job for the batch deployment. To do that, we will use the Watson Machine Learning client to get our deployment and create a job. In the first code cell for Section2.1 , be sure to update the wml_credentials variable. The url should be the hostname of the Cloud Pak for Data instance. The username and password should be the same credentials you used to log into Cloud Pak for Data. In section 2.2, be sure to update the DEPLOYMENT_SPACE_NAME variable with your deployment space name (copy and past the name which is within the output of the previous code cell). In section 2.3, be sure to update the DEPLOYMENT_NAME variable with the name of the batch deployment you created previously (copy and past the name which is within the output of the previous code cell). Continue to run the rest of the cells in section 2 which will load the batch input data set and create the job. The last code cell in section 2 will show that the job is in a queued state. Section 3.0 Monitor Batch Job Status will start polling the job status until it completes or fails. The code cell will output the status every 5 seconds as the job goes from queued to running to completed or failed. Once the job completes, continue to run the cells until the end of the notebook. Cleanup and Stop Environment \u00b6 Important : In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the Environments tab near the top of the page. Then in the Active environment runtimes section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window. (Optional) Integrate Model to Python Flask Application \u00b6 You can also access the online model deployment directly through the REST API. This allows you to use your model for inference in any of your apps. For this workshop we'll be using a Python Flask application to collect information, score it against the model, and show the results. IMPORTANT: This SAMPLE application only runs on python 3.6 and above, so the instructions here are for python 3.6+ only. You will need to have Python 3.6 or later already installed on your machine Note: The instructions below assume you have completed the pre-work module and thus have the Git repository already on your machine (cloned or downloaded). Install Dependencies \u00b6 The general recommendation for Python development is to use a virtual environment ( venv ). To install and initialize a virtual environment, use the venv module on Python 3: Initialize a virtual environment with venv . Run the following commands in a terminal (or command prompt): # Create the virtual environment using Python. # Note, it may be named python3 on your system. python -m venv venv # Python 3.X # Source the virtual environment. Use one of the two commands depending on your OS. source venv/bin/activate # Mac or Linux ./venv/Scripts/activate # Windows PowerShell TIP To terminate the virtual environment use the deactivate command. Unzip the python application zip file that you downloaded in the pre-work section. To install the Python requirements, from a terminal (or command prompt) navigate to where you unzipped the python application. Run the following commands: pip install -r requirements.txt Update Environment Variables \u00b6 It's best practice to store configurable information as environment variables, instead of hard-coding any important information. To reference our model and supply an API key, we'll pass these values in via a file that is read, the key-value pairs in this files are stored as environment variables. Copy the env.sample file to .env . cp env.sample .env Edit .env to and fill in the MODEL_URL as well as the AUTH_URL , AUTH_USERNAME , and AUTH_PASSWORD . MODEL_URL is your web service URL for scoring which you got from the section above AUTH_URL is the preauth url of your CloudPak4Data and will look like this: https://<cluster_url>/v1/preauth/validateAuth AUTH_USERNAME is your username with which you login to the CloudPak4Data environment AUTH_PASSWORD is your password with which you login to the CloudPak4Data environment Note: Alternatively, you can fill in the AUTH_TOKEN instead of AUTH_URL , AUTH_USERNAME , and AUTH_PASSWORD . You will have generated this token in the section above. However, since tokens expire after a few hours and you would need to restart your app to update the token, this option is not suggested. Instead, if you use the username/password option, the app can generate a new token every time for you so it will always have a non-expired ones. Here's an example of a completed lines of the .env file. # Required: Provide your web service URL for scoring. # E.g., MODEL_URL=https://<cluster_url>/v4/deployments/<deployment_space_guid>/predictions MODEL_URL = https://cp4d.cp4dworkshops.com/v4/deployments/5f939979-14c2-4538-a2af-a970aeb59abd/predictions # Required: Please fill in EITHER section A OR B below: # #### A: Authentication using username and password # Fill in the authntication url, your CloudPak4Data username, and CloudPak4Data password. # Example: # AUTH_URL=<cluster_url>/v1/preauth/validateAuth # AUTH_USERNAME=my_username # AUTH_PASSWORD=super_complex_password AUTH_URL = https://cp4d.cp4dworkshops.com/v1/preauth/validateAuth AUTH_USERNAME = username_001 AUTH_PASSWORD = my_secure_password_! Start Application \u00b6 Start the flask server by running the following command: python creditriskapp.py Use your browser to go to http://0.0.0.0:5000 and try it out. TIP : Use ctrl + c to stop the Flask server when you are done. Test the application \u00b6 Either use the default values pre-filled in the input form, or modify the value and then click the Submit button. The python application will invoke the predictive model and a risk prediction & probability is returned: Conclusion \u00b6 In this section we covered the followings: Creating and Testing Online Deployments for models. (Optional) Creating and Testing Batch Deployments for models. (Optional) Integrating the model deployment in an external application. Taking a predictive model and infusing AI into applications.","title":"Deploy and Test Machine Learning Models"},{"location":"machine-learning-deployment-scoring/#machine-learning-model-deployment-and-scoring","text":"In this module, we will go through the process of deploying a machine learning model so it can be used by others. Deploying a model allows us to put a model into production, so that data can be passed to it to return a prediction. The deployment will result in an endpoint that makes the model available for wider use in applications and to make business decisions. There are several types of deployments available ( depending on the model framework used ), of which we will explore: Online Deployments - Creates an endpoint to generate a score or prediction in real time. Batch Deployments - Creates an endpoint to schedule the processing of bulk data to return predictions. This module is broken up into several sections that explore the different model deployment options as well as the different ways to invoke or consume the model. The first section of this lab will build an online deployment and test the model endpoint using both the built in testing tool as well as external testing tools. The remaining sections are optional, they build and test the batch deployment, followed by using the model from a python application. Online Deployment for a Model Create Online Deployment Test model using Cloud Pak for Data tooling (Optional) Test model using cURL (Optional) Batch Deployment for a Model Create Batch Deployment Create and Schedule a Job (Optional) Integrate Model to an External Application Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks. Note: It is also assumed that you have completed one of the machine learning modules to promote a model to the deployment space. If not, follow the instructions in one of the machine learning modules to create and promote a machine learning model.","title":"Machine Learning Model Deployment and Scoring"},{"location":"machine-learning-deployment-scoring/#online-model-deployment","text":"After a model has been created and saved / promoted to our deployment space, we will want to deploy the model so it can be used by others. For this section, we will be creating an online deployment. This type of deployment will make an instance of the model available to make predictions in real time via an API. Although we will use the Cloud Pak for Data UI to deploy the model, the same can be done programmatically. Navigate to the left-hand (\u2630) hamburger menu and click on Deployments . Click on the Spaces tab and then choose the deployment space you setup previously by clicking on the name of your space. From your deployment space overview, in the table, find the model name for the model you previously built and now want to create a deployment against. Use your mouse to hover over the right side of that table row and click the Deploy rocket icon (the icons are not visible by default until you hover over them). Note: There may be more than one model listed in the 'Models' section. This can happen if you have run the Jupyter notebook more than once or if you have run through both the Jupyter notebook and AutoAI modules to create models. Although you could select any of the models you see listed in the page, the recommendation is to start with whichever model is available that is using a spark-mllib_2.4 software specification. On the 'Create a deployment' screen, choose Online for the Deployment Type , give the Deployment a name and optional description and click the Create button. Click on the Deployments tab. The online deployment will show as In progress and then switch to Deployed when done.","title":"Online Model Deployment"},{"location":"machine-learning-deployment-scoring/#test-online-model-deployment","text":"Cloud Pak for Data offers tools to quickly test out Watson Machine Learning models. We begin with the built-in tooling. From the Model deployment page, once the deployment status shows as Deployed , click on the name of your deployment. The deployment API reference tab shows how to use the model using cURL , Java , Javascript , Python , and Scala . To get to the built-in test tool, click on the Test tab and then click on the Provide input data as JSON icon. Copy and paste the following data objects into the Body panel (replace the text that was in the input panel). Note: Click the tab appropriate for the model you are testing (either an AutoAI model or one built using the Jupyter notebook). Also make sure the input below is the only content in the field. Do not append it to the default content { \"input_data\": [] } that may already be in the test input panel. Jupyter Spark Model { \"input_data\" : [{ \"fields\" : [ \"CheckingStatus\" , \"LoanDuration\" , \"CreditHistory\" , \"LoanPurpose\" , \"LoanAmount\" , \"ExistingSavings\" , \"EmploymentDuration\" , \"InstallmentPercent\" , \"Sex\" , \"OthersOnLoan\" , \"CurrentResidenceDuration\" , \"OwnsProperty\" , \"Age\" , \"InstallmentPlans\" , \"Housing\" , \"ExistingCreditsCount\" , \"Job\" , \"Dependents\" , \"Telephone\" , \"ForeignWorker\" ], \"values\" : [[ \"no_checking\" , 13 , \"credits_paid_to_date\" , \"car_new\" , 1343 , \"100_to_500\" , \"1_to_4\" , 2 , \"female\" , \"none\" , 3 , \"savings_insurance\" , 46 , \"none\" , \"own\" , 2 , \"skilled\" , 1 , \"none\" , \"yes\" ]] }]} AutoAI Model { \"input_data\" : [{ \"fields\" : [ \"CustomerId\" , \"CheckingStatus\" , \"LoanDuration\" , \"CreditHistory\" , \"LoanPurpose\" , \"LoanAmount\" , \"ExistingSavings\" , \"EmploymentDuration\" , \"InstallmentPercent\" , \"Sex\" , \"OthersOnLoan\" , \"CurrentResidenceDuration\" , \"OwnsProperty\" , \"Age\" , \"InstallmentPlans\" , \"Housing\" , \"ExistingCreditsCount\" , \"Job\" , \"Dependents\" , \"Telephone\" , \"ForeignWorker\" ], \"values\" : [[ \"\" , \"no_checking\" , 13 , \"credits_paid_to_date\" , \"car_new\" , 1343 , \"100_to_500\" , \"1_to_4\" , 2 , \"female\" , \"none\" , 3 , \"savings_insurance\" , 46 , \"none\" , \"own\" , 2 , \"skilled\" , 1 , \"none\" , \"yes\" ]] }]} Click the Predict button. The model will be called with the input data and the results will display in the Result window. Scroll down to the bottom of the result to see the prediction (i.e \"Risk\" or \"No Risk\"): Note: For some deployed models (for example AutoAI based models), you can provide the request payload using a generated form by clicking on the Provide input using form icon and providing values for the input fields of the form. If the form is not available for the model you deployed, the icon will not be displayed.","title":"Test Online Model Deployment"},{"location":"machine-learning-deployment-scoring/#optional-test-online-model-deployment-using-curl","text":"Now that the model is deployed, we can also test it from external applications. One way to invoke the model API is using the cURL command. NOTE: Windows users will need the cURL command. It's recommended to download gitbash for this, as you'll also have other tools and you'll be able to easily use the shell environment variables in the following steps. Also note that if you are not using gitbash, you may need to change export commands to set commands. In a terminal window (or command prompt in Windows), run the following command to get a token to access the API. Replace <username> and <password> with the username and password you used to log into the Cloud pak for data cluster. Replace <cluster-url> with just the hostname of the cloud pak for data cluster (i.e the url from your web browser address bar) curl -k -X GET https://<cluster-url>/v1/preauth/validateAuth -u <username>:<password> A json string will be returned with a value for \"accessToken\" that will look similar to this: { \"username\" : \"scottda\" , \"role\" : \"Admin\" , \"permissions\" :[ \"access_catalog\" , \"administrator\" , \"manage_catalog\" , \"can_provision\" ], \"sub\" : \"scottda\" , \"iss\" : \"KNOXSSO\" , \"aud\" : \"DSX\" , \"uid\" : \"1000331002\" , \"authenticator\" : \"default\" , \"accessToken\" : \"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6InNjb3R0ZGEiLCJyb2xlIjoiQWRtaW4iLCJwZXJtaXNzaW9ucyI6WyJhY2Nlc3NfY2F0YWxvZyIsImFkbWluaXN0cmF0b3IiLCJtYW5hZ2VfY2F0YWxvZyIsImNhbl9wcm92aXNpb24iXSwic3ViIjoic2NvdHRkYSIsImlzcyI6IktOT1hTU08iLCJhdWQiOiJEU1giLCJ1aWQiOiIxMDAwMzMxMDAyIiwiYXV0aGVudGljYXRvciI6ImRlZmF1bHQiLCJpYXQiOjE1NzM3NjM4NzYsImV4cCI6MTU3MzgwNzA3Nn0.vs90XYeKmLe0Efi5_3QV8F9UK1tjZmYIqmyCX575I7HY1QoH4DBhon2fa4cSzWLOM7OQ5Xm32hNUpxPH3xIi1PcxAntP9jBuM8Sue6JU4grTnphkmToSlN5jZvJOSa4RqqhjzgNKFoiqfl4D0t1X6uofwXgYmZESP3tla4f4dbhVz86RZ8ad1gS1_UNI-w8dfdmr-Q6e3UMDUaahh8JaAEiSZ_o1VTMdVPMWnRdD1_F0YnDPkdttwBFYcM9iSXHFt3gyJDCLLPdJkoyZFUa40iRB8Xf5-iA1sxGCkhK-NVHh-VTS2XmKAA0UYPGYXmouCTOUQHdGq2WXF7PkWQK0EA\" , \"_messageCode_\" : \"success\" , \"message\" : \"success\" } You will save this access token to a temporary environment variable in your terminal. Copy the access token value (without the quotes) in the terminal and then use the following export command to save the \"accessToken\" to a variable called WML_AUTH_TOKEN . export WML_AUTH_TOKEN = <value-of-access-token> Back on the model deployment page, gather the URL to invoke the deployed model from the API reference by copying the Endpoint . Now save that endpoint to a variable named URL in your terminal by exporting it. export URL = <value-of-endpoint> Now run this curl command from the terminal to invoke the model with the same payload we used previousy: curl -k -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' --header \"Authorization: Bearer $WML_AUTH_TOKEN \" -d '{\"input_data\": [{\"fields\": [ \"CheckingStatus\", \"LoanDuration\", \"CreditHistory\", \"LoanPurpose\", \"LoanAmount\", \"ExistingSavings\", \"EmploymentDuration\", \"InstallmentPercent\", \"Sex\", \"OthersOnLoan\", \"CurrentResidenceDuration\", \"OwnsProperty\", \"Age\", \"InstallmentPlans\", \"Housing\", \"ExistingCreditsCount\", \"Job\", \"Dependents\", \"Telephone\", \"ForeignWorker\"],\"values\": [[ \"no_checking\", 13, \"credits_paid_to_date\", \"car_new\", 1343, \"100_to_500\", \"1_to_4\", 2, \"female\", \"none\", 3, \"savings_insurance\", 46, \"none\", \"own\", 2, \"skilled\", 1, \"none\", \"yes\"]]}]}' $URL A json string will be returned with the response, including a prediction from the model (i.e a \"Risk\" or \"No Risk\" at the end indicating the prediction of this loan representing risk).","title":"(Optional) Test Online Model Deployment using cURL"},{"location":"machine-learning-deployment-scoring/#optional-batch-model-deployment","text":"Another approach to expose the model to be consumed by other users/applications is to create a batch deployment. This type of deployment will make an instance of the model available to make predictions against data assets or groups of records. The model prediction requests are scheduled as jobs, which are exected asynchronously. For the lab, we will break this into two steps: Creating the deployment (which we will do using the UI). Creating and scheduling a job with values (which we will do using a Jupyter Notebook). Lets start by creating the deployment: Navigate to the left-hand (\u2630) hamburger menu and click on Deployments . Click on the Spaces tab and then choose the deployment space you setup previously by clicking on the name of your space. From your deployment space overview, in the table, find the model name for the model you previously built and now want to create a deployment against. Use your mouse to hover over the right side of that table row and click the Deploy rocket icon (the icons are not visible by default until you hover over them). Note: There may be more than one model listed in them 'Models' section. This can happen if you have run the Jupyter notebook more than once or if you have run through both the Jupyter notebook and AutoAI modules to create models. Although you could select any of the models you see listed in the page, the recommendation is to start with whicever model is available that is using a spark-mllib_2.4 software specification. On the 'Create a deployment' screen: choose Batch for the Deployment Type , give the deployment a name and optional description. From the 'Hardware definition' drop down, select the smallest option ( 1 standard CPU, 4GB RAM in this case though for large or frequent batch jobs, you might choose to scale the hardware up). Click the Create button. Once the status shows as Deployed you will be able to start submitting jobs to the deployment.","title":"(Optional) Batch Model Deployment"},{"location":"machine-learning-deployment-scoring/#create-and-schedule-a-job","text":"Next we can schedule a job to run against our batch deployment. We could create a job, with specific input data (or data asset) and schedule, either programmatically or through the UI. For this lab, we are going to do this programmatically using the Python client SDK. For this part of the exercise we're going to use a Jupyter notebook to create and submit a batch job to our model deployment. Note: The batch job input is impacted by the machine learning framework used to build the model. Currently, SparkML based model batch jobs require inline payload to be used. For other frameworks, we can use data assets (i.e CSV files) as the input payload.","title":"Create and Schedule a Job"},{"location":"machine-learning-deployment-scoring/#run-the-batch-notebook","text":"The Jupyter notebook is already included as an asset in the project you imported earlier. Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click the project name you created in the pre-work section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the machinelearning-creditrisk-sparkmlmodel notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.","title":"Run the Batch Notebook"},{"location":"machine-learning-deployment-scoring/#notebook-sections","text":"With the notebook open, spend a minute looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Please note that some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. Section 1.0 Install required packages will install some of the libraries we are going to use in the notebook (many libraries come pre-installed on Cloud Pak for Data). Note that we upgrade the installed version of Watson Machine Learning Python Client. Ensure the output of the first code cell is that the python packages were successfully installed. Section 2.0 Create Batch Deployment Job will create a job for the batch deployment. To do that, we will use the Watson Machine Learning client to get our deployment and create a job. In the first code cell for Section2.1 , be sure to update the wml_credentials variable. The url should be the hostname of the Cloud Pak for Data instance. The username and password should be the same credentials you used to log into Cloud Pak for Data. In section 2.2, be sure to update the DEPLOYMENT_SPACE_NAME variable with your deployment space name (copy and past the name which is within the output of the previous code cell). In section 2.3, be sure to update the DEPLOYMENT_NAME variable with the name of the batch deployment you created previously (copy and past the name which is within the output of the previous code cell). Continue to run the rest of the cells in section 2 which will load the batch input data set and create the job. The last code cell in section 2 will show that the job is in a queued state. Section 3.0 Monitor Batch Job Status will start polling the job status until it completes or fails. The code cell will output the status every 5 seconds as the job goes from queued to running to completed or failed. Once the job completes, continue to run the cells until the end of the notebook.","title":"Notebook sections"},{"location":"machine-learning-deployment-scoring/#cleanup-and-stop-environment","text":"Important : In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the Environments tab near the top of the page. Then in the Active environment runtimes section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window.","title":"Cleanup and Stop Environment"},{"location":"machine-learning-deployment-scoring/#optional-integrate-model-to-python-flask-application","text":"You can also access the online model deployment directly through the REST API. This allows you to use your model for inference in any of your apps. For this workshop we'll be using a Python Flask application to collect information, score it against the model, and show the results. IMPORTANT: This SAMPLE application only runs on python 3.6 and above, so the instructions here are for python 3.6+ only. You will need to have Python 3.6 or later already installed on your machine Note: The instructions below assume you have completed the pre-work module and thus have the Git repository already on your machine (cloned or downloaded).","title":"(Optional) Integrate Model to Python Flask Application"},{"location":"machine-learning-deployment-scoring/#install-dependencies","text":"The general recommendation for Python development is to use a virtual environment ( venv ). To install and initialize a virtual environment, use the venv module on Python 3: Initialize a virtual environment with venv . Run the following commands in a terminal (or command prompt): # Create the virtual environment using Python. # Note, it may be named python3 on your system. python -m venv venv # Python 3.X # Source the virtual environment. Use one of the two commands depending on your OS. source venv/bin/activate # Mac or Linux ./venv/Scripts/activate # Windows PowerShell TIP To terminate the virtual environment use the deactivate command. Unzip the python application zip file that you downloaded in the pre-work section. To install the Python requirements, from a terminal (or command prompt) navigate to where you unzipped the python application. Run the following commands: pip install -r requirements.txt","title":"Install Dependencies"},{"location":"machine-learning-deployment-scoring/#update-environment-variables","text":"It's best practice to store configurable information as environment variables, instead of hard-coding any important information. To reference our model and supply an API key, we'll pass these values in via a file that is read, the key-value pairs in this files are stored as environment variables. Copy the env.sample file to .env . cp env.sample .env Edit .env to and fill in the MODEL_URL as well as the AUTH_URL , AUTH_USERNAME , and AUTH_PASSWORD . MODEL_URL is your web service URL for scoring which you got from the section above AUTH_URL is the preauth url of your CloudPak4Data and will look like this: https://<cluster_url>/v1/preauth/validateAuth AUTH_USERNAME is your username with which you login to the CloudPak4Data environment AUTH_PASSWORD is your password with which you login to the CloudPak4Data environment Note: Alternatively, you can fill in the AUTH_TOKEN instead of AUTH_URL , AUTH_USERNAME , and AUTH_PASSWORD . You will have generated this token in the section above. However, since tokens expire after a few hours and you would need to restart your app to update the token, this option is not suggested. Instead, if you use the username/password option, the app can generate a new token every time for you so it will always have a non-expired ones. Here's an example of a completed lines of the .env file. # Required: Provide your web service URL for scoring. # E.g., MODEL_URL=https://<cluster_url>/v4/deployments/<deployment_space_guid>/predictions MODEL_URL = https://cp4d.cp4dworkshops.com/v4/deployments/5f939979-14c2-4538-a2af-a970aeb59abd/predictions # Required: Please fill in EITHER section A OR B below: # #### A: Authentication using username and password # Fill in the authntication url, your CloudPak4Data username, and CloudPak4Data password. # Example: # AUTH_URL=<cluster_url>/v1/preauth/validateAuth # AUTH_USERNAME=my_username # AUTH_PASSWORD=super_complex_password AUTH_URL = https://cp4d.cp4dworkshops.com/v1/preauth/validateAuth AUTH_USERNAME = username_001 AUTH_PASSWORD = my_secure_password_!","title":"Update Environment Variables"},{"location":"machine-learning-deployment-scoring/#start-application","text":"Start the flask server by running the following command: python creditriskapp.py Use your browser to go to http://0.0.0.0:5000 and try it out. TIP : Use ctrl + c to stop the Flask server when you are done.","title":"Start Application"},{"location":"machine-learning-deployment-scoring/#test-the-application","text":"Either use the default values pre-filled in the input form, or modify the value and then click the Submit button. The python application will invoke the predictive model and a risk prediction & probability is returned:","title":"Test the application"},{"location":"machine-learning-deployment-scoring/#conclusion","text":"In this section we covered the followings: Creating and Testing Online Deployments for models. (Optional) Creating and Testing Batch Deployments for models. (Optional) Integrating the model deployment in an external application. Taking a predictive model and infusing AI into applications.","title":"Conclusion"},{"location":"machine-learning-in-jupyter-notebook/","text":"Machine Learning in Jupyter Notebook \u00b6 In this module, we will go through the process of exploring our data set and building a predictive model that can be used to determine the likelyhood of a credit loan having 'Risk' or 'No Risk'. For this use case, the machine learning model we are building is a classification model that will return a prediction of 'Risk' (the features of the loan applicant predict that there is a good chance of default on the loan) or No Risk (the applicant's inputs predict that the loan will be paid off). The approach we will take in this lab is to some fairly popular libraries / frameworks to build the model in Python using a Jupyter notebook. Once we have built the model, we will make it available for deployment so that it can be used by others. Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks. Build and Save a model \u00b6 For this part of the exercise we're going to use a Jupyter notebook to create the model. The Jupyter notebook is already included as an asset in the project you imported earlier. Open the Jupyter notebook \u00b6 Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click the project name you created in the pre-work section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the machinelearning-creditrisk-sparkmlmodel notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section. Run the Jupyter notebook \u00b6 Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell ( Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Note: Some of the comments in the notebook (those in bold red) are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. Load and Prepare Dataset \u00b6 Section 1.0 Install required packages will install some of the libraries we are going to use in the notebook (many libraries come pre-installed on Cloud Pak for Data). Note that we upgrade the installed version of Watson Machine Learning Python Client. Ensure the output of the first code cell is that the python packages were successfully installed. Run the code cells in section 1.1 and 1.2. Ensuring that the cells complete before continuing. Section 2.0 Load and Clean data will load the data set we will use to build out machine learning model. In order to import the data into the notebook, we are going to use the code generation capability of Watson Studio. Highlight the code cell below by clicking it. Ensure you place the cursor below the first comment line. Click the 01/00 \"Find data\" icon in the upper right of the notebook to find the data asset you need to import. If you are using virtualized data, then choose your virtualized merged view (i.e. USERXXXX.APPLICANTFINANCIALPERSONALLOANSDATA ). If you are using this notebook without virtualized data, you can use the german_credit_data.csv CSV file version of the data set that has been included in the project. For your dataset, Click Insert to code and choose Insert Pandas DataFrame . The code to bring the data into the notebook environment and create a Pandas DataFrame will be added to the cell below. Run the cell and you will see the first five rows of our dataset. Since we are using generated code to import the data, you will need to update the next cell to assign the df variable. Copy the variable that was generated in the previous cell ( it will look like df=data_df_1 , data_df_2 , etc) and assign it to the df variable (for example df=df_data_1 ). Continue to run the remaining cells in section 2 to explore and clean the data. Build Machine Learning Model \u00b6 Section 3.0 Create a model cells will run through the steps to build a model pipeline. We will split our data into training and test data, encode the categorial string values, create a model using the Random Forest Classifier algorithm, and evaluate the model against the test set. Run all the cells in section 3 to build the model. Save the model \u00b6 Section 4.0 Save the model will save the model to your project. We will be saving and deploying the model to the Watson Machine Learning service within our Cloud Pak for Data platform. In the first code cell in section 4.1, be sure to update the wml_credentials variable as follows: The url should be the full hostname of the Cloud Pak for Data instance, which you can copy from your browsers address bar (for example, it may look like this: https://zen.clustername.us-east.containers.appdomain.cloud ) The username and password should be the same credentials you used to log into Cloud Pak for Data. You will update the MODEL_NAME and DEPLOYMENT_SPACE_NAME variables. For the MODEL_NAME , create a unique and easily identifiable model name. For the DEPLOYMENT_SPACE_NAME , copy the name of your deployment space which was output in the previous code cell. MODEL_NAME = \"user123 credit risk model\" DEPLOYMENT_SPACE_NAME = \"Name you used for deployment space\" Continue to run the cells in the section to save the model to Cloud Pak for Data. Once your model is saved, the call to wml_client.repository.list_models() will show it in the output. We've successfully built and saved a machine learning model programmatically. Congratulations! Stop the Environment \u00b6 Important : In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window. Conclusion \u00b6 In this section we covered one approach to building machine learning models on Cloud Pak for Data. We have seen:: How to build a model using Jupyter Notebook Saving models using the Watson Machine Learning SDK. With this knowledge you should feel right at home within the Jupyter notebook. Moreover, you now know how to build a model and use it in a real life scenario.","title":"Machine Learning with Jupyter"},{"location":"machine-learning-in-jupyter-notebook/#machine-learning-in-jupyter-notebook","text":"In this module, we will go through the process of exploring our data set and building a predictive model that can be used to determine the likelyhood of a credit loan having 'Risk' or 'No Risk'. For this use case, the machine learning model we are building is a classification model that will return a prediction of 'Risk' (the features of the loan applicant predict that there is a good chance of default on the loan) or No Risk (the applicant's inputs predict that the loan will be paid off). The approach we will take in this lab is to some fairly popular libraries / frameworks to build the model in Python using a Jupyter notebook. Once we have built the model, we will make it available for deployment so that it can be used by others. Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks.","title":"Machine Learning in Jupyter Notebook"},{"location":"machine-learning-in-jupyter-notebook/#build-and-save-a-model","text":"For this part of the exercise we're going to use a Jupyter notebook to create the model. The Jupyter notebook is already included as an asset in the project you imported earlier.","title":"Build and Save a model"},{"location":"machine-learning-in-jupyter-notebook/#open-the-jupyter-notebook","text":"Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click the project name you created in the pre-work section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the machinelearning-creditrisk-sparkmlmodel notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.","title":"Open the Jupyter notebook"},{"location":"machine-learning-in-jupyter-notebook/#run-the-jupyter-notebook","text":"Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell ( Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Note: Some of the comments in the notebook (those in bold red) are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell.","title":"Run the Jupyter notebook"},{"location":"machine-learning-in-jupyter-notebook/#load-and-prepare-dataset","text":"Section 1.0 Install required packages will install some of the libraries we are going to use in the notebook (many libraries come pre-installed on Cloud Pak for Data). Note that we upgrade the installed version of Watson Machine Learning Python Client. Ensure the output of the first code cell is that the python packages were successfully installed. Run the code cells in section 1.1 and 1.2. Ensuring that the cells complete before continuing. Section 2.0 Load and Clean data will load the data set we will use to build out machine learning model. In order to import the data into the notebook, we are going to use the code generation capability of Watson Studio. Highlight the code cell below by clicking it. Ensure you place the cursor below the first comment line. Click the 01/00 \"Find data\" icon in the upper right of the notebook to find the data asset you need to import. If you are using virtualized data, then choose your virtualized merged view (i.e. USERXXXX.APPLICANTFINANCIALPERSONALLOANSDATA ). If you are using this notebook without virtualized data, you can use the german_credit_data.csv CSV file version of the data set that has been included in the project. For your dataset, Click Insert to code and choose Insert Pandas DataFrame . The code to bring the data into the notebook environment and create a Pandas DataFrame will be added to the cell below. Run the cell and you will see the first five rows of our dataset. Since we are using generated code to import the data, you will need to update the next cell to assign the df variable. Copy the variable that was generated in the previous cell ( it will look like df=data_df_1 , data_df_2 , etc) and assign it to the df variable (for example df=df_data_1 ). Continue to run the remaining cells in section 2 to explore and clean the data.","title":"Load and Prepare Dataset"},{"location":"machine-learning-in-jupyter-notebook/#build-machine-learning-model","text":"Section 3.0 Create a model cells will run through the steps to build a model pipeline. We will split our data into training and test data, encode the categorial string values, create a model using the Random Forest Classifier algorithm, and evaluate the model against the test set. Run all the cells in section 3 to build the model.","title":"Build Machine Learning Model"},{"location":"machine-learning-in-jupyter-notebook/#save-the-model","text":"Section 4.0 Save the model will save the model to your project. We will be saving and deploying the model to the Watson Machine Learning service within our Cloud Pak for Data platform. In the first code cell in section 4.1, be sure to update the wml_credentials variable as follows: The url should be the full hostname of the Cloud Pak for Data instance, which you can copy from your browsers address bar (for example, it may look like this: https://zen.clustername.us-east.containers.appdomain.cloud ) The username and password should be the same credentials you used to log into Cloud Pak for Data. You will update the MODEL_NAME and DEPLOYMENT_SPACE_NAME variables. For the MODEL_NAME , create a unique and easily identifiable model name. For the DEPLOYMENT_SPACE_NAME , copy the name of your deployment space which was output in the previous code cell. MODEL_NAME = \"user123 credit risk model\" DEPLOYMENT_SPACE_NAME = \"Name you used for deployment space\" Continue to run the cells in the section to save the model to Cloud Pak for Data. Once your model is saved, the call to wml_client.repository.list_models() will show it in the output. We've successfully built and saved a machine learning model programmatically. Congratulations!","title":"Save the model"},{"location":"machine-learning-in-jupyter-notebook/#stop-the-environment","text":"Important : In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window.","title":"Stop the Environment"},{"location":"machine-learning-in-jupyter-notebook/#conclusion","text":"In this section we covered one approach to building machine learning models on Cloud Pak for Data. We have seen:: How to build a model using Jupyter Notebook Saving models using the Watson Machine Learning SDK. With this knowledge you should feel right at home within the Jupyter notebook. Moreover, you now know how to build a model and use it in a real life scenario.","title":"Conclusion"},{"location":"openscale-fastpath/","text":"Monitoring models with OpenScale GUI tool using Auto setup \u00b6 This exercise shows a few of the features of the OpenScale GUI tool. When you first provision Watson OpenScale, either in the IBM Cloud or on Cloud Pak for Data, you will be offered the choice to automatically configure and setup OpenScale. This is called the Auto setup, and it walks the admin through the required steps and loads some sample data to demonstrate the features of OpenScale. We will use this automated Auto setup in this lab. It is presumed that OpenScale Auto setup and Watson Machine Learning have already been configured. Use the Insights Dashboard \u00b6 To launch the OpenScale service, go the (\u2630) navigation menu and click Services -> Instances . Click the 3 horizontal dots next to the OpenScale instance that your Administrator has provisioned and click Open . Now lets interact with the tools. OpenScale will load the Insights Dashboard . This will contain tiles for any models being monitored. The tile for GermanCreditRiskModelICP will be the one we will use for this lab, which was configured using the Auto setup script. Click on the left-hand menu icon for Insights , make sure that you are on the Model monitors tab, and then open the tile for the GermanCreditRiskModelICP model (click the 3-dot menu on the tile and then View Details ): Notice the red alert indicators on the various monitors (Fairness, Quality, Drift). You should see a red indicator under Fairness. Click on the Fairness score . Click on the triangle with ! under Fairness -> Sex . This indicates that there has been an alert for this attribute in the Fairness monitor. Alerts are configurable, based on thresholds for fairness outcomes which can be set and altered as desired. By moving your mouse pointer over the trend chart, you can see the values change, and which contains bias. Find and click on a spot in the graph that is below the red threshold line to view details. Once you click on one of the time periods, you will see details of the Fairness monitor, including a bar chart that shows how many females recieved the \"No Risk\" outcome vs. males. You can click view calculation to see how the fairness score is calculated. Click on View payload transactions . You will see a list of Transactions. Look for one of the Monitored Group - Female with a \"Group Bias\" check mark and Prediction of \"Risk\". Click Explain prediction . If the time period on the graph for Fairness Monitoring doesn't contain such an element, go back and choose another time period until you can find one. This will make the explanation more interesting. Note: Each of the individual transactions can be examined to see them in detail. Doing so will cache that transaction, as we will see later. Be aware of the fact that the Explainability feature requires 1000's of REST calls to the endpoint using variations of the data that are slightly perturbed, which can require several seconds to complete. On the Explain tab for this individual transaction, you can see the relative weights of the most important features for this prediction. Examine the data, then click the Inspect tab. In the Inspect view of this transaction you can see the original features that led to this prediction as well as a series of drop downs and input boxes that offer the ability to change each feature. We can find which features will change the outcome (in this case, from \"Risk\" to \"No Risk\") by clicking the Analysis button. Note that this requires 1000's of REST calls to the endpoint with slight perturbations in the data, so it can take a few minutes. Click the Analysis tab now. In this particular transaction, we see that the presence of a \"guarantor\" on the loan is the only thing required to flip the outcome from \"Risk\" to \"No Risk\". Other transactions might show a different analysis, so please be aware that your results might vary from this. In the case in this example, you can click the drop down for Others on Loan and change to guarantor . Choosing this new value for gurantor will expose a button for Score new values . Click this button. In this example, we can see that the outcome has now been flipped from \"Risk\" to \"No Risk\". Now, go back to the Insights Dashboard page by clicking on the left-hand menu icon for Insights , make sure that you are on the Model monitors tab. This time open the monitor configuration for the GermanCreditRiskModelICP model by clicking the 3-dot menu on the tile and then Configure monitors . Click the Endpoints menu on the left, then the Endpoints tab. Use the Endpoint pulldown to select Debiased transactions . This is the REST endpoint that offers a debiased version of the credit risk ML model, based on the features that were configured (i.e. Sex and Age). It will present an inference that attempts to remove the bias that has been detected. You can see code snippets using cURL, Java, and Python, which can be used in your scripts or applications. Similarly, you can choose the Feedback logging endpoint to get code for Feedback Logging. This provides an endpoint for sending fresh test data for ongoing quality evaluation. You can upload feedback data here or work with your developer to integrate the code snippet provided to publish feedback data to your Watson OpenScale database. Using the Analytics tools \u00b6 Click on the left-hand menu icon for Insights , make sure that you are on the Model monitors tab, and then open the tile for the GermanCreditRiskModelICP model (click the 3-dot menu on the tile and then View Details ): Notice the red alert indicators on the various monitors (Fairness, Quality, Drift). You should see a red indicator under Fairness. Click on the Fairness score . Click on Analytics -> Predictions by Confidence . It may take a minute or more to create the chart. Here you can see a bar chart that indicates confidence levels and predictions of \"Risk\" and \"No Risk\". From this dashboard click on Analytics -> Chart Builder . Here you can create charts using various Measurements, Features, and Dimensions of your machine learning model. You can see a chart that breaks down Predictions by Confidence Note: You may need to click the date range for 'Past Week' or 'Yesterday' to load the data. You can experiment with changing the values and examine the charts that are created. Conclusion \u00b6 This lab provides a walkthrough of many of the GUI features using the Watson OpenScale tools. The Auto setup deployment creates a machine learning model, deploys it, and inserts historical data to simulate a model that has been used in production over time. The OpenScale monitors are configured, and the user can then explore the various metrics and data. Please continue to explore on your own.","title":"Monitoring models with OpenScale GUI (Auto setup Monitoring)"},{"location":"openscale-fastpath/#monitoring-models-with-openscale-gui-tool-using-auto-setup","text":"This exercise shows a few of the features of the OpenScale GUI tool. When you first provision Watson OpenScale, either in the IBM Cloud or on Cloud Pak for Data, you will be offered the choice to automatically configure and setup OpenScale. This is called the Auto setup, and it walks the admin through the required steps and loads some sample data to demonstrate the features of OpenScale. We will use this automated Auto setup in this lab. It is presumed that OpenScale Auto setup and Watson Machine Learning have already been configured.","title":"Monitoring models with OpenScale GUI tool using Auto setup"},{"location":"openscale-fastpath/#use-the-insights-dashboard","text":"To launch the OpenScale service, go the (\u2630) navigation menu and click Services -> Instances . Click the 3 horizontal dots next to the OpenScale instance that your Administrator has provisioned and click Open . Now lets interact with the tools. OpenScale will load the Insights Dashboard . This will contain tiles for any models being monitored. The tile for GermanCreditRiskModelICP will be the one we will use for this lab, which was configured using the Auto setup script. Click on the left-hand menu icon for Insights , make sure that you are on the Model monitors tab, and then open the tile for the GermanCreditRiskModelICP model (click the 3-dot menu on the tile and then View Details ): Notice the red alert indicators on the various monitors (Fairness, Quality, Drift). You should see a red indicator under Fairness. Click on the Fairness score . Click on the triangle with ! under Fairness -> Sex . This indicates that there has been an alert for this attribute in the Fairness monitor. Alerts are configurable, based on thresholds for fairness outcomes which can be set and altered as desired. By moving your mouse pointer over the trend chart, you can see the values change, and which contains bias. Find and click on a spot in the graph that is below the red threshold line to view details. Once you click on one of the time periods, you will see details of the Fairness monitor, including a bar chart that shows how many females recieved the \"No Risk\" outcome vs. males. You can click view calculation to see how the fairness score is calculated. Click on View payload transactions . You will see a list of Transactions. Look for one of the Monitored Group - Female with a \"Group Bias\" check mark and Prediction of \"Risk\". Click Explain prediction . If the time period on the graph for Fairness Monitoring doesn't contain such an element, go back and choose another time period until you can find one. This will make the explanation more interesting. Note: Each of the individual transactions can be examined to see them in detail. Doing so will cache that transaction, as we will see later. Be aware of the fact that the Explainability feature requires 1000's of REST calls to the endpoint using variations of the data that are slightly perturbed, which can require several seconds to complete. On the Explain tab for this individual transaction, you can see the relative weights of the most important features for this prediction. Examine the data, then click the Inspect tab. In the Inspect view of this transaction you can see the original features that led to this prediction as well as a series of drop downs and input boxes that offer the ability to change each feature. We can find which features will change the outcome (in this case, from \"Risk\" to \"No Risk\") by clicking the Analysis button. Note that this requires 1000's of REST calls to the endpoint with slight perturbations in the data, so it can take a few minutes. Click the Analysis tab now. In this particular transaction, we see that the presence of a \"guarantor\" on the loan is the only thing required to flip the outcome from \"Risk\" to \"No Risk\". Other transactions might show a different analysis, so please be aware that your results might vary from this. In the case in this example, you can click the drop down for Others on Loan and change to guarantor . Choosing this new value for gurantor will expose a button for Score new values . Click this button. In this example, we can see that the outcome has now been flipped from \"Risk\" to \"No Risk\". Now, go back to the Insights Dashboard page by clicking on the left-hand menu icon for Insights , make sure that you are on the Model monitors tab. This time open the monitor configuration for the GermanCreditRiskModelICP model by clicking the 3-dot menu on the tile and then Configure monitors . Click the Endpoints menu on the left, then the Endpoints tab. Use the Endpoint pulldown to select Debiased transactions . This is the REST endpoint that offers a debiased version of the credit risk ML model, based on the features that were configured (i.e. Sex and Age). It will present an inference that attempts to remove the bias that has been detected. You can see code snippets using cURL, Java, and Python, which can be used in your scripts or applications. Similarly, you can choose the Feedback logging endpoint to get code for Feedback Logging. This provides an endpoint for sending fresh test data for ongoing quality evaluation. You can upload feedback data here or work with your developer to integrate the code snippet provided to publish feedback data to your Watson OpenScale database.","title":"Use the Insights Dashboard"},{"location":"openscale-fastpath/#using-the-analytics-tools","text":"Click on the left-hand menu icon for Insights , make sure that you are on the Model monitors tab, and then open the tile for the GermanCreditRiskModelICP model (click the 3-dot menu on the tile and then View Details ): Notice the red alert indicators on the various monitors (Fairness, Quality, Drift). You should see a red indicator under Fairness. Click on the Fairness score . Click on Analytics -> Predictions by Confidence . It may take a minute or more to create the chart. Here you can see a bar chart that indicates confidence levels and predictions of \"Risk\" and \"No Risk\". From this dashboard click on Analytics -> Chart Builder . Here you can create charts using various Measurements, Features, and Dimensions of your machine learning model. You can see a chart that breaks down Predictions by Confidence Note: You may need to click the date range for 'Past Week' or 'Yesterday' to load the data. You can experiment with changing the values and examine the charts that are created.","title":"Using the Analytics tools"},{"location":"openscale-fastpath/#conclusion","text":"This lab provides a walkthrough of many of the GUI features using the Watson OpenScale tools. The Auto setup deployment creates a machine learning model, deploys it, and inserts historical data to simulate a model that has been used in production over time. The OpenScale monitors are configured, and the user can then explore the various metrics and data. Please continue to explore on your own.","title":"Conclusion"},{"location":"openscale-manual-config/","text":"WARNING: This Module is Unsupported and has been Deprecated. \u00b6 Trust in AI & Watson OpenScale \u00b6 This lab will demonstrate how to monitor your deployed machine learning model using Watson OpenScale. We will run several Jupyter notebooks to show the OpenScale APIs and how they configure various monitors. For each notebook, we'll use the OpenScale GUI tool to explore the results. We'll use several jupyter notebook and instructions: These instructions for basic OpenScale setup Fairness and Explainiblity monitors Quality monitor and Feedback logging Drift monitor Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks. Steps for basic OpenScale setup \u00b6 The submodule contains the following steps: Introduction Open the notebook Run the notebook Begin to Explore the Watson OpenScale UI 1. Introduction \u00b6 Watson OpenScale tracks and measures outcomes from your AI models, and helps ensure they remain fair, explainable and compliant wherever your models were built or are running. OpenScale is designed as an open platform that will operate with various model development environments and various open source tools, including TensorFlow, Keras, SparkML, Seldon, AWS SageMaker, AzureML and more. Watson OpenScale provides a set of monitoring and management tools that help you build trust and implement control and governance structures around your AI investments. Providing production monitoring for compliance and safeguards \\(auditing model decisions, detecting biases, etc\\) Ensuring that models are resilient to changing situations Aligning model performance with business outcomes In this lab will walk through the process of deploying a credit risk model and then monitoring the model to explore the different aspects of trusted AI. By the end of the lab, we will have: Deployed a model from development to a runtime environment. Monitored the performance \\(operational\\) of the model over time. Tracked the model quality \\(accuracy metrics\\) over time. Identified and explored the fairness of the model as it's receiving new data. Understood how the model arrived at its predictions. Tracked the robustness of the model. Prerequisites \u00b6 It is assumed that an admin has already connected a database to OpenScale, and associated a Machine Learning Provider (in our case, Watson Machine Learning on Cloud Pak for Data). You have already provided a set of sample data to your model when you tested your deployed ML model earlier in the workshop. For example, using the UI to test the deployed model, or using cURL or the Python app. Do this now if you have not already run a test . 2. Open the notebook \u00b6 Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and Click on the pencil icon at the right of the openscale-initial-setup notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section. 3. Run the Notebook \u00b6 Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Please note that there are several places in the notebook where you need to update variables. Some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. These changes are described below. WOS_CREDENTIALS \u00b6 In the notebook section 2.0 you will add your Cloud Pak for Data platform credentials for the WOS_CREDENTIALS . For the url , use the URL your Cloud Pak for Data cluster, i.e something like: \"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\" For the username , use your Cloud Pak for Data login username. For the password , user your Cloud Pak for Data login password. MODEL_NAME \u00b6 After running cell 5.1 containing ai_client.data_mart.bindings.list_assets() you should see the machine learning model that you deployed previously in the workshop. You will add the name of this model in cell 5.2 as MODEL_NAME default_space \u00b6 After running cell 6.1 containing wml_client.spaces.list() you should see the name of your deployment space. Use the GUID of this deployment space in cell 6.2 as default_space . DEPLOYMENT_NAME \u00b6 In section 6.3 set the DEPLOYMENT_NAME to the name that you gave to your Online deployment for the model during the 'Deploy and Test Machine Learning Modles' portion of the workshop. 4. Begin to Explore the Watson OpenScale UI \u00b6 Now that you have created a machine learning model and configured OpenScale with a subscription to that model deployment, you can utilize the OpenScale dashboard to monitor the model. Although we have not enabled any type of monitoring yet, with the deployment approach we are using for this lab \\( Watson Machine Learning as the model engine \\) , we will be able to see payload and some performance information out of the box. In the same browser \\(but a separate tab\\) , open the Services tab by clicking the Services icon on the top right. Find and click on the Watson OpenScale tile. Launch the OpenScale UI tooling by clicking on the Launch button When the Insights Dashboard loads, Click on the 'Model Monitors' tab. Here you will see tiles for all model subscriptions that are being monitored including the deployment you configured in the jupyter notebook when you ran it in the previous section: Note:Do not worry if the name you see does not match exactly with the screenshot. The subscription name you see will correspond to the variable used in the Jupyter notebook and the name you used when you deployed the model. At this point, it is normal for the subscription tile to show no monitors have ben configured (i.e N/A under Quality, Fairness, Drift) Confidence Distribution \u00b6 From the 'Model Monitors' tab, in the subscription tile you have created, click on one of the N/A values (i.e the N/A under the 'Fairness' heading). You will see some Analytics data, with the Date Range set to Today . We've just configured OpenScale to monitor our deployment, and sent a scoring request with 8 records, so there is not much here yet. We can see the distribution of confidence for those 8 predictions. If you hover over the bars you will see the number of 'Risk' and 'No Risk' predictions for each confidence range. Chart Builder \u00b6 Some additional data is present in the Chart Builder tab. Click on Analytics -> Chart Builder . Here you can create charts using various Measurements, Features, and Dimensions of your machine learning model. Experiment with different values from the drop downs and examine the charts that are created. Stop the Environment \u00b6 Important: When you have completed the last submodule in this \"openscale-manual-config\" section that you will be doing, it's recommended you stop the environment in order to conserve resources. You should only follow these steps to stop your environment if you are not going to proceed with the other sub-modules in this section. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window. Conclusion \u00b6 We begun the process of monitoring our machine learning deployment with openscale. At this point we have just a subscription from OpenScale for our deployed model. Proceed to the next sub-module to configure the Fairness and Explainability monitors.","title":"WARNING: This Module is Unsupported and has been Deprecated."},{"location":"openscale-manual-config/#warning-this-module-is-unsupported-and-has-been-deprecated","text":"","title":"WARNING: This Module is Unsupported and has been Deprecated."},{"location":"openscale-manual-config/#trust-in-ai-watson-openscale","text":"This lab will demonstrate how to monitor your deployed machine learning model using Watson OpenScale. We will run several Jupyter notebooks to show the OpenScale APIs and how they configure various monitors. For each notebook, we'll use the OpenScale GUI tool to explore the results. We'll use several jupyter notebook and instructions: These instructions for basic OpenScale setup Fairness and Explainiblity monitors Quality monitor and Feedback logging Drift monitor Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks.","title":"Trust in AI &amp; Watson OpenScale"},{"location":"openscale-manual-config/#steps-for-basic-openscale-setup","text":"The submodule contains the following steps: Introduction Open the notebook Run the notebook Begin to Explore the Watson OpenScale UI","title":"Steps for basic OpenScale setup"},{"location":"openscale-manual-config/#1-introduction","text":"Watson OpenScale tracks and measures outcomes from your AI models, and helps ensure they remain fair, explainable and compliant wherever your models were built or are running. OpenScale is designed as an open platform that will operate with various model development environments and various open source tools, including TensorFlow, Keras, SparkML, Seldon, AWS SageMaker, AzureML and more. Watson OpenScale provides a set of monitoring and management tools that help you build trust and implement control and governance structures around your AI investments. Providing production monitoring for compliance and safeguards \\(auditing model decisions, detecting biases, etc\\) Ensuring that models are resilient to changing situations Aligning model performance with business outcomes In this lab will walk through the process of deploying a credit risk model and then monitoring the model to explore the different aspects of trusted AI. By the end of the lab, we will have: Deployed a model from development to a runtime environment. Monitored the performance \\(operational\\) of the model over time. Tracked the model quality \\(accuracy metrics\\) over time. Identified and explored the fairness of the model as it's receiving new data. Understood how the model arrived at its predictions. Tracked the robustness of the model.","title":"1. Introduction"},{"location":"openscale-manual-config/#prerequisites","text":"It is assumed that an admin has already connected a database to OpenScale, and associated a Machine Learning Provider (in our case, Watson Machine Learning on Cloud Pak for Data). You have already provided a set of sample data to your model when you tested your deployed ML model earlier in the workshop. For example, using the UI to test the deployed model, or using cURL or the Python app. Do this now if you have not already run a test .","title":"Prerequisites"},{"location":"openscale-manual-config/#2-open-the-notebook","text":"Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and Click on the pencil icon at the right of the openscale-initial-setup notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.","title":"2. Open the notebook"},{"location":"openscale-manual-config/#3-run-the-notebook","text":"Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Please note that there are several places in the notebook where you need to update variables. Some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. These changes are described below.","title":"3. Run the Notebook"},{"location":"openscale-manual-config/#wos_credentials","text":"In the notebook section 2.0 you will add your Cloud Pak for Data platform credentials for the WOS_CREDENTIALS . For the url , use the URL your Cloud Pak for Data cluster, i.e something like: \"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\" For the username , use your Cloud Pak for Data login username. For the password , user your Cloud Pak for Data login password.","title":"WOS_CREDENTIALS"},{"location":"openscale-manual-config/#model_name","text":"After running cell 5.1 containing ai_client.data_mart.bindings.list_assets() you should see the machine learning model that you deployed previously in the workshop. You will add the name of this model in cell 5.2 as MODEL_NAME","title":"MODEL_NAME"},{"location":"openscale-manual-config/#default_space","text":"After running cell 6.1 containing wml_client.spaces.list() you should see the name of your deployment space. Use the GUID of this deployment space in cell 6.2 as default_space .","title":"default_space"},{"location":"openscale-manual-config/#deployment_name","text":"In section 6.3 set the DEPLOYMENT_NAME to the name that you gave to your Online deployment for the model during the 'Deploy and Test Machine Learning Modles' portion of the workshop.","title":"DEPLOYMENT_NAME"},{"location":"openscale-manual-config/#4-begin-to-explore-the-watson-openscale-ui","text":"Now that you have created a machine learning model and configured OpenScale with a subscription to that model deployment, you can utilize the OpenScale dashboard to monitor the model. Although we have not enabled any type of monitoring yet, with the deployment approach we are using for this lab \\( Watson Machine Learning as the model engine \\) , we will be able to see payload and some performance information out of the box. In the same browser \\(but a separate tab\\) , open the Services tab by clicking the Services icon on the top right. Find and click on the Watson OpenScale tile. Launch the OpenScale UI tooling by clicking on the Launch button When the Insights Dashboard loads, Click on the 'Model Monitors' tab. Here you will see tiles for all model subscriptions that are being monitored including the deployment you configured in the jupyter notebook when you ran it in the previous section: Note:Do not worry if the name you see does not match exactly with the screenshot. The subscription name you see will correspond to the variable used in the Jupyter notebook and the name you used when you deployed the model. At this point, it is normal for the subscription tile to show no monitors have ben configured (i.e N/A under Quality, Fairness, Drift)","title":"4. Begin to Explore the Watson OpenScale UI"},{"location":"openscale-manual-config/#confidence-distribution","text":"From the 'Model Monitors' tab, in the subscription tile you have created, click on one of the N/A values (i.e the N/A under the 'Fairness' heading). You will see some Analytics data, with the Date Range set to Today . We've just configured OpenScale to monitor our deployment, and sent a scoring request with 8 records, so there is not much here yet. We can see the distribution of confidence for those 8 predictions. If you hover over the bars you will see the number of 'Risk' and 'No Risk' predictions for each confidence range.","title":"Confidence Distribution"},{"location":"openscale-manual-config/#chart-builder","text":"Some additional data is present in the Chart Builder tab. Click on Analytics -> Chart Builder . Here you can create charts using various Measurements, Features, and Dimensions of your machine learning model. Experiment with different values from the drop downs and examine the charts that are created.","title":"Chart Builder"},{"location":"openscale-manual-config/#stop-the-environment","text":"Important: When you have completed the last submodule in this \"openscale-manual-config\" section that you will be doing, it's recommended you stop the environment in order to conserve resources. You should only follow these steps to stop your environment if you are not going to proceed with the other sub-modules in this section. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window.","title":"Stop the Environment"},{"location":"openscale-manual-config/#conclusion","text":"We begun the process of monitoring our machine learning deployment with openscale. At this point we have just a subscription from OpenScale for our deployed model. Proceed to the next sub-module to configure the Fairness and Explainability monitors.","title":"Conclusion"},{"location":"openscale-manual-config/DRIFT/","text":"WARNING: This Module is Unsupported and has been Deprecated. \u00b6 Configuring Drift Monitor for OpenScale \u00b6 Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks. Steps for Drift Monitor Configuration \u00b6 The submodule contains the following steps: Open the notebook Run the notebook Look at Drift in the Dashboard 1. Open the notebook \u00b6 If you Created the Project using the CreditRiskProject.zip file, your notebook will be present in that project. Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and Click on the pencil icon at the right of the openscale-drift-config notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section. 2. Run the Notebook \u00b6 Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Please note that there are several places in the notebook where you need to update variables. Some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. These changes are described below. WOS_CREDENTIALS \u00b6 In the notebook section 2.0 you will add your Cloud Pak for Data platform credentials for the WOS_CREDENTIALS . For the url , use the URL your Cloud Pak for Data cluster, i.e something like: \"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\" For the username , use your Cloud Pak for Data login username. For the password , user your Cloud Pak for Data login password. 3. Look at Drift in the Dashboard \u00b6 In the same browser \\(but a separate tab\\) , open the Services tab by clicking the Services icon on the top right. Find and click on the Watson OpenScale tile. Launch the OpenScale UI tooling by clicking on the Launch button When the dashboard loads, Click on the 'Model Monitors' tab and you will see the deployment you configured in the jupyter notebook when you ran it in the previous section. Click on the Drift section of the tile to bring up the Drift monitor . Note: Do not worry if the name you see does not match exactly with the screenshot. The deployment name you see will correspond to the variable used in the Jupyter notebook. Note: If you click on the card itself, you can get to the drift monitor page by clicking on the percentage shown for drift in the UI. Click on Drop in accuracy and then look for a time slot on the graph that shows bias (i.e. below the red threshold line). The monitor only runs every three hours, so there may only be one teal colored \"dot\" representing a single run when you first visit the graph. Click on it for more details that will show the number of transactions responsible \"for drop in accuracy\" and \"for drop in data consistency\". You can choose to get details about \"Transactions responsible for drop in accuracy and data consistency\": From here you can see groupings of transactions that caused drift. Where the groups are formed by shared features. Drilling deeper will bring up the individual transactions, and as we've seen before, we can choose a transaction to get the details. Stop the Environment \u00b6 Important: When you have completed the last submodule in this \"openscale-manual-config\" section that you will be doing, it's recommended you stop the environment in order to conserve resources. You should only follow these steps to stop your environment if you are not going to proceed with the other sub-modules in this section. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window. Conclusion \u00b6 We've seen how to configure Drift monitoring using a Jupyter notebook. Next, we'll add some historical data to emulate what would happen for a Machine learning model that is deployed in production, monitored with OpenScale, and continually receiving scoring requests. Proceed to the next sub-module to load historical data","title":"WARNING: This Module is Unsupported and has been Deprecated."},{"location":"openscale-manual-config/DRIFT/#warning-this-module-is-unsupported-and-has-been-deprecated","text":"","title":"WARNING: This Module is Unsupported and has been Deprecated."},{"location":"openscale-manual-config/DRIFT/#configuring-drift-monitor-for-openscale","text":"Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks.","title":"Configuring Drift Monitor for OpenScale"},{"location":"openscale-manual-config/DRIFT/#steps-for-drift-monitor-configuration","text":"The submodule contains the following steps: Open the notebook Run the notebook Look at Drift in the Dashboard","title":"Steps for Drift Monitor Configuration"},{"location":"openscale-manual-config/DRIFT/#1-open-the-notebook","text":"If you Created the Project using the CreditRiskProject.zip file, your notebook will be present in that project. Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and Click on the pencil icon at the right of the openscale-drift-config notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.","title":"1. Open the notebook"},{"location":"openscale-manual-config/DRIFT/#2-run-the-notebook","text":"Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Please note that there are several places in the notebook where you need to update variables. Some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. These changes are described below.","title":"2. Run the Notebook"},{"location":"openscale-manual-config/DRIFT/#wos_credentials","text":"In the notebook section 2.0 you will add your Cloud Pak for Data platform credentials for the WOS_CREDENTIALS . For the url , use the URL your Cloud Pak for Data cluster, i.e something like: \"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\" For the username , use your Cloud Pak for Data login username. For the password , user your Cloud Pak for Data login password.","title":"WOS_CREDENTIALS"},{"location":"openscale-manual-config/DRIFT/#3-look-at-drift-in-the-dashboard","text":"In the same browser \\(but a separate tab\\) , open the Services tab by clicking the Services icon on the top right. Find and click on the Watson OpenScale tile. Launch the OpenScale UI tooling by clicking on the Launch button When the dashboard loads, Click on the 'Model Monitors' tab and you will see the deployment you configured in the jupyter notebook when you ran it in the previous section. Click on the Drift section of the tile to bring up the Drift monitor . Note: Do not worry if the name you see does not match exactly with the screenshot. The deployment name you see will correspond to the variable used in the Jupyter notebook. Note: If you click on the card itself, you can get to the drift monitor page by clicking on the percentage shown for drift in the UI. Click on Drop in accuracy and then look for a time slot on the graph that shows bias (i.e. below the red threshold line). The monitor only runs every three hours, so there may only be one teal colored \"dot\" representing a single run when you first visit the graph. Click on it for more details that will show the number of transactions responsible \"for drop in accuracy\" and \"for drop in data consistency\". You can choose to get details about \"Transactions responsible for drop in accuracy and data consistency\": From here you can see groupings of transactions that caused drift. Where the groups are formed by shared features. Drilling deeper will bring up the individual transactions, and as we've seen before, we can choose a transaction to get the details.","title":"3. Look at Drift in the Dashboard"},{"location":"openscale-manual-config/DRIFT/#stop-the-environment","text":"Important: When you have completed the last submodule in this \"openscale-manual-config\" section that you will be doing, it's recommended you stop the environment in order to conserve resources. You should only follow these steps to stop your environment if you are not going to proceed with the other sub-modules in this section. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window.","title":"Stop the Environment"},{"location":"openscale-manual-config/DRIFT/#conclusion","text":"We've seen how to configure Drift monitoring using a Jupyter notebook. Next, we'll add some historical data to emulate what would happen for a Machine learning model that is deployed in production, monitored with OpenScale, and continually receiving scoring requests. Proceed to the next sub-module to load historical data","title":"Conclusion"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/","text":"WARNING: This Module is Unsupported and has been Deprecated. \u00b6 Configure Fairness and Explainability monitors for OpenScale \u00b6 Watson OpenScale utilizes several monitors to gather data about machine learning inferences and the GUI tool can then present that data in a form that is useful. In this sub-module we will use a Jupyter notebook to configure the monitor for Fairness, allowing us to choose a feature to monitor. In our loan risk scenario, we'll monitor the Risk feature and look at 2 groups. For Gender , we'll designate Male as the majority group and Female as the minority group, and then use the Fairness monitor to make sure that the majority group does not recieve a favorable outcome more often than the minority group, within a tolerance of 5%. We'll also designate the Age group of people 18-25 the minority and 26-75 the majority to look for bias against those in the minority age cohort. We'll then enable the Explainability monitor, which allows us to then use the API or GUI tool to explain individual transactions. By sending slightly perturbed data to the scoring endpoint, the explainability algorithm can build a model of which features contributed to the category of Risk or No Risk , and give a quantitative breakdown of the contributions of each feature to the results. Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks. Steps for OpenScale Fairness and Explainabilty monitor setup \u00b6 The submodule contains the following steps: Open the notebook Run the notebook Begin to Explore the Watson OpenScale UI 1. Open the notebook \u00b6 Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and Click on the pencil icon at the right of the openscale-fairness-explainability notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section. 2. Run the Notebook \u00b6 Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Please note that there are several places in the notebook where you need to update variables. Some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. These changes are described below. WOS_CREDENTIALS \u00b6 In the notebook section 2.0 you will add your Cloud Pak for Data platform credentials for the WOS_CREDENTIALS . For the url , use the URL your Cloud Pak for Data cluster, i.e something like: \"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\" For the username , use your Cloud Pak for Data login username. For the password , user your Cloud Pak for Data login password. Get Explainability transaction_id \u00b6 In order to use the Explainability feature, we will need the ID for an individual transaction. In the notebook, after running cell 5.5 Run explanation for sample record , the output will print a transaction_id . Copy this id. 3. Begin to Explore the Watson OpenScale UI \u00b6 We've enabled the monitors for Fairness and Explainability, now let's explore the results in the OpenScale GUI. In the same browser \\(but a separate tab\\) , open the Services tab by clicking the Services icon on the top right. Find and click on the Watson OpenScale tile. Launch the OpenScale UI tooling by clicking on the Launch button When the dashboard loads, Click on the 'Model Monitors' tab and you will see the deployment you configured in the jupyter notebook when you ran it in the previous section. Click on the Fairness section of the tile to bring up the Fairness monitor . Note: Do not worry if the name you see does not match exactly with the screenshot. The deployment name you see will correspond to the variable used in the Jupyter notebook. Fairness Monitor \u00b6 You will see that the Fairness monitor is enabled and shows a graph of recent transactions. Details for the Fairness monitor are on the left side of the graph, including the Threshold to trigger a bias alert, details of the monitored groups, and Schedule information. You can hover over sections of the graph to see the results of each hour and the scoring requests that were performed. Look for a time slot that shows bias (i.e. below the red threshold line). The monitor only runs once per hour, so there may only be one teal colored \"dot\" representing a single run when you first visit the graph. Click on it for more details: For a given time slot, we will see a comparison between the 2 groups, in this case, female and male. A bar chart presents information for percent of favorable vs. unfavorable outcomes. We can choose our Data set from a series of radio buttons, whether Payload + Perturbed , Payload , Training , or Debiased . The Monitored features can be toggled, and we can change the Date and time . Click on the various radio buttons, toggle the monitored features, and change the time to see how the tool works: Now back to our biased time slot, click on Debiased Data Set and Sex and view how the use of the Debiased endpoint has made our scoring more fair: Click on the View Debiased Endpoint button. Here you can see some Debiased Endpoint Code Snippet examples, showing cURL , Java , and Python code that can utilize the endpoint for debiased transactions, enabling a developer to get a machine learning model score that prevents the biased outcome. (You may need to change the 'Code language' drop down list to see code snippets in different languages): Click the Back button on your browser to go back to the Transactions details for the Fairness monitor page. Click on the View Transactions button. We can see the various transactions that took place during this time slot, as well as some aggregate information around \"Risk\" and \"No Risk\" for both the Current model and the Debiased model. We can click on Explain for one of the transactions for more detailed information (HINT: A \"Risk\" transaction might be more interesting): Note that the explanation of a transaction requires that 1000's of scoring transactions take place using slightly perturbed data for each of the features. This can take several seconds, or even minutes. Future use of this individual transaction will be cached, so the network latency and overhead from performing the scoring will not be a factor. There is a lot of information available for a single transaction: Click the \"i\" information icons next to Minimum changes for another outcome and Maximum changes allowed for the same outcome to help understand the use of the Pertinent Negative and Pertinent Positive : You can see information about the Pertinent Negative , for example, that shows what the minimum changes would need to be to cause a different outcome, i.e from Risk to No Risk , or changing No Risk to Risk . In my example shown, you can see that this is not always calculated: Scroll down and you can see that the Most important factors influencing prediction are highlighted, and below is a complet breakdown of all features, with the percent of influence for the score of either Risk or No Risk : (Optional) If you saved the transaction_id that you copied after running cell 5.5 in the notebook, you can past it into the search bar and press enter: Any previous Explainability transactions will be cached and presented as a tab that you can click on to revisit in the future. Stop the Environment \u00b6 Important: When you have completed the last submodule in this \"openscale-manual-config\" section that you will be doing, it's recommended you stop the environment in order to conserve resources. You should only follow these steps to stop your environment if you are not going to proceed with the other sub-modules in this section. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window. Conclusion \u00b6 You were able to configure Fairness monitoring of your machine learning deployment using the OpenScale python SDK and the APIs in a Juypyter notebook. You then scored 200 randomly selected records to provide enough information to calculate fairness, and run the monitor. After that, Explainability was enabled and an indiviudal transaction selected for later inspection. The OpenScale GUI tool was utilized to look at the Fairness monitor and the Explainability tool. Later in this workshop we will add more historical data to simulate a machine learning model deployed in production and the data available from these monitors. Proceed to the next sub-module to configure Quality monitor and Feedback logging","title":"WARNING: This Module is Unsupported and has been Deprecated."},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#warning-this-module-is-unsupported-and-has-been-deprecated","text":"","title":"WARNING: This Module is Unsupported and has been Deprecated."},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#configure-fairness-and-explainability-monitors-for-openscale","text":"Watson OpenScale utilizes several monitors to gather data about machine learning inferences and the GUI tool can then present that data in a form that is useful. In this sub-module we will use a Jupyter notebook to configure the monitor for Fairness, allowing us to choose a feature to monitor. In our loan risk scenario, we'll monitor the Risk feature and look at 2 groups. For Gender , we'll designate Male as the majority group and Female as the minority group, and then use the Fairness monitor to make sure that the majority group does not recieve a favorable outcome more often than the minority group, within a tolerance of 5%. We'll also designate the Age group of people 18-25 the minority and 26-75 the majority to look for bias against those in the minority age cohort. We'll then enable the Explainability monitor, which allows us to then use the API or GUI tool to explain individual transactions. By sending slightly perturbed data to the scoring endpoint, the explainability algorithm can build a model of which features contributed to the category of Risk or No Risk , and give a quantitative breakdown of the contributions of each feature to the results. Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks.","title":"Configure Fairness and Explainability monitors for OpenScale"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#steps-for-openscale-fairness-and-explainabilty-monitor-setup","text":"The submodule contains the following steps: Open the notebook Run the notebook Begin to Explore the Watson OpenScale UI","title":"Steps for OpenScale Fairness and Explainabilty monitor setup"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#1-open-the-notebook","text":"Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and Click on the pencil icon at the right of the openscale-fairness-explainability notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.","title":"1. Open the notebook"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#2-run-the-notebook","text":"Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Please note that there are several places in the notebook where you need to update variables. Some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. These changes are described below.","title":"2. Run the Notebook"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#wos_credentials","text":"In the notebook section 2.0 you will add your Cloud Pak for Data platform credentials for the WOS_CREDENTIALS . For the url , use the URL your Cloud Pak for Data cluster, i.e something like: \"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\" For the username , use your Cloud Pak for Data login username. For the password , user your Cloud Pak for Data login password.","title":"WOS_CREDENTIALS"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#get-explainability-transaction_id","text":"In order to use the Explainability feature, we will need the ID for an individual transaction. In the notebook, after running cell 5.5 Run explanation for sample record , the output will print a transaction_id . Copy this id.","title":"Get Explainability transaction_id"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#3-begin-to-explore-the-watson-openscale-ui","text":"We've enabled the monitors for Fairness and Explainability, now let's explore the results in the OpenScale GUI. In the same browser \\(but a separate tab\\) , open the Services tab by clicking the Services icon on the top right. Find and click on the Watson OpenScale tile. Launch the OpenScale UI tooling by clicking on the Launch button When the dashboard loads, Click on the 'Model Monitors' tab and you will see the deployment you configured in the jupyter notebook when you ran it in the previous section. Click on the Fairness section of the tile to bring up the Fairness monitor . Note: Do not worry if the name you see does not match exactly with the screenshot. The deployment name you see will correspond to the variable used in the Jupyter notebook.","title":"3. Begin to Explore the Watson OpenScale UI"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#fairness-monitor","text":"You will see that the Fairness monitor is enabled and shows a graph of recent transactions. Details for the Fairness monitor are on the left side of the graph, including the Threshold to trigger a bias alert, details of the monitored groups, and Schedule information. You can hover over sections of the graph to see the results of each hour and the scoring requests that were performed. Look for a time slot that shows bias (i.e. below the red threshold line). The monitor only runs once per hour, so there may only be one teal colored \"dot\" representing a single run when you first visit the graph. Click on it for more details: For a given time slot, we will see a comparison between the 2 groups, in this case, female and male. A bar chart presents information for percent of favorable vs. unfavorable outcomes. We can choose our Data set from a series of radio buttons, whether Payload + Perturbed , Payload , Training , or Debiased . The Monitored features can be toggled, and we can change the Date and time . Click on the various radio buttons, toggle the monitored features, and change the time to see how the tool works: Now back to our biased time slot, click on Debiased Data Set and Sex and view how the use of the Debiased endpoint has made our scoring more fair: Click on the View Debiased Endpoint button. Here you can see some Debiased Endpoint Code Snippet examples, showing cURL , Java , and Python code that can utilize the endpoint for debiased transactions, enabling a developer to get a machine learning model score that prevents the biased outcome. (You may need to change the 'Code language' drop down list to see code snippets in different languages): Click the Back button on your browser to go back to the Transactions details for the Fairness monitor page. Click on the View Transactions button. We can see the various transactions that took place during this time slot, as well as some aggregate information around \"Risk\" and \"No Risk\" for both the Current model and the Debiased model. We can click on Explain for one of the transactions for more detailed information (HINT: A \"Risk\" transaction might be more interesting): Note that the explanation of a transaction requires that 1000's of scoring transactions take place using slightly perturbed data for each of the features. This can take several seconds, or even minutes. Future use of this individual transaction will be cached, so the network latency and overhead from performing the scoring will not be a factor. There is a lot of information available for a single transaction: Click the \"i\" information icons next to Minimum changes for another outcome and Maximum changes allowed for the same outcome to help understand the use of the Pertinent Negative and Pertinent Positive : You can see information about the Pertinent Negative , for example, that shows what the minimum changes would need to be to cause a different outcome, i.e from Risk to No Risk , or changing No Risk to Risk . In my example shown, you can see that this is not always calculated: Scroll down and you can see that the Most important factors influencing prediction are highlighted, and below is a complet breakdown of all features, with the percent of influence for the score of either Risk or No Risk : (Optional) If you saved the transaction_id that you copied after running cell 5.5 in the notebook, you can past it into the search bar and press enter: Any previous Explainability transactions will be cached and presented as a tab that you can click on to revisit in the future.","title":"Fairness Monitor"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#stop-the-environment","text":"Important: When you have completed the last submodule in this \"openscale-manual-config\" section that you will be doing, it's recommended you stop the environment in order to conserve resources. You should only follow these steps to stop your environment if you are not going to proceed with the other sub-modules in this section. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window.","title":"Stop the Environment"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#conclusion","text":"You were able to configure Fairness monitoring of your machine learning deployment using the OpenScale python SDK and the APIs in a Juypyter notebook. You then scored 200 randomly selected records to provide enough information to calculate fairness, and run the monitor. After that, Explainability was enabled and an indiviudal transaction selected for later inspection. The OpenScale GUI tool was utilized to look at the Fairness monitor and the Explainability tool. Later in this workshop we will add more historical data to simulate a machine learning model deployed in production and the data available from these monitors. Proceed to the next sub-module to configure Quality monitor and Feedback logging","title":"Conclusion"},{"location":"openscale-manual-config/HISTORIC-DATA-README/","text":"WARNING: This Module is Unsupported and has been Deprecated. \u00b6 Load Historic Data for OpenScale \u00b6 For a deployed machine learning model, OpenScale will record all of the requests for scoring and the results in the datamart using feedback logging. In this submodule, we'll emulate a production system that has been used for a week to score many requests, allowing the various configured monitors to present some interesting data. Note that this Historic Data submodule can be run at any time. Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks. Steps for Historical Data Load \u00b6 The submodule contains the following steps: Open the notebook Run the notebook Explore the Watson OpenScale UI 1. Open the notebook \u00b6 Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and Click on the pencil icon at the right of the openscale-historic-data notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section. 2. Run the Notebook \u00b6 Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Please note that there are several places in the notebook where you need to update variables. Some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. These changes are described below. WOS_CREDENTIALS \u00b6 In the notebook section 2.0 you will add your Cloud Pak for Data platform credentials for the WOS_CREDENTIALS . For the url , use the URL your Cloud Pak for Data cluster, i.e something like: \"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\" For the username , use your Cloud Pak for Data login username. For the password , user your Cloud Pak for Data login password. 3. Explore the Watson OpenScale UI \u00b6 Now that we've simulated a Machine Learning deployment in production, we can look at the associated monitors again and see more detail. Re-visit the various monitors and look again at the graphs, charts and explanations after the addition of the historical data: Fairness monitor and Explainability Quality monitor and Feedback logging Drift monitor Stop the Environment \u00b6 Important: When you have completed the last submodule in this \"openscale-manual-config\" section that you will be doing, it's recommended you stop the environment in order to conserve resources. You should only follow these steps to stop your environment if you are not going to proceed with the other sub-modules in this section. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window. Conclusion \u00b6 With the addition of historical data, we can now use the OpenScale tools in a simulated production environment. We can look at Fairness, Explainability, Quality, and Drift, and see how all transactions are logged. This workshop contains API code, configuration tools, and details around using the UI tool to enable a user to monitor production machine learning environments.","title":"WARNING: This Module is Unsupported and has been Deprecated."},{"location":"openscale-manual-config/HISTORIC-DATA-README/#warning-this-module-is-unsupported-and-has-been-deprecated","text":"","title":"WARNING: This Module is Unsupported and has been Deprecated."},{"location":"openscale-manual-config/HISTORIC-DATA-README/#load-historic-data-for-openscale","text":"For a deployed machine learning model, OpenScale will record all of the requests for scoring and the results in the datamart using feedback logging. In this submodule, we'll emulate a production system that has been used for a week to score many requests, allowing the various configured monitors to present some interesting data. Note that this Historic Data submodule can be run at any time. Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks.","title":"Load Historic Data for OpenScale"},{"location":"openscale-manual-config/HISTORIC-DATA-README/#steps-for-historical-data-load","text":"The submodule contains the following steps: Open the notebook Run the notebook Explore the Watson OpenScale UI","title":"Steps for Historical Data Load"},{"location":"openscale-manual-config/HISTORIC-DATA-README/#1-open-the-notebook","text":"Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and Click on the pencil icon at the right of the openscale-historic-data notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.","title":"1. Open the notebook"},{"location":"openscale-manual-config/HISTORIC-DATA-README/#2-run-the-notebook","text":"Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Please note that there are several places in the notebook where you need to update variables. Some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. These changes are described below.","title":"2. Run the Notebook"},{"location":"openscale-manual-config/HISTORIC-DATA-README/#wos_credentials","text":"In the notebook section 2.0 you will add your Cloud Pak for Data platform credentials for the WOS_CREDENTIALS . For the url , use the URL your Cloud Pak for Data cluster, i.e something like: \"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\" For the username , use your Cloud Pak for Data login username. For the password , user your Cloud Pak for Data login password.","title":"WOS_CREDENTIALS"},{"location":"openscale-manual-config/HISTORIC-DATA-README/#3-explore-the-watson-openscale-ui","text":"Now that we've simulated a Machine Learning deployment in production, we can look at the associated monitors again and see more detail. Re-visit the various monitors and look again at the graphs, charts and explanations after the addition of the historical data: Fairness monitor and Explainability Quality monitor and Feedback logging Drift monitor","title":"3. Explore the Watson OpenScale UI"},{"location":"openscale-manual-config/HISTORIC-DATA-README/#stop-the-environment","text":"Important: When you have completed the last submodule in this \"openscale-manual-config\" section that you will be doing, it's recommended you stop the environment in order to conserve resources. You should only follow these steps to stop your environment if you are not going to proceed with the other sub-modules in this section. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window.","title":"Stop the Environment"},{"location":"openscale-manual-config/HISTORIC-DATA-README/#conclusion","text":"With the addition of historical data, we can now use the OpenScale tools in a simulated production environment. We can look at Fairness, Explainability, Quality, and Drift, and see how all transactions are logged. This workshop contains API code, configuration tools, and details around using the UI tool to enable a user to monitor production machine learning environments.","title":"Conclusion"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/","text":"WARNING: This Module is Unsupported and has been Deprecated. \u00b6 Configure Quality monitoring and Feedback logging \u00b6 Watson OpenScale utilizes several monitors to gather data about machine learning inferences and the GUI tool can then present that data in a form that is useful. In this sub-module we will use a Jupyter notebook to configure the monitor for Quality and enable Feedback logging. Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks. Steps for OpenScale Quality monitor and Feedback logging \u00b6 The submodule contains the following steps: Open the notebook Run the notebook Begin to Explore the Watson OpenScale UI 1. Open the Notebook \u00b6 Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and Click on the pencil icon at the right of the openscale-quality-feedback notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section. 2. Run the Notebook \u00b6 Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Please note that there are several places in the notebook where you need to update variables. Some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. These changes are described below. WOS_CREDENTIALS \u00b6 In the notebook section 2.0 you will add your Cloud Pak for Data platform credentials for the WOS_CREDENTIALS . For the url , use the URL your Cloud Pak for Data cluster, i.e something like: \"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\" For the username , use your Cloud Pak for Data login username. For the password , user your Cloud Pak for Data login password. 3. Begin to Explore the Watson OpenScale UI \u00b6 We've enabled the monitor for Quality and Feedback logging, now let's explore the results in the OpenScale GUI. In the same browser \\(but a separate tab\\) , open the Services tab by clicking the Services icon on the top right. Find and click on the Watson OpenScale tile. Launch the OpenScale UI tooling by clicking on the Launch button When the dashboard loads, Click on the 'Model Monitors' tab and you will see the deployment you configured in the jupyter notebook when you ran it in the previous section. Click on the Quality section of the tile to bring up the Fairness monitor . Note: Do not worry if the name you see does not match exactly with the screenshot. The deployment name you see will correspond to the variable used in the Jupyter notebook. Quality Monitor \u00b6 In our dashboard we can see that we have a choice for a variety of graphs under Quality . If we choose Area under ROC , where there is a threshold violation in my example, we'll see a limited chart due to the lack of scoring data. (More data will be added later to make this more interesting). Look for a time slot that shows a quality alert (i.e. below the red threshold line). The monitor only runs once per hour, so there may only be one teal colored \"dot\" representing a single run when you first visit the graph. Click on it for more details. We can see statistics for this time slot including Area under ROC , TPR , FPR , Recall , Precision , and more: Other time slots can be examined to gather the relevant quality statistics. Stop the Environment \u00b6 Important: When you have completed the last submodule in this \"openscale-manual-config\" section that you will be doing, it's recommended you stop the environment in order to conserve resources. You should only follow these steps to stop your environment if you are not going to proceed with the other sub-modules in this section. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window. Conclusion \u00b6 In this sub-module we've setup Payload logging and the Quality monitor. Proceed to the next sub-module to configure the Drift monitor","title":"WARNING: This Module is Unsupported and has been Deprecated."},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#warning-this-module-is-unsupported-and-has-been-deprecated","text":"","title":"WARNING: This Module is Unsupported and has been Deprecated."},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#configure-quality-monitoring-and-feedback-logging","text":"Watson OpenScale utilizes several monitors to gather data about machine learning inferences and the GUI tool can then present that data in a form that is useful. In this sub-module we will use a Jupyter notebook to configure the monitor for Quality and enable Feedback logging. Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks.","title":"Configure Quality monitoring and Feedback logging"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#steps-for-openscale-quality-monitor-and-feedback-logging","text":"The submodule contains the following steps: Open the notebook Run the notebook Begin to Explore the Watson OpenScale UI","title":"Steps for OpenScale Quality monitor and Feedback logging"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#1-open-the-notebook","text":"Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and Click on the pencil icon at the right of the openscale-quality-feedback notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.","title":"1. Open the Notebook"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#2-run-the-notebook","text":"Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Please note that there are several places in the notebook where you need to update variables. Some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. These changes are described below.","title":"2. Run the Notebook"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#wos_credentials","text":"In the notebook section 2.0 you will add your Cloud Pak for Data platform credentials for the WOS_CREDENTIALS . For the url , use the URL your Cloud Pak for Data cluster, i.e something like: \"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\" For the username , use your Cloud Pak for Data login username. For the password , user your Cloud Pak for Data login password.","title":"WOS_CREDENTIALS"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#3-begin-to-explore-the-watson-openscale-ui","text":"We've enabled the monitor for Quality and Feedback logging, now let's explore the results in the OpenScale GUI. In the same browser \\(but a separate tab\\) , open the Services tab by clicking the Services icon on the top right. Find and click on the Watson OpenScale tile. Launch the OpenScale UI tooling by clicking on the Launch button When the dashboard loads, Click on the 'Model Monitors' tab and you will see the deployment you configured in the jupyter notebook when you ran it in the previous section. Click on the Quality section of the tile to bring up the Fairness monitor . Note: Do not worry if the name you see does not match exactly with the screenshot. The deployment name you see will correspond to the variable used in the Jupyter notebook.","title":"3. Begin to Explore the Watson OpenScale UI"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#quality-monitor","text":"In our dashboard we can see that we have a choice for a variety of graphs under Quality . If we choose Area under ROC , where there is a threshold violation in my example, we'll see a limited chart due to the lack of scoring data. (More data will be added later to make this more interesting). Look for a time slot that shows a quality alert (i.e. below the red threshold line). The monitor only runs once per hour, so there may only be one teal colored \"dot\" representing a single run when you first visit the graph. Click on it for more details. We can see statistics for this time slot including Area under ROC , TPR , FPR , Recall , Precision , and more: Other time slots can be examined to gather the relevant quality statistics.","title":"Quality Monitor"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#stop-the-environment","text":"Important: When you have completed the last submodule in this \"openscale-manual-config\" section that you will be doing, it's recommended you stop the environment in order to conserve resources. You should only follow these steps to stop your environment if you are not going to proceed with the other sub-modules in this section. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window.","title":"Stop the Environment"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#conclusion","text":"In this sub-module we've setup Payload logging and the Quality monitor. Proceed to the next sub-module to configure the Drift monitor","title":"Conclusion"},{"location":"openscale-notebook/","text":"Configure OpenScale in a Jupyter Notebook \u00b6 There are several ways of configuring Watson OpenScale to monitor machine learning deployments, including the automatic configuration, using the GUI tool, a more manual configuration using the APIs, and some combintation of these. For this exercise we're going to configure our OpenScale service by running a Jupyter Notebook. This provides examples of using the OpenScale Python APIs programatically. This lab is comprised of the following steps: Open the notebook Update credentials Run the notebook Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks. 1. Open the Notebook \u00b6 Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and Click on the pencil icon at the right of the openscale-full-configuration notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section. 2. Update Credentials \u00b6 In the notebook section 1.2 you edit the first code cell to use your Cloud Pak for Data platform credentials in the WOS_CREDENTIALS . For the url , use the URL your cluster, i.e something like: \"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\" . For the username , use your login username. For the password , user your login password. 3. Run the Notebook \u00b6 Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Please note that some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. Get Transactions for Explainability \u00b6 Under 7.8 Identify transactions for Explainability run the cell. It will produce a series of UIDs for indidvidual ML scoring transactions. Copy one or more of them to examine in the next section. Stop the Environment \u00b6 Important : In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window. Conclusion \u00b6 In this section we covered one of the approaches to configure Watson OpenScale to monitor a machine learning model on Cloud Pak for Data. We have seen: How to build a model using Jupyter Notebook. How to use the OpenScale Python APIs programatically. How to configure all the monitors in OpenScale.","title":"Monitoring models with OpenScale (Notebook)"},{"location":"openscale-notebook/#configure-openscale-in-a-jupyter-notebook","text":"There are several ways of configuring Watson OpenScale to monitor machine learning deployments, including the automatic configuration, using the GUI tool, a more manual configuration using the APIs, and some combintation of these. For this exercise we're going to configure our OpenScale service by running a Jupyter Notebook. This provides examples of using the OpenScale Python APIs programatically. This lab is comprised of the following steps: Open the notebook Update credentials Run the notebook Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the Workshop Resources -> FAQs / Tips section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the Workshop Resources -> FAQs / Tips section for links to the completed notebooks.","title":"Configure OpenScale in a Jupyter Notebook"},{"location":"openscale-notebook/#1-open-the-notebook","text":"Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and Click on the pencil icon at the right of the openscale-full-configuration notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.","title":"1. Open the Notebook"},{"location":"openscale-notebook/#2-update-credentials","text":"In the notebook section 1.2 you edit the first code cell to use your Cloud Pak for Data platform credentials in the WOS_CREDENTIALS . For the url , use the URL your cluster, i.e something like: \"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\" . For the username , use your login username. For the password , user your login password.","title":"2. Update Credentials"},{"location":"openscale-notebook/#3-run-the-notebook","text":"Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Please note that some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell.","title":"3. Run the Notebook"},{"location":"openscale-notebook/#get-transactions-for-explainability","text":"Under 7.8 Identify transactions for Explainability run the cell. It will produce a series of UIDs for indidvidual ML scoring transactions. Copy one or more of them to examine in the next section.","title":"Get Transactions for Explainability"},{"location":"openscale-notebook/#stop-the-environment","text":"Important : In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window.","title":"Stop the Environment"},{"location":"openscale-notebook/#conclusion","text":"In this section we covered one of the approaches to configure Watson OpenScale to monitor a machine learning model on Cloud Pak for Data. We have seen: How to build a model using Jupyter Notebook. How to use the OpenScale Python APIs programatically. How to configure all the monitors in OpenScale.","title":"Conclusion"},{"location":"pre-work/","text":"Pre-work \u00b6 Before we get started, we will download some assets and complete some setup for our workshop. This section is broken up into the following steps: Download or Clone the Repository Create an Analytics Project and Deployment Space 1. Download Workshop Assets \u00b6 Various parts of this workshop will require the attendee to upload files or run scripts. These artifacts have been collected in the following zip files which you can download by clicking the link below and saving the zip files locally to your machine. Cloud Pak for Data Analytics Project Sample Python Application Note: The analytics project zip file does not to be unzipped/expanded. It will be directly uploaded to the Cloud Pak for Data platform as a zip file. For reference, all these assets are also in the GitHub repo for this workshop . 2. Create a Project and Deployment Space \u00b6 At this point of the workshop we will be using Cloud Pak for Data for the remaining steps. Launch a browser and navigate to your Cloud Pak for Data deployment. NOTE: Your instructor will provide a URL and credentials to log into Cloud Pak for Data! Create a New Project \u00b6 In Cloud Pak for Data, we use the concept of a project to collect / organize the resources used to achieve a particular goal (resources to build a solution to a problem). Your project resources can include data, collaborators, and analytic assets like notebooks and models, etc. Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click on the New project button on the top right. Select the Analytics project radio button and click the Next button. We are going to create a project from an existing file (which contains the assets we will use throughout this workshop), as opposed to creating an empty project. Select the Create a project from a sample or file option. Click on the browse link and in the file browser pop-up, navigate to where you downloaded the CreditRiskProject.zip file in the previous section, then click the open button. Give the project a name and click the Create button. From the project creation succesfully created pop up window, click on the View new project button. Create a Deployment Space \u00b6 Cloud Pak for Data uses the concept of Deployment Spaces to configure and manage the deployment of a set of related deployable assets. These assets can be data files, machine learning models, etc. Go the (\u2630) navigation menu and click Deployments . Click on the New deployment space button. We will create an empty deployment space, so click on the Create an empty space option. Give your deployment space a unique name, optional description, then click the Create button. From the deployment space creation pop up window, click on the View new space button. Next, we will add a collaborator to the new deployment space. Collaborators allow others to view/edit/manage the assets being deployed. In this workshop, we want the models we deploy to be visible and monitored in the OpenScale model monitoring lab. Click on the Access control tab and then click on Add collaborators on the right. Enter \"admin\" as a Collaborator input field and select the Admin user from the drop down list. Then click on the Add to list button. NOTE: We are adding the user that configured the machine learning instance for OpenScale monitoring. In this case, the user is the admin user. Click the Add button to finish adding the collaborator. You should be brought back to the deployment space page and see your user ID along with the Admin user as collaborators for this space. Conclusion \u00b6 We've completed creating the project and deployment space that we will be using for the rest of this workshop.","title":"Pre-work"},{"location":"pre-work/#pre-work","text":"Before we get started, we will download some assets and complete some setup for our workshop. This section is broken up into the following steps: Download or Clone the Repository Create an Analytics Project and Deployment Space","title":"Pre-work"},{"location":"pre-work/#1-download-workshop-assets","text":"Various parts of this workshop will require the attendee to upload files or run scripts. These artifacts have been collected in the following zip files which you can download by clicking the link below and saving the zip files locally to your machine. Cloud Pak for Data Analytics Project Sample Python Application Note: The analytics project zip file does not to be unzipped/expanded. It will be directly uploaded to the Cloud Pak for Data platform as a zip file. For reference, all these assets are also in the GitHub repo for this workshop .","title":"1. Download Workshop Assets"},{"location":"pre-work/#2-create-a-project-and-deployment-space","text":"At this point of the workshop we will be using Cloud Pak for Data for the remaining steps. Launch a browser and navigate to your Cloud Pak for Data deployment. NOTE: Your instructor will provide a URL and credentials to log into Cloud Pak for Data!","title":"2. Create a Project and Deployment Space"},{"location":"pre-work/#create-a-new-project","text":"In Cloud Pak for Data, we use the concept of a project to collect / organize the resources used to achieve a particular goal (resources to build a solution to a problem). Your project resources can include data, collaborators, and analytic assets like notebooks and models, etc. Go the (\u2630) navigation menu and under the Projects section click on All Projects . Click on the New project button on the top right. Select the Analytics project radio button and click the Next button. We are going to create a project from an existing file (which contains the assets we will use throughout this workshop), as opposed to creating an empty project. Select the Create a project from a sample or file option. Click on the browse link and in the file browser pop-up, navigate to where you downloaded the CreditRiskProject.zip file in the previous section, then click the open button. Give the project a name and click the Create button. From the project creation succesfully created pop up window, click on the View new project button.","title":"Create a New Project"},{"location":"pre-work/#create-a-deployment-space","text":"Cloud Pak for Data uses the concept of Deployment Spaces to configure and manage the deployment of a set of related deployable assets. These assets can be data files, machine learning models, etc. Go the (\u2630) navigation menu and click Deployments . Click on the New deployment space button. We will create an empty deployment space, so click on the Create an empty space option. Give your deployment space a unique name, optional description, then click the Create button. From the deployment space creation pop up window, click on the View new space button. Next, we will add a collaborator to the new deployment space. Collaborators allow others to view/edit/manage the assets being deployed. In this workshop, we want the models we deploy to be visible and monitored in the OpenScale model monitoring lab. Click on the Access control tab and then click on Add collaborators on the right. Enter \"admin\" as a Collaborator input field and select the Admin user from the drop down list. Then click on the Add to list button. NOTE: We are adding the user that configured the machine learning instance for OpenScale monitoring. In this case, the user is the admin user. Click the Add button to finish adding the collaborator. You should be brought back to the deployment space page and see your user ID along with the Admin user as collaborators for this space.","title":"Create a Deployment Space"},{"location":"pre-work/#conclusion","text":"We've completed creating the project and deployment space that we will be using for the rest of this workshop.","title":"Conclusion"},{"location":"watson-knowledge-catalog-admin/","text":"Watson Knowledge Catalog for Admins \u00b6 This exercise demonstrates how to solve the problems of enterprise data governance using Watson Knowledge Catalog on the Cloud Pak for Data platform. We'll explain how to use governance, data quality and active policy management in order to help your organization protect and govern sensitive data, trace data lineage and manage data lakes. This knowledge will help users quickly discover, curate, categorize and share data assets, data sets, analytical models and their relationships with other members of your organization. It serves as a single source of truth for data engineers, data stewards, data scientists and business analysts to gain self-service access to data they can trust. You will need the Admin role to create a catalog. This section is comprised of the following steps: Set up Catalog Add Data Assets Add collaborators and control access Add categories Add data classes Add Business terms Add rules for policies 1. Set up Catalog \u00b6 NOTE: The default catalog is your enterprise catalog. It is created automatically after you install the Watson Knowledge Catalog service and is the only catalog to which advanced data curation tools apply. The default catalog is governed so that data protection rules are enforced. The information assets view shows additional properties of the assets in the default catalog to aid curation. Any subsequent catalogs that you create can be governed or ungoverned, do not have an information assets view, and supply basic data curation tools. First we'll create a catalog and load some data Create the catalog \u00b6 Go to the upper-left (\u2630) hamburger menu and choose Catalogs -> All catalogs . From the Your catalogs page, click the Create catalog button. Give your catalog a name, check the Enforce data protection rules checkbox and provide an optional description. Then click the Create button. Note: Click Ok in the pop up window when selecting the data protection checkbox. 2. Add Data Assets \u00b6 There are several ways to add assets to the catalog. We are going to add a local data asset. There are also optional sections to add connection assets below. Local Data Asset \u00b6 Click Add to Catalog + in the top right and choose Local files . Click the browse link in the 'Select file(s) panel. Browse to the /data/split/applicant_personal_data.csv file to select it. Add an optional description and click the Add button. NOTE: Stay in the catalog until loading is complete! If you leave the catalog, the incomplete asset will be deleted. The newly added file will show up under the Browse Assets tab of your catalog: (Optional) Add Connection \u00b6 You can add a connection to various data sources, for example DB2 Warehouse in IBM Cloud , by choosing Add to Catalog + -> Connection : Click on the data source type you want to add (for example, Db2 Warehouse ). Enter the connection details and click Create : The connection now shows up in the catalog. Note: Virtualized data can be added to the Default catalog by someone with Administrator or Editor access to that catalog. There is an option to add Data Virtualization as a connection. (Optional) Add Data from Connection \u00b6 Once you have a connection to a data source, you will be able to add assets from that connection. Click +Add to Catalog -> Connected asset : Click Source -> Select source . Browse under DV to you Schema (i.e. UserXYZW) and choose the joined table. Click Select . A user can now add this to a project like any other asset from a catalog. 2. Add Collaborators and Review Data \u00b6 Under the Access Control tab you can click Add Collaborator to give other users access to your catalog. You can search for a user by entering their name in the Collaborators field. Click on the name to select the user., and click Add . You can choose a role for the user - Admin , Editor , or Viewer . Then click the Add button. To access data in the catalog, click on the name of the data. An overview of the data will open with metadata and Governance artifacts. Click on the Asset tab to see a preview of the first 1000 rows. You can click the Review tab and rate the data, as well as comment on it, to provide feedback to consumers of the data. 3. Add categories \u00b6 The fundamental abstraction in Watson Knowledge Catalog is the Category. A category is analogous to a folder. You can add categories as needed, or you can import them in .csv format. Import categories (optional) \u00b6 To import categories with unique names, you will need to be comfortable with running a command in a terminal window. Please skip this if you are not familiar with that process. All category names are global in scope, so you'll need to import a file with unique names. Go to where you cloned or downloaded this repository, and navigate to the file data/wkc/glossary-organize-categories.csv . Run the script data/wkc/prepend-user-tag.py using your intials or some other tag in order to create a unique file. For example, I might run ./prepend-user-tag -T scottda . If you do not add a tag with the -T parameter, a unique file with unique Category names will be generated with a python time.time() string. Import a category for your assets by going to the upper-left (\u2630) hamburger menu, choose Governance -> Categories , then the click the Add category button and choose Import from file . Click the Add file and navigate to where you cloned/downloaded the workshop repository, choosing the file that you have created using the prepend-user-tag.py script, i.e data/wkc/scottda-glossary-organize-categories.csv would be the file I created by running ./prepend-user-tag.py -T scottda . Click the Next button. Under Select merge option choose Replace all values and click Import . You will see \"The import completed succesfully\" when it is completed. In this way, you can import Categories, Business Terms, Classifications, Policies, etc. to populate your governance catalogs. Add category manually \u00b6 NOTE: Categories, Business Terms, Data Classes, and othe Governance artifacts are global in scope. When you are asked to create one, pre-pend your initials or some unique tag, or it will fail. For example, below I would use scottda-Personal Data in place of XXX-Personal Data . In addition to importing, you can manually create categories. Add a category for your assets by going to the upper-left (\u2630) hamburger menu, choose Governance -> Categories , then click the Add category button and then New category . Give your category a name pre-pended with initials or a unique tag, such as XXX-Personal Data , and an optional description, and then click the Save button. Now, if you hit the Create category link on the Personal Data category screen under Subcategories , you can create a subcategory, such as Residence Information . For the Personal Data category you can select a Type , such as Business term . We can also create classifications for assets, similar to Confidential , Personally Identifiable Information , or Sensitive Personal Information in a similar way, by going to the upper-left (\u2630) hamburger menu, choose Governance -> Classifications . Click on the Create classification button on the top right and then New classification from the drop down menu. These classifications can then be added to your category as a Type : 4. Add data classes \u00b6 NOTE: Categories, Business Terms, Data Classes, and othe Governance artifacts are global in scope. When you are asked to create one, pre-pend your initials or some unique tag, or it will fail. For example, below I would use scottda-alphanumeric in place of XXX-alphanumeric . When you profile your assets, a data class will be inferred from the contents where possible. We'll see more on this later. You can also add your own data classes. Add a data class for your assets by going to the upper-left (\u2630) hamburger menu, choose Governance -> Data classes , then click the Add data class button and then the New data class option from the drop down menu. Give your new data class a name pre-pended with initials or a tag, i.e. XXX-alphanumeric , and then click Change for Primary category. Choose the Personal Data primary category and click Add . Now you can click Save as draft . Once the data class is created, we can optionally: add Stewards for this class, and associate classifications and business terms . When you are ready, click the Publish button and again Publish in the pop up window. Now let's add that data class to a column in our applicant_personal_data.csv asset. Go back to the catalog you created earlier (i.e CreditDataCatalog ) and open it ((\u2630) hamburger menu Catalogs -> All catalogs and choose CreditDataCatalog ). Under the Browse assets tab, click on the data set applicant_personal_data.csv , and then the Asset tab, to get the column/row preview. Find the CustomerID column and click the down arrow next to \"Customer Number\" and then View all : In the window that opens, search for your newly created data class, alphanumeric and click it when it returns in the search. Then click the Select button. 5. Add Business terms \u00b6 NOTE: Categories, Business Terms, Data Classes, and othe Governance artifacts are global in scope. When you are asked to create one, pre-pend your initials or some unique tag, or it will fail. For example, below I would use scottda-Contact Information in place of XXX-Contact Information . You can use Business terms to standardize definitions of business concepts so that your data is described in a uniform and easily understood way across your enterprise. You already saw how to create a category and make it a business term . You can also create the business term as it's own entity. From the upper-left (\u2630) hamburger menu, choose Governance -> Business terms : Click on the upper-right Add business term button and then the New business term option in the drop down menu. Give the new Business term a name pre-pended with initials or a tag, such as XXX-Contact Information and optional description. Click Change under Primary category and choose Personal data , then Click the Save as draft button. A window will come up once the term is created. You can see a rich set of options for creating related terms and adding other metadata. For now, click Publish to make this term available to users of the platform. Go ahead and click Publish on the pop up confirmation window. Go back to the catalog you created earlier (i.e CreditDataCatalog ) and open it ((\u2630) hamburger menu Catalog -> All catalogs and choose CreditDataCatalog ). Under the Browse assets tab, click on the data set applicant_personal_data.csv , and then the Asset tab, to get the column/row preview. Find the Email column and click the Column information icon (looks like an \"eye\"). In the window that opens, click the edit icon (looks like a \"pencil\") next to Business terms : Enter XXX-Contact Information (your uniquely named term such as scottda-ContactInfo ) term you created earlier under Business terms and the term will be searched for. Click on the Contact Information term that is found, and click Apply : Click Close in that window once the term has been applied. Now, do the same thing to add the Contact Information Business term to the Telephone column. You will now be able to search for these terms from within the platform. For example, going back to your top level CreditDataCatalog , in the search bar with the comment \"What assets are you searching for?\" enter your unique Contact Information term: The applicant_personal_data.csv data set will show up, since it contains columns tagged with the Contact Infomation business term. 6. Add rules for policies \u00b6 We can now create rules to control how a user can access data. NOTE: Workshop teammates can simply reuse 1 term to associate with a rule, i.e. CustomerID , or you can proceed below to create a uniquely named one. Create a business term called XXX-CustomerID , or re-use one of your workshop teammates buisness terms for this expercise. Assign it to your CustomerID column in the data set using the instructions above. See below if you need details, but try it yourself first, and skip to Adding a rule below if you do not need a reminder. How to create a Business term review \u00b6 From the upper-left (\u2630) hamburger menu, choose Governance -> Business terms . Click on the upper-right Add business term button and then the New business term option in the drop down menu. Give the new Business term the name XXX-CustomerID and optional description. Click Change under Primary category and choose Personal data , then Click the Save as draft button. In the next window, click Publish . Go back to the catalog you created earlier (i.e CreditDataCatalog ) and open it ((\u2630) hamburger menu Catalog -> All catalogs and choose CreditDataCatalog ). Under the Browse assets tab, click on the data set applicant_personal_data.csv , and then the Asset tab, to get the column/row preview. Find the CustomerID column and click the Column information icon (looks like an \"eye\"). In the window that opens, click the edit icon (looks like a \"pencil\") next to Business terms . Enter CustomerID under Business terms and the term will be searched for. Click on the CustumerID term that is found, and click Apply . Then close the pop up window. Adding a rule \u00b6 From the upper-left (\u2630) hamburger menu, choose Governance -> Rules . Click the Add rule button on the top right and then select the New rule option from the drop down menu. In the 'Create a new rule' page, select the Data protection rule option. Give your rule a unique XXX-Name , leave the Type set to Access , and add a Business definition . Under Rule builder > Condition1 : For the If condition, select Business term Contains any CustomerID . Under Action , for the then panel, select mask data in columns containing alphanumeric . Choose the tile for Substitute , which will make a non-identifiable hash. This obscures the actual CustomerID, but allows actions like database joins to still work. Click the Create rule button. Now if we go back to our applicant_personal_data.csv asset in the catalog at the CustomerID column, it will look the same as before. But a non-admin user will see the \"lock\" icon and see that the customerID has now been substituted with a hash value. To add a rule to Obfuscate data, create a new data class called Age . See the instructions above if needed, don't forget to publish the class. Back in the CreditDataCatalog , under the applicant_personal_data.csv asset, go to the Overview tab and scroll to the Age column. Click the \"down arrow\" and you can see that the data has been inferred to be classified as a Code . Change the classifier by clicking View all . Now change the classifier by starting to type Age . When this comes up in the search, select it and then click the Select button. Following the prior instructions, you can build a new data protection rule to Obfuscate this Age column. And now when that column is viewed by a non-admin user, it will have data that is replaced with similarly formatted data. Conclusion \u00b6 In this lab, we learned how to: Set up Catalog and Data Add collaborators and control access Add categories Add data classes Add Business terms Add rules for policies","title":"Enterprise data governance for Admins using Watson Knowledge Catalog"},{"location":"watson-knowledge-catalog-admin/#watson-knowledge-catalog-for-admins","text":"This exercise demonstrates how to solve the problems of enterprise data governance using Watson Knowledge Catalog on the Cloud Pak for Data platform. We'll explain how to use governance, data quality and active policy management in order to help your organization protect and govern sensitive data, trace data lineage and manage data lakes. This knowledge will help users quickly discover, curate, categorize and share data assets, data sets, analytical models and their relationships with other members of your organization. It serves as a single source of truth for data engineers, data stewards, data scientists and business analysts to gain self-service access to data they can trust. You will need the Admin role to create a catalog. This section is comprised of the following steps: Set up Catalog Add Data Assets Add collaborators and control access Add categories Add data classes Add Business terms Add rules for policies","title":"Watson Knowledge Catalog for Admins"},{"location":"watson-knowledge-catalog-admin/#1-set-up-catalog","text":"NOTE: The default catalog is your enterprise catalog. It is created automatically after you install the Watson Knowledge Catalog service and is the only catalog to which advanced data curation tools apply. The default catalog is governed so that data protection rules are enforced. The information assets view shows additional properties of the assets in the default catalog to aid curation. Any subsequent catalogs that you create can be governed or ungoverned, do not have an information assets view, and supply basic data curation tools. First we'll create a catalog and load some data","title":"1. Set up Catalog"},{"location":"watson-knowledge-catalog-admin/#create-the-catalog","text":"Go to the upper-left (\u2630) hamburger menu and choose Catalogs -> All catalogs . From the Your catalogs page, click the Create catalog button. Give your catalog a name, check the Enforce data protection rules checkbox and provide an optional description. Then click the Create button. Note: Click Ok in the pop up window when selecting the data protection checkbox.","title":"Create the catalog"},{"location":"watson-knowledge-catalog-admin/#2-add-data-assets","text":"There are several ways to add assets to the catalog. We are going to add a local data asset. There are also optional sections to add connection assets below.","title":"2. Add Data Assets"},{"location":"watson-knowledge-catalog-admin/#local-data-asset","text":"Click Add to Catalog + in the top right and choose Local files . Click the browse link in the 'Select file(s) panel. Browse to the /data/split/applicant_personal_data.csv file to select it. Add an optional description and click the Add button. NOTE: Stay in the catalog until loading is complete! If you leave the catalog, the incomplete asset will be deleted. The newly added file will show up under the Browse Assets tab of your catalog:","title":"Local Data Asset"},{"location":"watson-knowledge-catalog-admin/#optional-add-connection","text":"You can add a connection to various data sources, for example DB2 Warehouse in IBM Cloud , by choosing Add to Catalog + -> Connection : Click on the data source type you want to add (for example, Db2 Warehouse ). Enter the connection details and click Create : The connection now shows up in the catalog. Note: Virtualized data can be added to the Default catalog by someone with Administrator or Editor access to that catalog. There is an option to add Data Virtualization as a connection.","title":"(Optional) Add Connection"},{"location":"watson-knowledge-catalog-admin/#optional-add-data-from-connection","text":"Once you have a connection to a data source, you will be able to add assets from that connection. Click +Add to Catalog -> Connected asset : Click Source -> Select source . Browse under DV to you Schema (i.e. UserXYZW) and choose the joined table. Click Select . A user can now add this to a project like any other asset from a catalog.","title":"(Optional) Add Data from Connection"},{"location":"watson-knowledge-catalog-admin/#2-add-collaborators-and-review-data","text":"Under the Access Control tab you can click Add Collaborator to give other users access to your catalog. You can search for a user by entering their name in the Collaborators field. Click on the name to select the user., and click Add . You can choose a role for the user - Admin , Editor , or Viewer . Then click the Add button. To access data in the catalog, click on the name of the data. An overview of the data will open with metadata and Governance artifacts. Click on the Asset tab to see a preview of the first 1000 rows. You can click the Review tab and rate the data, as well as comment on it, to provide feedback to consumers of the data.","title":"2. Add Collaborators and Review Data"},{"location":"watson-knowledge-catalog-admin/#3-add-categories","text":"The fundamental abstraction in Watson Knowledge Catalog is the Category. A category is analogous to a folder. You can add categories as needed, or you can import them in .csv format.","title":"3. Add categories"},{"location":"watson-knowledge-catalog-admin/#import-categories-optional","text":"To import categories with unique names, you will need to be comfortable with running a command in a terminal window. Please skip this if you are not familiar with that process. All category names are global in scope, so you'll need to import a file with unique names. Go to where you cloned or downloaded this repository, and navigate to the file data/wkc/glossary-organize-categories.csv . Run the script data/wkc/prepend-user-tag.py using your intials or some other tag in order to create a unique file. For example, I might run ./prepend-user-tag -T scottda . If you do not add a tag with the -T parameter, a unique file with unique Category names will be generated with a python time.time() string. Import a category for your assets by going to the upper-left (\u2630) hamburger menu, choose Governance -> Categories , then the click the Add category button and choose Import from file . Click the Add file and navigate to where you cloned/downloaded the workshop repository, choosing the file that you have created using the prepend-user-tag.py script, i.e data/wkc/scottda-glossary-organize-categories.csv would be the file I created by running ./prepend-user-tag.py -T scottda . Click the Next button. Under Select merge option choose Replace all values and click Import . You will see \"The import completed succesfully\" when it is completed. In this way, you can import Categories, Business Terms, Classifications, Policies, etc. to populate your governance catalogs.","title":"Import categories (optional)"},{"location":"watson-knowledge-catalog-admin/#add-category-manually","text":"NOTE: Categories, Business Terms, Data Classes, and othe Governance artifacts are global in scope. When you are asked to create one, pre-pend your initials or some unique tag, or it will fail. For example, below I would use scottda-Personal Data in place of XXX-Personal Data . In addition to importing, you can manually create categories. Add a category for your assets by going to the upper-left (\u2630) hamburger menu, choose Governance -> Categories , then click the Add category button and then New category . Give your category a name pre-pended with initials or a unique tag, such as XXX-Personal Data , and an optional description, and then click the Save button. Now, if you hit the Create category link on the Personal Data category screen under Subcategories , you can create a subcategory, such as Residence Information . For the Personal Data category you can select a Type , such as Business term . We can also create classifications for assets, similar to Confidential , Personally Identifiable Information , or Sensitive Personal Information in a similar way, by going to the upper-left (\u2630) hamburger menu, choose Governance -> Classifications . Click on the Create classification button on the top right and then New classification from the drop down menu. These classifications can then be added to your category as a Type :","title":"Add category manually"},{"location":"watson-knowledge-catalog-admin/#4-add-data-classes","text":"NOTE: Categories, Business Terms, Data Classes, and othe Governance artifacts are global in scope. When you are asked to create one, pre-pend your initials or some unique tag, or it will fail. For example, below I would use scottda-alphanumeric in place of XXX-alphanumeric . When you profile your assets, a data class will be inferred from the contents where possible. We'll see more on this later. You can also add your own data classes. Add a data class for your assets by going to the upper-left (\u2630) hamburger menu, choose Governance -> Data classes , then click the Add data class button and then the New data class option from the drop down menu. Give your new data class a name pre-pended with initials or a tag, i.e. XXX-alphanumeric , and then click Change for Primary category. Choose the Personal Data primary category and click Add . Now you can click Save as draft . Once the data class is created, we can optionally: add Stewards for this class, and associate classifications and business terms . When you are ready, click the Publish button and again Publish in the pop up window. Now let's add that data class to a column in our applicant_personal_data.csv asset. Go back to the catalog you created earlier (i.e CreditDataCatalog ) and open it ((\u2630) hamburger menu Catalogs -> All catalogs and choose CreditDataCatalog ). Under the Browse assets tab, click on the data set applicant_personal_data.csv , and then the Asset tab, to get the column/row preview. Find the CustomerID column and click the down arrow next to \"Customer Number\" and then View all : In the window that opens, search for your newly created data class, alphanumeric and click it when it returns in the search. Then click the Select button.","title":"4. Add data classes"},{"location":"watson-knowledge-catalog-admin/#5-add-business-terms","text":"NOTE: Categories, Business Terms, Data Classes, and othe Governance artifacts are global in scope. When you are asked to create one, pre-pend your initials or some unique tag, or it will fail. For example, below I would use scottda-Contact Information in place of XXX-Contact Information . You can use Business terms to standardize definitions of business concepts so that your data is described in a uniform and easily understood way across your enterprise. You already saw how to create a category and make it a business term . You can also create the business term as it's own entity. From the upper-left (\u2630) hamburger menu, choose Governance -> Business terms : Click on the upper-right Add business term button and then the New business term option in the drop down menu. Give the new Business term a name pre-pended with initials or a tag, such as XXX-Contact Information and optional description. Click Change under Primary category and choose Personal data , then Click the Save as draft button. A window will come up once the term is created. You can see a rich set of options for creating related terms and adding other metadata. For now, click Publish to make this term available to users of the platform. Go ahead and click Publish on the pop up confirmation window. Go back to the catalog you created earlier (i.e CreditDataCatalog ) and open it ((\u2630) hamburger menu Catalog -> All catalogs and choose CreditDataCatalog ). Under the Browse assets tab, click on the data set applicant_personal_data.csv , and then the Asset tab, to get the column/row preview. Find the Email column and click the Column information icon (looks like an \"eye\"). In the window that opens, click the edit icon (looks like a \"pencil\") next to Business terms : Enter XXX-Contact Information (your uniquely named term such as scottda-ContactInfo ) term you created earlier under Business terms and the term will be searched for. Click on the Contact Information term that is found, and click Apply : Click Close in that window once the term has been applied. Now, do the same thing to add the Contact Information Business term to the Telephone column. You will now be able to search for these terms from within the platform. For example, going back to your top level CreditDataCatalog , in the search bar with the comment \"What assets are you searching for?\" enter your unique Contact Information term: The applicant_personal_data.csv data set will show up, since it contains columns tagged with the Contact Infomation business term.","title":"5. Add Business terms"},{"location":"watson-knowledge-catalog-admin/#6-add-rules-for-policies","text":"We can now create rules to control how a user can access data. NOTE: Workshop teammates can simply reuse 1 term to associate with a rule, i.e. CustomerID , or you can proceed below to create a uniquely named one. Create a business term called XXX-CustomerID , or re-use one of your workshop teammates buisness terms for this expercise. Assign it to your CustomerID column in the data set using the instructions above. See below if you need details, but try it yourself first, and skip to Adding a rule below if you do not need a reminder.","title":"6. Add rules for policies"},{"location":"watson-knowledge-catalog-admin/#how-to-create-a-business-term-review","text":"From the upper-left (\u2630) hamburger menu, choose Governance -> Business terms . Click on the upper-right Add business term button and then the New business term option in the drop down menu. Give the new Business term the name XXX-CustomerID and optional description. Click Change under Primary category and choose Personal data , then Click the Save as draft button. In the next window, click Publish . Go back to the catalog you created earlier (i.e CreditDataCatalog ) and open it ((\u2630) hamburger menu Catalog -> All catalogs and choose CreditDataCatalog ). Under the Browse assets tab, click on the data set applicant_personal_data.csv , and then the Asset tab, to get the column/row preview. Find the CustomerID column and click the Column information icon (looks like an \"eye\"). In the window that opens, click the edit icon (looks like a \"pencil\") next to Business terms . Enter CustomerID under Business terms and the term will be searched for. Click on the CustumerID term that is found, and click Apply . Then close the pop up window.","title":"How to create a Business term review"},{"location":"watson-knowledge-catalog-admin/#adding-a-rule","text":"From the upper-left (\u2630) hamburger menu, choose Governance -> Rules . Click the Add rule button on the top right and then select the New rule option from the drop down menu. In the 'Create a new rule' page, select the Data protection rule option. Give your rule a unique XXX-Name , leave the Type set to Access , and add a Business definition . Under Rule builder > Condition1 : For the If condition, select Business term Contains any CustomerID . Under Action , for the then panel, select mask data in columns containing alphanumeric . Choose the tile for Substitute , which will make a non-identifiable hash. This obscures the actual CustomerID, but allows actions like database joins to still work. Click the Create rule button. Now if we go back to our applicant_personal_data.csv asset in the catalog at the CustomerID column, it will look the same as before. But a non-admin user will see the \"lock\" icon and see that the customerID has now been substituted with a hash value. To add a rule to Obfuscate data, create a new data class called Age . See the instructions above if needed, don't forget to publish the class. Back in the CreditDataCatalog , under the applicant_personal_data.csv asset, go to the Overview tab and scroll to the Age column. Click the \"down arrow\" and you can see that the data has been inferred to be classified as a Code . Change the classifier by clicking View all . Now change the classifier by starting to type Age . When this comes up in the search, select it and then click the Select button. Following the prior instructions, you can build a new data protection rule to Obfuscate this Age column. And now when that column is viewed by a non-admin user, it will have data that is replaced with similarly formatted data.","title":"Adding a rule"},{"location":"watson-knowledge-catalog-admin/#conclusion","text":"In this lab, we learned how to: Set up Catalog and Data Add collaborators and control access Add categories Add data classes Add Business terms Add rules for policies","title":"Conclusion"},{"location":"watson-knowledge-catalog-user/","text":"Watson Knowledge Catalog for Users \u00b6 This exercise demonstrates how to solve the problems of enterprise data governance using Watson Knowledge Catalog on the Cloud Pak for Data platform. We'll explain how to use governance, data quality and active policy management in order to help your organization protect and govern sensitive data, trace data lineage and manage data lakes. This knowledge will help users quickly discover, curate, categorize and share data assets, data sets, analytical models and their relationships with other members of your organization. It serves as a single source of truth for data engineers, data stewards, data scientists and business analysts to gain self-service access to data they can trust. This section is comprised of the following steps: Find the Right Data Understand the Data Understand the Data Content 1. Find the Right Data \u00b6 We need to find the right data and business information related to the Mortgage Default analysis project. You can use the global search to search across catalogs, projects and the business glossary to find all assets that you may be interested in. Enter the words mortgage in the global search area and press the enter key to start finding what you need. Place your cursor inside the global search area next to the word mortgage and press the enter key: The search returns all data and information assets related to the search criteria across all catalogs, projects and governance artifacts. Scroll down through the list to take a closer look at what was found. You can further refine your search results by using the filters supplied by type, tag, catalog, project etc. First and foremost, we need to verify that all the data needed for the project has been cataloged and available to the project team to use. We can do this very easily by refining the search to only display mortgage data that exists across all catalogs. From the search drop down menu select Governance > All governance artifacts . All data assets across catalogs meeting the criteria are displayed. This is the data we are looking for. The connection to the Analytics Data Warehouse and the 4 mortgage tables are what the project team requested; Mortgage Default, Applicant, Property and Customer are all in the Enterprise catalog. However, before we proceed to the catalog we need to also find all the business information related to the project to review the terms and content of the data and identify if there are any policies and rules set by the business that the project team needs to be aware of and adhere to. From the search drop down menu select Governance > Business terms . Notice that the governance team has been hard at work and has defined 25 business terms related to the mortgage data the project will be using. Also notice that they are tagged with the key word Mortgage to easily find them Scroll down the list of business terms to view them all. Note that the Email Address, Phone Number and Social Security Number have all been tagged as Sensitive information. Click on the Clear all button to get ready for the next search task. Click on the Any type filter and select the Category type from the list to refine the results. Notice that there is a Mortgage Default Analysis category defined that contains all the business information related to the project and a category named Sensitive Information that is a subcategory. This is a good indication that the mortgage data being used by the project contains sensitive information that needs to be protected. 2. Understand the Data \u00b6 The best data is data that is fully understood and trusted. You can be confident in your data when you know where it comes from, that it complies to a set of policies and rules that address data privacy regulations and that it is clean and conforms to data quality policies, rules and standards, and that others have used it and trust it and are willing to share that information, to ensure you can produce meaningful and accurate analytical and AI results that will benefit better business outcomes. Understand Data Policies and Rules \u00b6 In this section you will use the Business glossary to gain a deeper understanding of the business terminology defined by the data steward and the governance team responsible for establishing policies and rules to govern and protect the data. Since we see that there is an indication of sensitive information Let\u2019s take a closer look at the Sensitive Information category content. Click on the Sensitive Information category from the list. The description clearly states that this subcategory contains references to business terms that relate to data that will be used by the project team that need to be governed by data protection rules. Let\u2019s get more information on the Social Security Number. In the global search area, enter the words social security number and press the enter key. A Social Security Number term appears in the list with tags of Mortgage applicant and Sensitive information. This is an indication that the Mortgage Applicant table has a social security number. 3. Understand the Data Content \u00b6 You have gained an understanding of the policies and rules and information related to sensitive data and validated and trust the data quality. In this section we will go to the Enterprise catalog, which is where we identified all the data we need resides, and use all of the features it provides to gain an even better understanding of the data content and have even more confidence in the data based on what others are saying and by utilizing the AI assisted recommendations, automatic profiling and additional data content statistics provided. Click the (\u2630) hamburger menu in the upper left corner and click Catalog -> All catalogs Click on the Enterprise catalog. Watson Knowledge Catalog provides suggested assets to you based on recommendations using AI, things you might be interested in based on your past viewing history. Notice that it is already recommending Mortgage data to you based on your past searches. It also keeps track of what\u2019s hot and Highly Rated based on reviews and ranks them in order of their rating highest to lowest. Click on the Highly Rated section to see what\u2019s hot: Notice that the Analytics Data Warehouse, MORTGAGE_APPLICANT and MORTGAGE_CUSTOMER tables have been reviewed and are rated quite high. A good indication of their quality and usability. Lastly, Watson Knowledge Catalog keeps track of what\u2019s new that has been Recently Added since the last time you visited the catalog. This are all means to help you find and understand the data more quickly and easily. Click on the Recently Added section to see what\u2019s new. Notice that the MORTGAGE_APPLICANT table was the most recent data asset added to the catalog. Click on the MORTGAGE_APPLICANT table to review its content and metadata. You are brought into the Overview section of the MORTGAGE_APPLICANT table. Click on the Asset tab. You may see that Data masking is in progress and that 3 columns are being masked. The asset is being masked by the Protect Sensitive Personal Information data protection rule being enforced by the data governance team. Because you are not authorized to view the sensitive information, the data is being protected. The data that is masked is indicated with a lock icon next to their column names; EMAIL_ADDRESS, PHONE_NUMBER and SOCIAL_SECURITY_NUMBER. Scroll to the right to view all the masked columns. Click on the lock icon on the EMAIL_ADDRESS column to view the data protection message. Click on the Review tab to read the review. The table has a 5 star rating with a very positive review. Reviews can be written by anyone who has access to the catalog and the asset to notify and inform others of the content and usability of the data. Click on the Profile tab to view the data profile. As data assets are discovered and added to the catalog they are automatically profiled and classified to give end users a more in-depth understanding of the data content, quality and usability. Data classifications are used to identify what type of data it is and to autonomously enforce data protect rules to mask sensitive data, like you just saw. Scroll to the right to view the other columns. Notice that the protected data does not have any profile information displayed. Lineage is captured for every data asset in a catalog. It keeps track of where it came from, any updates or changes that have been made to its metadata and any movement of the data outside of the catalog. Click on the Activities tab to view the data asset lineage. Click on any node to see the detail pane on the right. Do this for every node in the lineage graph to see what is tracked. Conclusion \u00b6 In this section we covered several aspects of data organization and governance. We've seen: How data is cataloged. How to search for data. How policies and rules can be applied to protect and govern sensitive data.","title":"Enterprise data governance for Viewers using Watson Knowledge Catalog"},{"location":"watson-knowledge-catalog-user/#watson-knowledge-catalog-for-users","text":"This exercise demonstrates how to solve the problems of enterprise data governance using Watson Knowledge Catalog on the Cloud Pak for Data platform. We'll explain how to use governance, data quality and active policy management in order to help your organization protect and govern sensitive data, trace data lineage and manage data lakes. This knowledge will help users quickly discover, curate, categorize and share data assets, data sets, analytical models and their relationships with other members of your organization. It serves as a single source of truth for data engineers, data stewards, data scientists and business analysts to gain self-service access to data they can trust. This section is comprised of the following steps: Find the Right Data Understand the Data Understand the Data Content","title":"Watson Knowledge Catalog for Users"},{"location":"watson-knowledge-catalog-user/#1-find-the-right-data","text":"We need to find the right data and business information related to the Mortgage Default analysis project. You can use the global search to search across catalogs, projects and the business glossary to find all assets that you may be interested in. Enter the words mortgage in the global search area and press the enter key to start finding what you need. Place your cursor inside the global search area next to the word mortgage and press the enter key: The search returns all data and information assets related to the search criteria across all catalogs, projects and governance artifacts. Scroll down through the list to take a closer look at what was found. You can further refine your search results by using the filters supplied by type, tag, catalog, project etc. First and foremost, we need to verify that all the data needed for the project has been cataloged and available to the project team to use. We can do this very easily by refining the search to only display mortgage data that exists across all catalogs. From the search drop down menu select Governance > All governance artifacts . All data assets across catalogs meeting the criteria are displayed. This is the data we are looking for. The connection to the Analytics Data Warehouse and the 4 mortgage tables are what the project team requested; Mortgage Default, Applicant, Property and Customer are all in the Enterprise catalog. However, before we proceed to the catalog we need to also find all the business information related to the project to review the terms and content of the data and identify if there are any policies and rules set by the business that the project team needs to be aware of and adhere to. From the search drop down menu select Governance > Business terms . Notice that the governance team has been hard at work and has defined 25 business terms related to the mortgage data the project will be using. Also notice that they are tagged with the key word Mortgage to easily find them Scroll down the list of business terms to view them all. Note that the Email Address, Phone Number and Social Security Number have all been tagged as Sensitive information. Click on the Clear all button to get ready for the next search task. Click on the Any type filter and select the Category type from the list to refine the results. Notice that there is a Mortgage Default Analysis category defined that contains all the business information related to the project and a category named Sensitive Information that is a subcategory. This is a good indication that the mortgage data being used by the project contains sensitive information that needs to be protected.","title":"1. Find the Right Data"},{"location":"watson-knowledge-catalog-user/#2-understand-the-data","text":"The best data is data that is fully understood and trusted. You can be confident in your data when you know where it comes from, that it complies to a set of policies and rules that address data privacy regulations and that it is clean and conforms to data quality policies, rules and standards, and that others have used it and trust it and are willing to share that information, to ensure you can produce meaningful and accurate analytical and AI results that will benefit better business outcomes.","title":"2. Understand the Data"},{"location":"watson-knowledge-catalog-user/#understand-data-policies-and-rules","text":"In this section you will use the Business glossary to gain a deeper understanding of the business terminology defined by the data steward and the governance team responsible for establishing policies and rules to govern and protect the data. Since we see that there is an indication of sensitive information Let\u2019s take a closer look at the Sensitive Information category content. Click on the Sensitive Information category from the list. The description clearly states that this subcategory contains references to business terms that relate to data that will be used by the project team that need to be governed by data protection rules. Let\u2019s get more information on the Social Security Number. In the global search area, enter the words social security number and press the enter key. A Social Security Number term appears in the list with tags of Mortgage applicant and Sensitive information. This is an indication that the Mortgage Applicant table has a social security number.","title":"Understand Data Policies and Rules"},{"location":"watson-knowledge-catalog-user/#3-understand-the-data-content","text":"You have gained an understanding of the policies and rules and information related to sensitive data and validated and trust the data quality. In this section we will go to the Enterprise catalog, which is where we identified all the data we need resides, and use all of the features it provides to gain an even better understanding of the data content and have even more confidence in the data based on what others are saying and by utilizing the AI assisted recommendations, automatic profiling and additional data content statistics provided. Click the (\u2630) hamburger menu in the upper left corner and click Catalog -> All catalogs Click on the Enterprise catalog. Watson Knowledge Catalog provides suggested assets to you based on recommendations using AI, things you might be interested in based on your past viewing history. Notice that it is already recommending Mortgage data to you based on your past searches. It also keeps track of what\u2019s hot and Highly Rated based on reviews and ranks them in order of their rating highest to lowest. Click on the Highly Rated section to see what\u2019s hot: Notice that the Analytics Data Warehouse, MORTGAGE_APPLICANT and MORTGAGE_CUSTOMER tables have been reviewed and are rated quite high. A good indication of their quality and usability. Lastly, Watson Knowledge Catalog keeps track of what\u2019s new that has been Recently Added since the last time you visited the catalog. This are all means to help you find and understand the data more quickly and easily. Click on the Recently Added section to see what\u2019s new. Notice that the MORTGAGE_APPLICANT table was the most recent data asset added to the catalog. Click on the MORTGAGE_APPLICANT table to review its content and metadata. You are brought into the Overview section of the MORTGAGE_APPLICANT table. Click on the Asset tab. You may see that Data masking is in progress and that 3 columns are being masked. The asset is being masked by the Protect Sensitive Personal Information data protection rule being enforced by the data governance team. Because you are not authorized to view the sensitive information, the data is being protected. The data that is masked is indicated with a lock icon next to their column names; EMAIL_ADDRESS, PHONE_NUMBER and SOCIAL_SECURITY_NUMBER. Scroll to the right to view all the masked columns. Click on the lock icon on the EMAIL_ADDRESS column to view the data protection message. Click on the Review tab to read the review. The table has a 5 star rating with a very positive review. Reviews can be written by anyone who has access to the catalog and the asset to notify and inform others of the content and usability of the data. Click on the Profile tab to view the data profile. As data assets are discovered and added to the catalog they are automatically profiled and classified to give end users a more in-depth understanding of the data content, quality and usability. Data classifications are used to identify what type of data it is and to autonomously enforce data protect rules to mask sensitive data, like you just saw. Scroll to the right to view the other columns. Notice that the protected data does not have any profile information displayed. Lineage is captured for every data asset in a catalog. It keeps track of where it came from, any updates or changes that have been made to its metadata and any movement of the data outside of the catalog. Click on the Activities tab to view the data asset lineage. Click on any node to see the detail pane on the right. Do this for every node in the lineage graph to see what is tracked.","title":"3. Understand the Data Content"},{"location":"watson-knowledge-catalog-user/#conclusion","text":"In this section we covered several aspects of data organization and governance. We've seen: How data is cataloged. How to search for data. How policies and rules can be applied to protect and govern sensitive data.","title":"Conclusion"}]}