{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Watson Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook will train, create and deploy a Credit Risk model, configure OpenScale to monitor that deployment, and inject seven days' worth of historical records and measurements for viewing in the OpenScale Insights dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Model building and deployment](#model)\n",
    "- [OpenScale configuration](#openscale)\n",
    "- [Quality monitor and feedback logging](#quality)\n",
    "- [Fairness, drift monitoring and explanations](#fairness)\n",
    "- [Custom monitors and metrics](#custom)\n",
    "- [Payload analytics](#analytics)\n",
    "- [Historical data](#historical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Setup <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.1 Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /home/spark/shared/user-libs/python3.6*\n",
    "\n",
    "!pip install --upgrade numpy==1.19.2 --user --no-cache | tail -n 1\n",
    "!pip install --upgrade pandas==0.25.3 --user --no-cache | tail -n 1\n",
    "!pip install --upgrade requests==2.23 --user --no-cache | tail -n 1\n",
    "!pip install --upgrade SciPy==1.5.2 --user --no-cache | tail -n 1\n",
    "!pip install --upgrade lime==0.2.0.1 --user --no-cache | tail -n 1\n",
    "!pip install --upgrade pixiedust==1.1.18 --user --no-cache | tail -n 1\n",
    "!pip install --upgrade pyspark==2.4.0 --user --no-cache | tail -n 1\n",
    "\n",
    "!pip install --upgrade ibm-cloud-sdk-core==3.3.0 --user --no-cache | tail -n 1\n",
    "!pip install --upgrade ibm-watson-machine-learning==1.0.22 --user --no-cache | tail -n 1\n",
    "!pip install --upgrade ibm-watson-openscale==3.0.1 --user --no-cache | tail -n 1\n",
    "\n",
    "!pip uninstall watson-machine-learning-client  -y | tail -n 1\n",
    "!pip uninstall watson-machine-learning-client-V4 -y | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configure credentials\n",
    "\n",
    "To authenticate the Watson Machine Learning service and Watson OpenScale on IBM Cloud, you need to provide a platform `api_key` and an endpoint URL. Where the endpoint URL is based on the `location` of the WML instance. To get these values you can use either the IBM Cloud CLI or the IBM Cloud UI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IBM Cloud CLI\n",
    "\n",
    "You can use the [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/index.html) to create a platform API Key and retrieve your instance location.\n",
    "\n",
    "- To generate the Cloud API Key, run the following commands:\n",
    "```\n",
    "ibmcloud login\n",
    "ibmcloud iam api-key-create API_KEY_NAME\n",
    "```\n",
    "  - Copy the value of `api_key` from the output.\n",
    "\n",
    "\n",
    "- To retrieve the location of your WML instance, run the following commands:\n",
    "```\n",
    "ibmcloud login --apikey API_KEY -a https://cloud.ibm.com\n",
    "ibmcloud resource service-instance \"WML_INSTANCE_NAME\"\n",
    "```\n",
    "> Note: WML_INSTANCE_NAME is the name of your Watson Machine Learning instance and should be quoted in the command.\n",
    "\n",
    "  - Copy the value of `Location` from the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IBM Cloud UI\n",
    "\n",
    "To generate Cloud API key:\n",
    "- Go to the [**Users** section of the Cloud console](https://cloud.ibm.com/iam#/users). \n",
    "- From that page, click your name in the top right corner, scroll down to the **API Keys** section, and click **Create an IBM Cloud API key**. \n",
    "- Give your key a name and click **Create**, then copy the created key and to use it below.\n",
    "\n",
    "To retrieve the location of your WML instance:\n",
    "- Go to the [**Resources List** section of the Cloud console](https://cloud.ibm.com/resources).\n",
    "- From that page, expand the **Services** section and find your Watson Machine Learning Instance.\n",
    "- Based on the Location displayed in that page, select one of the following values for location variable:\n",
    "\n",
    "|Displayed Location|Location|\n",
    "|-|-|\n",
    "|Dallas|us-south|\n",
    "|London|eu-gb|\n",
    "|Frankfurt|eu-de|\n",
    "|Tokyo|jp-tok|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOUD_API_KEY = \"<INSERT-YOUR-CLOUD-API-KEY>\"\n",
    "WML_LOCATION = \"<INSERT-YOUR-WML-LOCATION>\" # example: \"us-south\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WML_CREDENTIALS = {\n",
    "    \"apikey\": CLOUD_API_KEY,\n",
    "    \"url\": \"https://\" + WML_LOCATION + \".ml.cloud.ibm.com\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next cells, you will need to paste credentials to Cloud Object Storage. If you haven't worked with COS yet please visit [getting started with COS tutorial](https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-getting-started). \n",
    "- You can find COS_API_KEY_ID and COS_RESOURCE_CRN in the **Service Credentials** menu of your COS instance (copy the `apikey` and `resource_instance_id` respectively). The COS Service Credentials must be created with Role parameter set as Writer. \n",
    "- The COS_ENDPOINT variable can be found in Endpoint panel of your COS instance(Note: this is not the same as the `Endpoint` in the service credentials). From this page, click on one of the regions and copy the public endpoint.\n",
    "- The BUCKET_NAME can be anything you like as long as its globally unique. You an use the suggested value, appended with your initials and date.\n",
    "\n",
    "Later in the notebook, the training data file will be loaded to the bucket of your instance and used as training reference in subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COS_API_KEY_ID = \"<INSERT-YOUR-COS-API-KEY>\"\n",
    "COS_RESOURCE_CRN = \"<INSERT-YOUR-COS-RESOURCE-CRN>\"\n",
    "COS_ENDPOINT = \"<INSERT-YOUR-COS-ENDPOINT>\" #Example: \"https://s3.us.cloud-object-storage.appdomain.cloud\"\n",
    "\n",
    "BUCKET_NAME = \"<INSERT-YOUR-BUCKET-NAME>\" #Example: \"credit-risk-training-data-uniqueID\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial can use Databases for PostgreSQL, Db2 Warehouse, or a free internal verison of PostgreSQL to create a datamart for OpenScale.\n",
    "\n",
    "**For most scenarios, do not update the cell below (leave the values as they are).**\n",
    "\n",
    "If you have previously configured OpenScale, it will use your existing datamart, and not interfere with any models you are currently monitoring. Or, if you do not have a paid Cloud account or would prefer not to provision this paid service, you may use the free internal PostgreSQL service with OpenScale. Leave the values as is.\n",
    "\n",
    "Otherwise, if you want to use an external datastore as the datamart or if you previously used the internal datastore but want to delete it and create a new one:\n",
    "\n",
    "- To use a new Db2 Warehouse or Databases for PostgreSQL instance as the datamart, provision the desired service via the Cloud catalog and create a set of credentials. Copy and paste the credentials from that service into the cell below. \n",
    "- If you previously configured OpenScale to use the free internal version of PostgreSQL, you can switch to a new datamart using a paid database service. \n",
    "- If you would like to delete the internal PostgreSQL configuration and create a new one using service credentials supplied in the cell above, set the KEEP_MY_INTERNAL_POSTGRES variable below to False below. In this case, the notebook will remove your existing internal PostgreSQL datamart and create a new one with the supplied credentials. ***NO DATA MIGRATION WILL OCCUR.***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CREDENTIALS = None\n",
    "SCHEMA_NAME = None\n",
    "KEEP_MY_INTERNAL_POSTGRES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Set Custom Name\n",
    "\n",
    "Provide a custom name to be concatenated to create a model name, deployment name and open scale monitor name. Sample value for CUSTOM_NAME could be CUSTOM_NAME = 'JRT_WOSTest1020'\n",
    "\n",
    "**<font color='red'><< UPDATE THE VARIABLE 'CUSTOM_NAME' TO A UNIQUE NAME OF YOUR CHOOSING>></font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_NAME = '<INSERT-YOUR-CUSTOM-NAME-HERE>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Run the notebook\n",
    "\n",
    "At this point, the notebook is ready to run. You can either run the cells one at a time, or click the **Kernel** option above and select **Restart and Run All** to run all the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Model building and deployment <a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will learn how to train Spark MLLib model and next deploy it as web-service using Watson Machine Learning service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm german_credit_data_biased_training.csv\n",
    "with io.capture_output() as captured:\n",
    "    !wget https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/data/openscale/german_credit_data_biased_training.csv -O german_credit_data_biased_training.csv\n",
    "    \n",
    "!ls -lh german_credit_data_biased_training.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "training_data_file_name = \"german_credit_data_biased_training.csv\"\n",
    "pd_data = pd.read_csv(training_data_file_name, sep=\",\", header=0)\n",
    "df_data = spark.createDataFrame(pd_data)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of records: \" + str(df_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = df_data\n",
    "(train_data, test_data) = spark_df.randomSplit([0.8, 0.2], 24)\n",
    "\n",
    "print(\"Number of records for training: \" + str(train_data.count()))\n",
    "print(\"Number of records for evaluation: \" + str(test_data.count()))\n",
    "\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Save training data to COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config, ClientError\n",
    "\n",
    "cos_client = ibm_boto3.resource(\n",
    "    \"s3\",\n",
    "    ibm_api_key_id=COS_API_KEY_ID,\n",
    "    ibm_service_instance_id=COS_RESOURCE_CRN,\n",
    "    ibm_auth_endpoint=\"https://iam.bluemix.net/oidc/token\",\n",
    "    config=Config(signature_version=\"oauth\"),\n",
    "    endpoint_url=COS_ENDPOINT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bucket = True\n",
    "try:\n",
    "    buckets = cos_client.buckets.all()\n",
    "    for bucket in buckets:\n",
    "        if BUCKET_NAME == bucket.name:\n",
    "            print(\"Existing Bucket Found: {0}\".format(bucket.name))\n",
    "            create_bucket = False\n",
    "            break\n",
    "            \n",
    "except ClientError as be:\n",
    "    print(\"Client Error: {0}\\n\".format(be))\n",
    "except Exception as e:\n",
    "    print(\"Unable to retrieve list buckets: {0}\\n\".format(e))\n",
    "    \n",
    "if create_bucket:\n",
    "    print(\"Creating new bucket: {0}\".format(BUCKET_NAME))\n",
    "    try:\n",
    "        cos_client.create_bucket(Bucket=BUCKET_NAME)\n",
    "        print(\"Bucket: {0} created!\".format(BUCKET_NAME))\n",
    "    except ClientError as be:\n",
    "        print(\"Client Error: {0}\\n\".format(be))\n",
    "    except Exception as e:\n",
    "        print(\"Unable to create bucket: {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(training_data_file_name, \"rb\") as file_data:\n",
    "        cos_client.Object(BUCKET_NAME, training_data_file_name).upload_fileobj(\n",
    "            Fileobj=file_data\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(\"An exception occurred: {0}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Create a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will use the CUSTOM_NAME variable set earlier to create a model name and online deployment name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = CUSTOM_NAME + '_WOSNotebook_Model'\n",
    "DEPLOYMENT_NAME = CUSTOM_NAME + '_WOSNotebook_Deployment'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a Random Forest Classifier with Spark, setting up string indexers for the categorical features and the label column. Finally, this notebook creates a pipeline including the indexers and the model, and does an initial Area Under ROC evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model\n",
    "\n",
    "si_CheckingStatus = StringIndexer(inputCol = \"CheckingStatus\", outputCol = \"CheckingStatus_IX\")\n",
    "si_CreditHistory = StringIndexer(inputCol = \"CreditHistory\", outputCol = \"CreditHistory_IX\")\n",
    "si_LoanPurpose = StringIndexer(inputCol = \"LoanPurpose\", outputCol = \"LoanPurpose_IX\")\n",
    "si_ExistingSavings = StringIndexer(inputCol = \"ExistingSavings\", outputCol = \"ExistingSavings_IX\")\n",
    "si_EmploymentDuration = StringIndexer(inputCol = \"EmploymentDuration\", outputCol = \"EmploymentDuration_IX\")\n",
    "si_Sex = StringIndexer(inputCol = \"Sex\", outputCol = \"Sex_IX\")\n",
    "si_OthersOnLoan = StringIndexer(inputCol = \"OthersOnLoan\", outputCol = \"OthersOnLoan_IX\")\n",
    "si_OwnsProperty = StringIndexer(inputCol = \"OwnsProperty\", outputCol = \"OwnsProperty_IX\")\n",
    "si_InstallmentPlans = StringIndexer(inputCol = \"InstallmentPlans\", outputCol = \"InstallmentPlans_IX\")\n",
    "si_Housing = StringIndexer(inputCol = \"Housing\", outputCol = \"Housing_IX\")\n",
    "si_Job = StringIndexer(inputCol = \"Job\", outputCol = \"Job_IX\")\n",
    "si_Telephone = StringIndexer(inputCol = \"Telephone\", outputCol = \"Telephone_IX\")\n",
    "si_ForeignWorker = StringIndexer(inputCol = \"ForeignWorker\", outputCol = \"ForeignWorker_IX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "si_Label = StringIndexer(inputCol=\"Risk\", outputCol=\"label\").fit(spark_df)\n",
    "label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_Label.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "va_features = VectorAssembler(inputCols=[\"CheckingStatus_IX\", \"CreditHistory_IX\", \"LoanPurpose_IX\", \"ExistingSavings_IX\", \"EmploymentDuration_IX\", \"Sex_IX\", \\\n",
    "                                         \"OthersOnLoan_IX\", \"OwnsProperty_IX\", \"InstallmentPlans_IX\", \"Housing_IX\", \"Job_IX\", \"Telephone_IX\", \"ForeignWorker_IX\", \\\n",
    "                                         \"LoanDuration\", \"LoanAmount\", \"InstallmentPercent\", \"CurrentResidenceDuration\", \"LoanDuration\", \"Age\", \"ExistingCreditsCount\", \\\n",
    "                                         \"Dependents\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[si_CheckingStatus, si_CreditHistory, si_EmploymentDuration, si_ExistingSavings, si_ForeignWorker, si_Housing, si_InstallmentPlans, si_Job, si_LoanPurpose, si_OthersOnLoan,\\\n",
    "                               si_OwnsProperty, si_Sex, si_Telephone, si_Label, va_features, classifier, label_converter])\n",
    "model = pipeline.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Evalutate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)\n",
    "evaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName=\"areaUnderROC\")\n",
    "area_under_curve = evaluatorDT.evaluate(predictions)\n",
    "\n",
    "evaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName=\"areaUnderPR\")\n",
    "area_under_PR = evaluatorDT.evaluate(predictions)\n",
    "\n",
    "print(\"areaUnderROC = %g\" % area_under_curve, \"areaUnderPR = %g\" % area_under_PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = predictions.toPandas()['prediction']\n",
    "y_pred = ['Risk' if pred == 1.0 else 'No Risk' for pred in y_pred]\n",
    "y_test = test_data.toPandas()['Risk']\n",
    "print(classification_report(y_test, y_pred, target_names=['Risk', 'No Risk']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Publish the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the notebook uses Watson Machine Learning to save the model (including the pipeline) to the WML instance. Previous versions of the model are removed so that the notebook can be run again, resetting all data for another demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watson_machine_learning import APIClient\n",
    "\n",
    "wml_client = APIClient(WML_CREDENTIALS)\n",
    "wml_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Set default space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deploy a model, you would have to create deployment spaces and deploy your models there. You can list all the spaces using the .list() function, or you can create new spaces by going to menu on top left corner --> analyze --> analytics deployments --> New Deployment Space. Once you know which space you want to deploy in, simply use the GUID of the space as argument for .set.default_space() function below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.spaces.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'><< UPDATE THE VARIABLE 'DEPLOYMENT_SPACE_NAME' TO THE NAME OF THE DEPLOYMENT SPACE YOU CREATED PREVIOUSLY>></font>**\n",
    "\n",
    "You should copy the name of your deployment space from the output of the previous cell to the variable in the next cell. The deployment space ID will be looked up based on the name specified below. If you do not receive a space GUID as an output to the next cell, do not proceed until you have created a deployment space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYMENT_SPACE_NAME = \"<INSERT_YOUR_DEPLOYMENT_SPACE_NAME_FROM_ABOVE_OUTPUT>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.spaces.list()\n",
    "all_spaces = wml_client.spaces.get_details()['resources']\n",
    "space_id = None\n",
    "for space in all_spaces:\n",
    "    if space['entity']['name'] == DEPLOYMENT_SPACE_NAME:\n",
    "        space_id = space[\"metadata\"][\"id\"]\n",
    "        print(\"\\nDeployment Space ID: \", space_id)\n",
    "\n",
    "if space_id is None:\n",
    "    print(\"WARNING: Your space does not exist. Create a deployment space before proceeding to the next cell.\")\n",
    "    #space_id = client.spaces.store(meta_props={client.spaces.ConfigurationMetaNames.NAME: space_name})[\"metadata\"][\"guid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WML_SPACE_ID = space_id\n",
    "wml_client.set.default_space(WML_SPACE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Remove existing model and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.repository.list_models()\n",
    "wml_client.deployments.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    deployments_list = wml_client.deployments.get_details()\n",
    "    for deployment in deployments_list[\"resources\"]:\n",
    "        model_id = deployment[\"entity\"][\"asset\"][\"id\"]\n",
    "        deployment_id = deployment[\"metadata\"][\"id\"]\n",
    "        if deployment[\"metadata\"][\"name\"] == DEPLOYMENT_NAME:\n",
    "            print(\"Deleting deployment id\", deployment_id)\n",
    "            wml_client.deployments.delete(deployment_id)\n",
    "            print(\"Deleting model id\", model_id)\n",
    "            wml_client.repository.delete(model_id)\n",
    "            wml_client.repository.list_models()\n",
    "            wml_client.deployments.list()\n",
    "except Exception as e:\n",
    "    print(\"An exception occurred: {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "software_spec_uid = wml_client.software_specifications.get_id_by_name(\"spark-mllib_2.4\")\n",
    "print(\"Software Specification ID: {}\".format(software_spec_uid))\n",
    "\n",
    "model_props = {\n",
    "    wml_client.repository.ModelMetaNames.NAME: \"{}\".format(MODEL_NAME),\n",
    "    wml_client._models.ConfigurationMetaNames.SPACE_UID: WML_SPACE_ID,\n",
    "    wml_client.repository.ModelMetaNames.TYPE: 'mllib_2.4',\n",
    "    wml_client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: software_spec_uid\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "published_model_details = wml_client.repository.store_model(model, model_props, training_data=df_data, pipeline=pipeline)\n",
    "model_uid = wml_client.repository.get_model_uid(published_model_details)\n",
    "\n",
    "print(\"Published Model Details: \")\n",
    "print(json.dumps(published_model_details, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section of the notebook deploys the model as a RESTful web service in Watson Machine Learning. The deployed model will have a scoring URL you can use to send data to the model for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Deploying model...\")\n",
    "meta_props = {\n",
    "    wml_client.deployments.ConfigurationMetaNames.NAME: DEPLOYMENT_NAME,\n",
    "    wml_client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "}\n",
    "deployment = wml_client.deployments.create(model_uid, meta_props=meta_props)\n",
    "deployment_uid = wml_client.deployments.get_uid(deployment)\n",
    "scoring_url = wml_client.deployments.get_scoring_href(deployment)\n",
    " \n",
    "print(\"Model id: {}\".format(model_uid))\n",
    "print(\"Deployment id: {}\".format(deployment_uid))\n",
    "print(\"Scoring URL:{}\".format(scoring_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wml_client.repository.list_models()\n",
    "wml_client.deployments.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 Call the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"CheckingStatus\", \"LoanDuration\", \"CreditHistory\", \"LoanPurpose\", \"LoanAmount\", \"ExistingSavings\",\n",
    "                  \"EmploymentDuration\", \"InstallmentPercent\", \"Sex\", \"OthersOnLoan\", \"CurrentResidenceDuration\",\n",
    "                  \"OwnsProperty\", \"Age\", \"InstallmentPlans\", \"Housing\", \"ExistingCreditsCount\", \"Job\", \"Dependents\",\n",
    "                  \"Telephone\", \"ForeignWorker\"]\n",
    "values = [\n",
    "            [\"no_checking\", 13, \"credits_paid_to_date\", \"car_new\", 1343, \"100_to_500\", \"1_to_4\", 2, \"female\", \"none\", 3,\n",
    "             \"savings_insurance\", 46, \"none\", \"own\", 2, \"skilled\", 1, \"none\", \"yes\"],\n",
    "            [\"no_checking\", 24, \"prior_payments_delayed\", \"furniture\", 4567, \"500_to_1000\", \"1_to_4\", 4, \"male\", \"none\",\n",
    "             4, \"savings_insurance\", 36, \"none\", \"free\", 2, \"management_self-employed\", 1, \"none\", \"yes\"],\n",
    "        ]\n",
    "\n",
    "scoring_payload = {\"input_data\": [{\"fields\": fields, \"values\": values}]}\n",
    "predictions = wml_client.deployments.score(deployment_uid, scoring_payload)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Configure OpenScale <a name=\"openscale\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook will now import the necessary libraries and set up a Python OpenScale client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "from ibm_watson_openscale import *\n",
    "from ibm_watson_openscale.supporting_classes.enums import *\n",
    "from ibm_watson_openscale.supporting_classes import *\n",
    "from ibm_watson_openscale.base_classes.watson_open_scale_v2 import *\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticator = IAMAuthenticator(apikey=CLOUD_API_KEY)\n",
    "wos_client = APIClient(authenticator=authenticator)\n",
    "\n",
    "#wos_client = APIClient(\n",
    "#    service_instance_id=\"xxxxxxxxxxxxxxxxxxxx\",\n",
    "#    authenticator=authenticator\n",
    "#)\n",
    "\n",
    "wos_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create datamart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Set up datamart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watson OpenScale uses a database to store payload logs and calculated metrics. If database credentials were **not** supplied above, the notebook will use the free, internal lite database. If database credentials were supplied, the datamart will be created there **unless** there is an existing datamart **and** the **KEEP_MY_INTERNAL_POSTGRES** variable is set to **True**. If an OpenScale datamart exists in Db2 or PostgreSQL, the existing datamart will be used and no data will be overwritten.\n",
    "\n",
    "Prior instances of the German Credit model will be removed from OpenScale monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.data_marts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_marts = wos_client.data_marts.list().result.data_marts\n",
    "if len(data_marts) == 0:\n",
    "    if DB_CREDENTIALS is not None:\n",
    "        if SCHEMA_NAME is None: \n",
    "            print(\"Please specify the SCHEMA_NAME and rerun the cell\")\n",
    "\n",
    "        print(\"Setting up external datamart\")\n",
    "        added_data_mart_result = wos_client.data_marts.add(\n",
    "                background_mode=False,\n",
    "                name=\"WOS Data Mart\",\n",
    "                description=\"Data Mart created by WOS tutorial notebook\",\n",
    "                database_configuration=DatabaseConfigurationRequest(\n",
    "                  database_type=DatabaseType.POSTGRESQL,\n",
    "                    credentials=PrimaryStorageCredentialsLong(\n",
    "                        hostname=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"hosts\"][0][\"hostname\"],\n",
    "                        username=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"authentication\"][\"username\"],\n",
    "                        password=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"authentication\"][\"password\"],\n",
    "                        db=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"database\"],\n",
    "                        port=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"hosts\"][0][\"port\"],\n",
    "                        ssl=True,\n",
    "                        sslmode=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"query_options\"][\"sslmode\"],\n",
    "                        certificate_base64=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"certificate\"][\"certificate_base64\"]\n",
    "                    ),\n",
    "                    location=LocationSchemaName(\n",
    "                        schema_name= SCHEMA_NAME\n",
    "                    )\n",
    "                )\n",
    "             ).result\n",
    "    else:\n",
    "        print(\"Setting up internal datamart\")\n",
    "        added_data_mart_result = wos_client.data_marts.add(\n",
    "                background_mode=False,\n",
    "                name=\"WOS Data Mart\",\n",
    "                description=\"Data Mart created by WOS tutorial notebook\", \n",
    "                internal_database = True).result\n",
    "        \n",
    "    data_mart_id = added_data_mart_result.metadata.id\n",
    "    \n",
    "else:\n",
    "    data_mart_id=data_marts[0].metadata.id\n",
    "    print(\"Using existing datamart {}\".format(data_mart_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2  Add Service Provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watson OpenScale needs to be bound to the Watson Machine Learning instance to capture payload data into and out of the model. If this binding already exists, this code will output a warning message and use the existing binding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.service_providers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_PROVIDER_NAME = \"Watson Machine Learning V2 Instance\"\n",
    "SERVICE_PROVIDER_DESCRIPTION = \"WML Instance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Remove existing service provider connected with used WML instance.\n",
    "\n",
    "Multiple service providers for the same engine instance are avaiable in Watson OpenScale. To avoid multiple service providers of used WML instance in the tutorial notebook the following code deletes existing service provder(s) and then adds new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_providers = wos_client.service_providers.list().result.service_providers\n",
    "for service_provider in service_providers:\n",
    "    service_instance_name = service_provider.entity.name\n",
    "    if service_instance_name == SERVICE_PROVIDER_NAME:\n",
    "        service_provider_id = service_provider.metadata.id\n",
    "        wos_client.service_providers.delete(service_provider_id)\n",
    "        print(\"Deleted existing service_provider for WML instance: {}\".format(service_provider_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Add Service Provider\n",
    "\n",
    "Watson OpenScale needs to be bound to the Watson Machine Learning instance to capture payload data into and out of the model.\n",
    "\n",
    "**Note:** You can bind more than one engine instance if needed by calling `wos_client.service_providers.add` method. Next, you can refer to particular service provider using `service_provider_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_service_provider_result = wos_client.service_providers.add(\n",
    "        name=SERVICE_PROVIDER_NAME,\n",
    "        description=SERVICE_PROVIDER_DESCRIPTION,\n",
    "        service_type=ServiceTypes.WATSON_MACHINE_LEARNING,\n",
    "        deployment_space_id = WML_SPACE_ID,\n",
    "        operational_space_id = \"production\",\n",
    "        credentials=WMLCredentialsCloud(\n",
    "            apikey=CLOUD_API_KEY,\n",
    "            url=WML_CREDENTIALS[\"url\"],\n",
    "            instance_id=None\n",
    "        ),\n",
    "        background_mode=False\n",
    "    ).result\n",
    "service_provider_id = added_service_provider_result.metadata.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.service_providers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_deployment_details = wos_client.service_providers.list_assets(data_mart_id=data_mart_id, service_provider_id=service_provider_id, deployment_space_id = WML_SPACE_ID).result['resources'][0]\n",
    "model_asset_details_from_deployment = wos_client.service_providers.get_deployment_asset(data_mart_id=data_mart_id,service_provider_id=service_provider_id,deployment_id=deployment_uid,deployment_space_id=WML_SPACE_ID)\n",
    "\n",
    "all_assets_response = wos_client.service_providers.list_assets(\n",
    "        data_mart_id,\n",
    "        service_provider_id,\n",
    "        deployment_space_id=WML_SPACE_ID\n",
    "     ).result\n",
    "print(json.dumps(all_assets_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Subscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.subscriptions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Remove existing credit risk subscriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code removes previous subscriptions to the Credit model to refresh the monitors with the new model and new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subscriptions = wos_client.subscriptions.list().result.subscriptions\n",
    "for subscription in subscriptions:\n",
    "    sub_model_id = subscription.entity.asset.asset_id\n",
    "    if sub_model_id == model_uid:\n",
    "        wos_client.subscriptions.delete(subscription.metadata.id)\n",
    "        print(\"Deleted existing subscription for model: {}\".format(model_uid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Add new credit risk model subscription.\n",
    "\n",
    "This code creates the model subscription in OpenScale using the Python client API. Note that we need to provide the model unique identifier, and some information about the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset = Asset(\n",
    "    asset_id=model_uid,\n",
    "    url=deployment[\"entity\"][\"status\"][\"online_url\"][\"url\"],\n",
    "    name=model_asset_details_from_deployment[\"entity\"][\"asset\"][\"name\"],\n",
    "    asset_type=AssetTypes.MODEL,\n",
    "    input_data_type=InputDataType.STRUCTURED,\n",
    "    problem_type=ProblemType.BINARY_CLASSIFICATION\n",
    ")\n",
    "asset_deployment = AssetDeploymentRequest(\n",
    "    deployment_id=deployment_uid,\n",
    "    name=DEPLOYMENT_NAME,\n",
    "    deployment_type=DeploymentTypes.ONLINE,\n",
    "    url=deployment[\"entity\"][\"status\"][\"online_url\"][\"url\"]\n",
    ")\n",
    "training_data_reference = TrainingDataReference(\n",
    "   type=\"cos\",\n",
    "   location=COSTrainingDataReferenceLocation(\n",
    "       bucket=BUCKET_NAME,\n",
    "       file_name=training_data_file_name\n",
    "   ),\n",
    "   connection=COSTrainingDataReferenceConnection.from_dict(\n",
    "       {\n",
    "           \"resource_instance_id\": COS_RESOURCE_CRN,\n",
    "           \"url\": COS_ENDPOINT,\n",
    "           \"api_key\": COS_API_KEY_ID,\n",
    "           \"iam_url\": \"https://iam.bluemix.net/oidc/token\"\n",
    "       }\n",
    "   )\n",
    ")\n",
    "\n",
    "asset_properties_request = AssetPropertiesRequest(\n",
    "    label_column=\"Risk\",\n",
    "    prediction_field='predictedLabel',\n",
    "    probability_fields=[\"probability\"],\n",
    "    feature_fields = [\"CheckingStatus\",\"LoanDuration\",\"CreditHistory\",\"LoanPurpose\",\"LoanAmount\",\"ExistingSavings\",\"EmploymentDuration\",\"InstallmentPercent\",\"Sex\",\"OthersOnLoan\",\"CurrentResidenceDuration\",\"OwnsProperty\",\"Age\",\"InstallmentPlans\",\"Housing\",\"ExistingCreditsCount\",\"Job\",\"Dependents\",\"Telephone\",\"ForeignWorker\"],\n",
    "    categorical_fields = [\"CheckingStatus\",\"CreditHistory\",\"LoanPurpose\",\"ExistingSavings\",\"EmploymentDuration\",\"Sex\",\"OthersOnLoan\",\"OwnsProperty\",\"InstallmentPlans\",\"Housing\",\"Job\",\"Telephone\",\"ForeignWorker\"],\n",
    "    training_data_reference=training_data_reference,\n",
    "    training_data_schema=SparkStruct.from_dict(model_asset_details_from_deployment[\"entity\"][\"asset_properties\"][\"training_data_schema\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subscription_details = wos_client.subscriptions.add(\n",
    "        data_mart_id=data_mart_id,\n",
    "        service_provider_id=service_provider_id,\n",
    "        asset=asset,\n",
    "        deployment=asset_deployment,\n",
    "        asset_properties=asset_properties_request).result\n",
    "subscription_id = subscription_details.metadata.id\n",
    "print(subscription_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Check Payload Logging Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "payload_data_set_id = None\n",
    "payload_data_set_id = wos_client.data_sets.list(type=DataSetTypes.PAYLOAD_LOGGING, \n",
    "                                                target_target_id=subscription_id, \n",
    "                                                target_target_type=TargetTypes.SUBSCRIPTION).result.data_sets[0].metadata.id\n",
    "if payload_data_set_id is None:\n",
    "    print(\"Payload data set not found. Please check subscription status.\")\n",
    "else:\n",
    "    print(\"Payload data set id: {}\".format(payload_data_set_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wos_client.data_sets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.subscriptions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Score the model so we can configure monitors\n",
    "\n",
    "Now that the WML service has been bound and the subscription has been created, we need to send a request to the model before we configure OpenScale. This allows OpenScale to create a payload log in the datamart with the correct schema, so it can capture data coming into and out of the model. First, the code gets the model deployment's endpoint URL, and then sends a few records for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fields = [\"CheckingStatus\",\"LoanDuration\",\"CreditHistory\",\"LoanPurpose\",\"LoanAmount\",\"ExistingSavings\",\"EmploymentDuration\",\n",
    "          \"InstallmentPercent\",\"Sex\",\"OthersOnLoan\",\"CurrentResidenceDuration\",\"OwnsProperty\",\"Age\",\"InstallmentPlans\",\n",
    "          \"Housing\",\"ExistingCreditsCount\",\"Job\",\"Dependents\",\"Telephone\",\"ForeignWorker\"]\n",
    "values = [\n",
    "  [\"no_checking\",13,\"credits_paid_to_date\",\"car_new\",1343,\"100_to_500\",\"1_to_4\",2,\"female\",\"none\",3,\"savings_insurance\",46,\"none\",\"own\",2,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"no_checking\",24,\"prior_payments_delayed\",\"furniture\",4567,\"500_to_1000\",\"1_to_4\",4,\"male\",\"none\",4,\"savings_insurance\",36,\"none\",\"free\",2,\"management_self-employed\",1,\"none\",\"yes\"],\n",
    "  [\"0_to_200\",26,\"all_credits_paid_back\",\"car_new\",863,\"less_100\",\"less_1\",2,\"female\",\"co-applicant\",2,\"real_estate\",38,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"0_to_200\",14,\"no_credits\",\"car_new\",2368,\"less_100\",\"1_to_4\",3,\"female\",\"none\",3,\"real_estate\",29,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"0_to_200\",4,\"no_credits\",\"car_new\",250,\"less_100\",\"unemployed\",2,\"female\",\"none\",3,\"real_estate\",23,\"none\",\"rent\",1,\"management_self-employed\",1,\"none\",\"yes\"],\n",
    "  [\"no_checking\",17,\"credits_paid_to_date\",\"car_new\",832,\"100_to_500\",\"1_to_4\",2,\"male\",\"none\",2,\"real_estate\",42,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n",
    "  [\"no_checking\",33,\"outstanding_credit\",\"appliances\",5696,\"unknown\",\"greater_7\",4,\"male\",\"co-applicant\",4,\"unknown\",54,\"none\",\"free\",2,\"skilled\",1,\"yes\",\"yes\"],\n",
    "  [\"0_to_200\",13,\"prior_payments_delayed\",\"retraining\",1375,\"100_to_500\",\"4_to_7\",3,\"male\",\"none\",3,\"real_estate\",37,\"none\",\"own\",2,\"management_self-employed\",1,\"none\",\"yes\"]\n",
    "]\n",
    "\n",
    "payload_scoring = {\"input_data\": [{\"fields\": fields, \"values\": values}]}\n",
    "predictions = wml_client.deployments.score(deployment_uid, payload_scoring)\n",
    "for pred in predictions[\"predictions\"][0][\"values\"]:\n",
    "    print(\"Scoring result: {}\".format(pred[-1]))#last item in the values array is the prediction for Spark classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4.1 Validate Payload Logging\n",
    "\n",
    "Check that automatic storing of scoring to payload logging table occured. If the payload logging table does not have expected number of records, we could manually store the records into the payload logging table.\n",
    "\n",
    "The number of records in the payload logging table should be equal to number of records submitted in previous cell (i.e 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from ibm_watson_openscale.supporting_classes.payload_record import PayloadRecord\n",
    "\n",
    "time.sleep(5)\n",
    "pl_records_count = wos_client.data_sets.get_records_count(payload_data_set_id)\n",
    "print(\"Number of records in the payload logging table: {}\".format(pl_records_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If automatic logging is not happening, we could manual log them\n",
    "if pl_records_count == 0:\n",
    "    print(\"Payload logging did not happen, performing explicit payload logging.\")\n",
    "    wos_client.data_sets.store_records(data_set_id=payload_data_set_id, request_body=[PayloadRecord(\n",
    "                   scoring_id=str(uuid.uuid4()),\n",
    "                   request=payload_scoring,\n",
    "                   response=predictions,\n",
    "                   response_time=460\n",
    "               )])\n",
    "    time.sleep(5)\n",
    "    pl_records_count = wos_client.data_sets.get_records_count(payload_data_set_id)\n",
    "    print(\"Number of records in the payload logging table: {}\".format(pl_records_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Quality monitoring and feedback logging <a name=\"quality\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Enable quality monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below waits ten seconds to allow the payload logging table to be set up before it begins enabling monitors. First, it turns on the quality (accuracy) monitor and sets an alert threshold of 70%. OpenScale will show an alert on the dashboard if the model accuracy measurement (area under the curve, in the case of a binary classifier) falls below this threshold.\n",
    "\n",
    "The second paramater supplied, min_records, specifies the minimum number of feedback records OpenScale needs before it calculates a new measurement. The quality monitor runs hourly, but the accuracy reading in the dashboard will not change until an additional 50 feedback records have been added, via the user interface, the Python client, or the supplied feedback endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "target = Target(\n",
    "        target_type=TargetTypes.SUBSCRIPTION,\n",
    "        target_id=subscription_id\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"min_feedback_data_size\": 50\n",
    "}\n",
    "\n",
    "thresholds = [\n",
    "    MetricThresholdOverride(\n",
    "        metric_id=\"area_under_roc\",\n",
    "        type=MetricThresholdTypes.LOWER_LIMIT,\n",
    "        value=0.7\n",
    "    )\n",
    "]\n",
    "\n",
    "quality_monitor_details = wos_client.monitor_instances.create(\n",
    "    data_mart_id=data_mart_id,\n",
    "    background_mode=False,\n",
    "    monitor_definition_id=wos_client.monitor_definitions.MONITORS.QUALITY.ID,\n",
    "    target=target,\n",
    "    parameters=parameters,\n",
    "    thresholds=thresholds\n",
    ").result\n",
    "\n",
    "quality_monitor_instance_id = quality_monitor_details.metadata.id\n",
    "print(\"Quality Monitor ID: {}\".format(quality_monitor_instance_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feedback logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below downloads and stores enough feedback data to meet the minimum threshold so that OpenScale can calculate a new accuracy measurement. It then kicks off the accuracy monitor. The monitors run hourly, or can be initiated via the Python API, the REST API, or the graphical user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm additional_feedback_data_v2.json\n",
    "with io.capture_output() as captured:\n",
    "    !wget https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/data/openscale/additional_feedback_data_v2.json -O additional_feedback_data_v2.json\n",
    "    \n",
    "!ls -lh additional_feedback_data_v2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_dataset_id = None\n",
    "feedback_dataset = wos_client.data_sets.list(type=DataSetTypes.FEEDBACK, \n",
    "                                                target_target_id=subscription_id, \n",
    "                                                target_target_type=TargetTypes.SUBSCRIPTION).result\n",
    "\n",
    "feedback_dataset_id = feedback_dataset.data_sets[0].metadata.id\n",
    "if feedback_dataset_id is None:\n",
    "    print(\"Feedback data set not found. Please check quality monitor status.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('additional_feedback_data_v2.json') as feedback_file:\n",
    "    additional_feedback_data = json.load(feedback_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.data_sets.get_records_count(data_set_id=feedback_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wos_client.data_sets.store_records(feedback_dataset_id, request_body=additional_feedback_data, background_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.data_sets.get_records_count(data_set_id=feedback_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Run Quality Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_details = wos_client.monitor_instances.run(monitor_instance_id=quality_monitor_instance_id, background_mode=False).result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=quality_monitor_instance_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Check Monitors\n",
    "\n",
    "We can show which monitors are currently enabled, at this point, it should only be the Quality Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show()\n",
    "wos_client.data_sets.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Fairness, drift monitoring and explanations \n",
    " <a name=\"fairness\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below configures fairness monitoring for our model. It turns on monitoring for two features, Sex and Age. In each case, we must specify:\n",
    "  * Which model feature to monitor\n",
    "  * One or more **majority** groups, which are values of that feature that we expect to receive a higher percentage of favorable outcomes\n",
    "  * One or more **minority** groups, which are values of that feature that we expect to receive a higher percentage of unfavorable outcomes\n",
    "  * The threshold at which we would like OpenScale to display an alert if the fairness measurement falls below (in this case, 95%)\n",
    "\n",
    "Additionally, we must specify which outcomes from the model are favourable outcomes, and which are unfavourable. We must also provide the number of records OpenScale will use to calculate the fairness score. In this case, OpenScale's fairness monitor will run hourly, but will not calculate a new fairness rating until at least 100 records have been added. Finally, to calculate fairness, OpenScale must perform some calculations on the training data, so we provide the dataframe containing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Enable Fairness Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target = Target(\n",
    "    target_type=TargetTypes.SUBSCRIPTION,\n",
    "    target_id=subscription_id\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"features\": [\n",
    "        {\"feature\": \"Sex\",\n",
    "         \"majority\": ['male'],\n",
    "         \"minority\": ['female'],\n",
    "         \"threshold\": 0.95\n",
    "         },\n",
    "        {\"feature\": \"Age\",\n",
    "         \"majority\": [[26, 75]],\n",
    "         \"minority\": [[18, 25]],\n",
    "         \"threshold\": 0.95\n",
    "         }\n",
    "    ],\n",
    "    \"favourable_class\": [\"No Risk\"],\n",
    "    \"unfavourable_class\": [\"Risk\"],\n",
    "    \"min_records\": 100\n",
    "}\n",
    "\n",
    "thresholds = [\n",
    "    MetricThresholdOverride(\n",
    "        metric_id=\"fairness_value\",\n",
    "        type=MetricThresholdTypes.LOWER_LIMIT,\n",
    "        value=0.95,\n",
    "        specific_values=[\n",
    "            MetricSpecificThresholdShortObject(\n",
    "                value=.95,\n",
    "                applies_to = [\n",
    "                    ThresholdConditionObject(\n",
    "                        type = \"tag\",\n",
    "                        value = \"Sex\",\n",
    "                        key = \"feature\"\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            MetricSpecificThresholdShortObject(\n",
    "                value=.95,\n",
    "                applies_to = [\n",
    "                    ThresholdConditionObject(\n",
    "                        type = \"tag\",\n",
    "                        value = \"Age\",\n",
    "                        key = \"feature\"\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "fairness_monitor_details = wos_client.monitor_instances.create(\n",
    "    data_mart_id=data_mart_id,\n",
    "    background_mode=False,\n",
    "    monitor_definition_id=wos_client.monitor_definitions.MONITORS.FAIRNESS.ID,\n",
    "    target=target,\n",
    "    parameters=parameters,\n",
    "    thresholds=thresholds\n",
    ").result\n",
    "\n",
    "fairness_monitor_instance_id =fairness_monitor_details.metadata.id\n",
    "print(\"Fairness Monitor ID: {}\".format(fairness_monitor_instance_id))\n",
    "#print(fairness_monitor_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Enable Drift Monitoring\n",
    "\n",
    "We can choose to enable model and/or data drift within the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_instances = wos_client.monitor_instances.list().result.monitor_instances\n",
    "for monitor_instance in monitor_instances:\n",
    "    monitor_def_id=monitor_instance.entity.monitor_definition_id\n",
    "    if monitor_def_id == \"drift\" and monitor_instance.entity.target.target_id == subscription_id:\n",
    "        wos_client.monitor_instances.delete(monitor_instance.metadata.id, background_mode=False)\n",
    "        print('Deleted existing drift monitor instance with id: ', monitor_instance.metadata.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Target(\n",
    "    target_type=TargetTypes.SUBSCRIPTION,\n",
    "    target_id=subscription_id\n",
    "\n",
    ")\n",
    "parameters = {\n",
    "    \"min_samples\": 100,\n",
    "    \"drift_threshold\": 0.05,\n",
    "    \"train_drift_model\": True,\n",
    "    \"enable_model_drift\": True,\n",
    "    \"enable_data_drift\": True\n",
    "}\n",
    "\n",
    "drift_monitor_details = wos_client.monitor_instances.create(\n",
    "    data_mart_id=data_mart_id,\n",
    "    background_mode=False,\n",
    "    monitor_definition_id=wos_client.monitor_definitions.MONITORS.DRIFT.ID,\n",
    "    target=target,\n",
    "    parameters=parameters\n",
    ").result\n",
    "\n",
    "drift_monitor_instance_id = drift_monitor_details.metadata.id\n",
    "print(\"Drift Monitor ID: {}\".format(drift_monitor_instance_id))\n",
    "print(drift_monitor_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Score the model again now that monitoring is configured\n",
    "\n",
    "This next section randomly selects 200 records from the data feed and sends those records to the model for predictions. This is enough to exceed the minimum threshold for records set in the previous section, which allows OpenScale to begin calculating fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm german_credit_feed.json\n",
    "with io.capture_output() as captured:\n",
    "    !wget https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/data/openscale/german_credit_feed.json -O german_credit_feed.json\n",
    "\n",
    "!ls -lh german_credit_feed.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score 200 randomly chosen records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open('german_credit_feed.json', 'r') as scoring_file:\n",
    "    scoring_data = json.load(scoring_file)\n",
    "\n",
    "fields = scoring_data['fields']\n",
    "values = []\n",
    "for _ in range(200):\n",
    "    values.append(random.choice(scoring_data['values']))\n",
    "payload_scoring = {\"input_data\": [{\"fields\": fields, \"values\": values}]}\n",
    "scoring_response = wml_client.deployments.score(deployment_uid, payload_scoring)\n",
    "print(\"Single record scoring result:\", scoring_response[\"predictions\"][0][\"values\"][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "\n",
    "# Manually log the scoring response if its still the original 8 from before (should be 208 at this point).\n",
    "if pl_records_count == 8:\n",
    "    print(\"Payload logging did not happen, performing explicit payload logging.\")\n",
    "    wos_client.data_sets.store_records(data_set_id=payload_data_set_id, request_body=[PayloadRecord(\n",
    "                   scoring_id=str(uuid.uuid4()),\n",
    "                   request=payload_scoring,\n",
    "                   response=scoring_response,\n",
    "                   response_time=460\n",
    "               )])\n",
    "    \n",
    "pl_records_count = wos_client.data_sets.get_records_count(payload_data_set_id)\n",
    "print(\"Number of records in the payload logging table: {}\".format(pl_records_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: The payload table should have a total of 208 records. However, this can vary depending on how many scoring requests have been sent to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Run Monitors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 Fairness Monitor\n",
    "\n",
    "Kick off a fairness monitor run on current data. The monitor runs hourly, but can be manually initiated using the Python client, the REST API, or the graphical user interface. We have a 10 second sleep so that the scoring of 200 payloads above can complete. NOTE: if the cell below finishes with errors, skip it and complete the notebook, then return and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "run_details = None\n",
    "try:\n",
    "    run_details = wos_client.monitor_instances.run(monitor_instance_id=fairness_monitor_instance_id, background_mode=False)\n",
    "except Exception as e:\n",
    "    print(\"An exception occurred: {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=fairness_monitor_instance_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 Drift Monitor\n",
    "\n",
    "Kick off a drift monitor run on current data. The monitor runs every hour, but can be manually initiated using the Python client, the REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "drift_run_details = None\n",
    "try:\n",
    "    drift_run_details = wos_client.monitor_instances.run(monitor_instance_id=drift_monitor_instance_id, background_mode=False)\n",
    "except Exception as e:\n",
    "    print(\"An exception occurred: {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=drift_monitor_instance_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Configure Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we provide OpenScale with the training data to enable and configure the explainability features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = Target(\n",
    "    target_type=TargetTypes.SUBSCRIPTION,\n",
    "    target_id=subscription_id\n",
    ")\n",
    "parameters = {\n",
    "    \"enabled\": True\n",
    "}\n",
    "explainability_details = wos_client.monitor_instances.create(\n",
    "    data_mart_id=data_mart_id,\n",
    "    background_mode=False,\n",
    "    monitor_definition_id=wos_client.monitor_definitions.MONITORS.EXPLAINABILITY.ID,\n",
    "    target=target,\n",
    "    parameters=parameters\n",
    ").result\n",
    "\n",
    "explainability_monitor_id = explainability_details.metadata.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 Run Explanation for Sample Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_records_resp = wos_client.data_sets.get_list_of_records(data_set_id=payload_data_set_id, limit=1, offset=0).result\n",
    "scoring_ids = [pl_records_resp[\"records\"][0][\"entity\"][\"values\"][\"scoring_id\"]]\n",
    "explanation_types = [\"lime\", \"contrastive\"]\n",
    "print(\"Running explanations on scoring IDs: {}\".format(scoring_ids))\n",
    "post_exp_task_resp = wos_client.monitor_instances.explanation_tasks(scoring_ids=scoring_ids, explanation_types=explanation_types).result\n",
    "print(post_exp_task_resp)\n",
    "explanation_task_id = post_exp_task_resp.metadata.explanation_task_ids[0] #Pick up the first explanation task id from list.\n",
    "print(\"Explanations task ID: {}\".format(explanation_task_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_limit = 5\n",
    "def poll_explainability_task(client, epx_task_id):\n",
    "    import time\n",
    "    pcounter = 1\n",
    "    while True:\n",
    "        exp_task = client.monitor_instances.get_explanation_tasks(epx_task_id).result\n",
    "        state = exp_task.entity.status.state\n",
    "        if state == 'finished':\n",
    "            return exp_task\n",
    "        elif pcounter > poll_limit:\n",
    "            print(\"Explanation task has not completed.\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"Explanation task not ready...\")\n",
    "            pcounter += 1\n",
    "            time.sleep(5)\n",
    "            \n",
    "exp_resp = poll_explainability_task(wos_client, explanation_task_id)\n",
    "print(exp_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Check Monitors\n",
    "\n",
    "We can show which monitors are currently enabled, at this point, it would be the Quality, Fairness, Drift and Explainability Monitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show()\n",
    "wos_client.data_sets.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Custom monitors and metrics <a name=\"custom\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Register custom monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_definition(monitor_name):\n",
    "    monitor_definitions = wos_client.monitor_definitions.list().result.monitor_definitions\n",
    "    \n",
    "    for definition in monitor_definitions:\n",
    "        if monitor_name == definition.entity.name:\n",
    "            return definition\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_name = CUSTOM_NAME + '_WOSNotebook_CustomMonitor'\n",
    "metrics = [MonitorMetricRequest(name='sensitivity',\n",
    "                                thresholds=[MetricThreshold(type=MetricThresholdTypes.LOWER_LIMIT, default=0.8)]),\n",
    "          MonitorMetricRequest(name='specificity',\n",
    "                                thresholds=[MetricThreshold(type=MetricThresholdTypes.LOWER_LIMIT, default=0.75)])]\n",
    "tags = [MonitorTagRequest(name='region', description='customer geographical region')]\n",
    "\n",
    "existing_definition = get_definition(monitor_name)\n",
    "\n",
    "if existing_definition is None:\n",
    "    custom_monitor_details = wos_client.monitor_definitions.add(name=monitor_name, metrics=metrics, tags=tags, background_mode=False).result\n",
    "else:\n",
    "    custom_monitor_details = existing_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_definitions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_monitor_id = custom_monitor_details.metadata.id\n",
    "print(\"Custom Monitor ID: {}\".format(custom_monitor_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_monitor_details = wos_client.monitor_definitions.get(monitor_definition_id=custom_monitor_id).result\n",
    "print(\"Custom monitor definition details:\", custom_monitor_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Enable custom monitor for subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Target(\n",
    "        target_type=TargetTypes.SUBSCRIPTION,\n",
    "        target_id=subscription_id\n",
    "    )\n",
    "\n",
    "thresholds = [MetricThresholdOverride(metric_id='sensitivity', type = MetricThresholdTypes.LOWER_LIMIT, value=0.9)]\n",
    "\n",
    "custom_monitor_instance_details = wos_client.monitor_instances.create(\n",
    "            data_mart_id=data_mart_id,\n",
    "            background_mode=False,\n",
    "            monitor_definition_id=custom_monitor_id,\n",
    "            target=target\n",
    ").result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Get monitor configuration details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_monitor_instance_id = custom_monitor_instance_details.metadata.id\n",
    "custom_monitor_instance_details = wos_client.monitor_instances.get(custom_monitor_instance_id).result\n",
    "print(custom_monitor_instance_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Storing custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "from ibm_watson_openscale.base_classes.watson_open_scale_v2 import MonitorMeasurementRequest\n",
    "custom_monitoring_run_id = CUSTOM_NAME + '_WOSNotebook_CustomMonitorRun'#\"11122223333111abc\"\n",
    "measurement_request = [MonitorMeasurementRequest(timestamp=datetime.now(timezone.utc), \n",
    "                                                 metrics=[{\"specificity\": 0.78, \"sensitivity\": 0.67, \"region\": \"us-south\"}], run_id=custom_monitoring_run_id)]\n",
    "print(measurement_request[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_measurement_response = wos_client.monitor_instances.measurements.add(\n",
    "    monitor_instance_id=custom_monitor_instance_id,\n",
    "    monitor_measurement_request=measurement_request).result\n",
    "published_measurement_id = published_measurement_response[0][\"measurement_id\"]\n",
    "print(published_measurement_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Get custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "published_measurement = wos_client.monitor_instances.measurements.get(monitor_instance_id=custom_monitor_instance_id, measurement_id=published_measurement_id).result\n",
    "print(published_measurement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Historical data <a name=\"historical\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "historyDays = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Insert historical debias metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm history_debias_v2.json\n",
    "with io.capture_output() as captured:\n",
    "    !wget https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/data/openscale/history_debias_v2.json\n",
    "!ls -lh history_debias_v2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history_debias_v2.json', 'r') as history_file:\n",
    "    payloads = json.load(history_file)\n",
    "\n",
    "for day in range(historyDays):\n",
    "    print('Loading day', day + 1)\n",
    "    daily_measurement_requests = []\n",
    "    for hour in range(24):\n",
    "        score_time = datetime.now(timezone.utc) + timedelta(hours=(-(24*day + hour + 1)))\n",
    "        index = (day * 24 + hour) % len(payloads) # wrap around and reuse values if needed\n",
    "\n",
    "        measurement_request = MonitorMeasurementRequest(timestamp=score_time,metrics = [payloads[index][0], payloads[index][1]])\n",
    "        \n",
    "        daily_measurement_requests.append(measurement_request)\n",
    "        \n",
    "    response = wos_client.monitor_instances.measurements.add(\n",
    "                                            monitor_instance_id=fairness_monitor_instance_id,\n",
    "                                            monitor_measurement_request=daily_measurement_requests).result     \n",
    "\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Insert historical quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "measurements = [0.76, 0.78, 0.68, 0.72, 0.73, 0.77, 0.80]\n",
    "for day in range(historyDays):\n",
    "    quality_measurement_requests = []\n",
    "    print('Loading day', day + 1)\n",
    "    for hour in range(24):\n",
    "        score_time = datetime.utcnow() + timedelta(hours=(-(24*day + hour + 1)))\n",
    "        score_time = score_time.isoformat() + \"Z\"\n",
    "        \n",
    "        metric = {\"area_under_roc\": measurements[day]}\n",
    "                \n",
    "        measurement_request = MonitorMeasurementRequest(timestamp=score_time,metrics = [metric])\n",
    "        quality_measurement_requests.append(measurement_request)\n",
    "        \n",
    "        \n",
    "    response = wos_client.monitor_instances.measurements.add(\n",
    "                                            monitor_instance_id=quality_monitor_instance_id,\n",
    "                                            monitor_measurement_request=quality_measurement_requests).result    \n",
    "    \n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Insert historical confusion matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm history_quality_metrics.json\n",
    "with io.capture_output() as captured:\n",
    "    !wget https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/data/openscale/history_quality_metrics.json\n",
    "!ls -lh history_quality_metrics.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_openscale.base_classes.watson_open_scale_v2 import Source\n",
    "\n",
    "with open('history_quality_metrics.json') as json_file:\n",
    "    records = json.load(json_file)\n",
    "    \n",
    "for day in range(historyDays):\n",
    "    index = 0\n",
    "    cm_measurement_requests = []\n",
    "    print('Loading day', day + 1)\n",
    "    \n",
    "    for hour in range(24):\n",
    "        score_time = datetime.utcnow() + timedelta(hours=(-(24*day + hour + 1)))\n",
    "        score_time = score_time.isoformat() + \"Z\"\n",
    "\n",
    "        metric = records[index]['metrics']\n",
    "        source = records[index]['sources']\n",
    "\n",
    "        measurement_request = {\"timestamp\": score_time, \"metrics\": [metric], \"sources\": [source]}\n",
    "        cm_measurement_requests.append(measurement_request)\n",
    "\n",
    "        index+=1\n",
    "\n",
    "    response = wos_client.monitor_instances.measurements.add(monitor_instance_id=quality_monitor_instance_id, monitor_measurement_request=cm_measurement_requests).result    \n",
    "\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Insert historical performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = Target(\n",
    "        target_type=TargetTypes.INSTANCE,\n",
    "        target_id=payload_data_set_id\n",
    "    )\n",
    "\n",
    "performance_monitor_instance_details = wos_client.monitor_instances.create(\n",
    "            data_mart_id=data_mart_id,\n",
    "            background_mode=False,\n",
    "            monitor_definition_id=wos_client.monitor_definitions.MONITORS.PERFORMANCE.ID,\n",
    "            target=target\n",
    ").result\n",
    "performance_monitor_instance_id = performance_monitor_instance_details.metadata.id\n",
    "\n",
    "for day in range(historyDays):\n",
    "    performance_measurement_requests = []\n",
    "    print('Loading day', day + 1)\n",
    "    for hour in range(24):\n",
    "        score_time = datetime.utcnow() + timedelta(hours=(-(24*day + hour + 1)))\n",
    "        score_time = score_time.isoformat() + \"Z\"\n",
    "        score_count = random.randint(60, 600)\n",
    "        \n",
    "        metric = {\"record_count\": score_count, \"data_set_type\": \"scoring_payload\"}\n",
    "        \n",
    "        measurement_request = {\"timestamp\": score_time, \"metrics\": [metric]}\n",
    "        performance_measurement_requests.append(measurement_request)\n",
    "        \n",
    "    response = wos_client.monitor_instances.measurements.add(\n",
    "                                            monitor_instance_id=performance_monitor_instance_id,\n",
    "                                            monitor_measurement_request=performance_measurement_requests).result    \n",
    "\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Insert historical drift measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm history_drift_measurement_*.json\n",
    "\n",
    "with io.capture_output() as captured_0:\n",
    "    !wget https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/data/openscale/history_drift_measurement_0.json\n",
    "    !wget https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/data/openscale/history_drift_measurement_1.json\n",
    "    !wget https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/data/openscale/history_drift_measurement_2.json\n",
    "    !wget https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/data/openscale/history_drift_measurement_3.json\n",
    "    !wget https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/data/openscale/history_drift_measurement_4.json\n",
    "    !wget https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/data/openscale/history_drift_measurement_5.json\n",
    "    !wget https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/data/openscale/history_drift_measurement_6.json\n",
    "\n",
    "!ls -lh history_drift_measurement_*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in range(historyDays):\n",
    "    drift_measurements = []\n",
    "\n",
    "    with open(\"history_drift_measurement_{}.json\".format(day), 'r') as history_file:\n",
    "        drift_daily_measurements = json.load(history_file)\n",
    "    print('Loading day', day + 1)\n",
    "\n",
    "    #Historical data contains 8 records per day - each represents 3 hour drift window.\n",
    "    \n",
    "    for nb_window, records in enumerate(drift_daily_measurements):\n",
    "        for record in records:\n",
    "            window_start =  datetime.utcnow() + timedelta(hours=(-(24 * day + (nb_window+1)*3 + 1))) # first_payload_record_timestamp_in_window (oldest)\n",
    "            window_end = datetime.utcnow() + timedelta(hours=(-(24 * day + nb_window*3 + 1)))# last_payload_record_timestamp_in_window (most recent)\n",
    "            #modify start and end time for each record\n",
    "            record['sources'][0]['data']['start'] = window_start.isoformat() + \"Z\"\n",
    "            record['sources'][0]['data']['end'] = window_end.isoformat() + \"Z\"\n",
    "            \n",
    "            metric = record['metrics'][0]\n",
    "            source = record['sources'][0]\n",
    "\n",
    "            measurement_request = {\"timestamp\": window_start.isoformat() + \"Z\", \"metrics\": [metric], \"sources\": [source]}\n",
    "            \n",
    "            drift_measurements.append(measurement_request)\n",
    "        \n",
    "    response = wos_client.monitor_instances.measurements.add(\n",
    "                                            monitor_instance_id=drift_monitor_instance_id,\n",
    "                                            monitor_measurement_request=drift_measurements).result    \n",
    "    \n",
    "    print(\"Daily loading finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 Additional data to help debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Datamart id: {}\".format(data_mart_id))\n",
    "print(\"Model id: {}\".format(model_uid))\n",
    "print(\"Deployment id: {}\".format(deployment_uid))\n",
    "print(\"Scoring URL:{}\".format(scoring_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.0 Identify transactions for Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transaction IDs identified by the cells below can be copied and pasted into the Explainability tab of the OpenScale dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wos_client.data_sets.show_records(payload_data_set_id, limit=5)\n",
    "pl_pd = wos_client.data_sets.get_list_of_records(data_set_id=payload_data_set_id, limit=5, output_type=ResponseTypes.PANDAS).result\n",
    "df = pl_pd[['scoring_id','predictedLabel','probability']]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You have finished the hands-on lab for IBM Watson OpenScale. You can now view the OpenScale dashboard. Choose the `OpenScale` service instance and launch the application UI. Click on the tile for the model you've created to see fairness, accuracy, and performance monitors. Click on the timeseries graph to get detailed information on transactions during a specific time window.\n",
    "\n",
    "OpenScale shows model performance over time. You have two options to keep data flowing to your OpenScale graphs:\n",
    "  * Download, configure and schedule the [model feed notebook](https://raw.githubusercontent.com/emartensibm/german-credit/master/german_credit_scoring_feed.ipynb). This notebook can be set up with your WML credentials, and scheduled to provide a consistent flow of scoring requests to your model, which will appear in your OpenScale monitors.\n",
    "  * Re-run this notebook. Running this notebook from the beginning will delete and re-create the model and deployment, and re-create the historical data. Please note that the payload and measurement logs for the previous deployment will continue to be stored in your datamart, and can be deleted if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
