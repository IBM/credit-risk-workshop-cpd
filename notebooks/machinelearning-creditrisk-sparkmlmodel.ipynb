{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "142ae155-3f4b-4ab0-a927-a91b58f87128"
   },
   "source": [
    "# Predicting Loan Risk using SparkML on IBM Cloud Pak for Data (ICP4D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5756a6f1-6a52-4450-a19b-8b29dda37c0d"
   },
   "source": [
    "We'll use this notebook to create a machine learning model to predict customer churn. In this notebook we will build the prediction model using the SparkML library.\n",
    "\n",
    "This notebook walks you through these steps:\n",
    "\n",
    "- Load and Visualize data set.\n",
    "- Build a predictive model with SparkML API\n",
    "- Save the model in the ML repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0409d64e-5cc9-4ae2-a118-67694a77cf9c"
   },
   "source": [
    "## 1.0 Install required packages\n",
    "\n",
    "There are a couple of Python packages we will use in this notebook. We will install/upgrade the Watson Machine Learning (WML) client library to interact with the Watson Machine Learning service from this notebook. We will also install the pyspark package since we are building a Spark ML machine learning model. These package may already be installed by default on CP4D.\n",
    "\n",
    "- WML Client: http://ibm-wml-api-pyclient.mybluemix.net/\n",
    "- PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "b2b7b6a3-a6e7-4e3d-b01b-7aef6cef0a9e"
   },
   "source": [
    "### 1.1 Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2f147332-9399-4c61-bc11-be2bb76829e8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5de8ac1d-29b6-4605-b550-71c4d8541ac1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install --user ibm-watson-machine-learning==1.0.53 --upgrade | tail -n 1\n",
    "!pip install --user pyspark==2.4 --upgrade | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36acf833-0860-4091-93f0-9b546f777393"
   },
   "source": [
    "### 1.2 Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03838646-dc94-4a59-8830-6b492563b5dd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Import the Project Library to read/write project assets\n",
    "from project_lib import Project\n",
    "project = Project.access()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88625621-6861-4904-a513-688217a0f595"
   },
   "source": [
    "## 2.0 Load and Clean data\n",
    "\n",
    "We'll load our data as a pandas data frame.\n",
    "\n",
    "**<font color='red'><< FOLLOW THE INSTRUCTIONS BELOW TO LOAD THE DATASET >></font>**\n",
    "\n",
    "* Highlight the cell below by clicking it.\n",
    "* Click the `10/01` \"Find data\" icon in the upper right of the notebook.\n",
    "* If you are using Virtualized data, begin by choosing the `Files` tab. Then choose your virtualized data (i.e. MYSCHEMA.APPLICANTFINANCIALPERSONALLOANDATA), click `Insert to code` and choose `Insert Pandas DataFrame`.\n",
    "* If you are using this notebook without virtualized data, add the locally uploaded file `german_credit_data.csv` by choosing the `Files` tab. Then choose the `german_credit_data.csv`. Click `Insert to code` and choose `Insert Pandas DataFrame`.\n",
    "* The code to bring the data into the notebook environment and create a Pandas DataFrame will be added to the cell below.\n",
    "* Run the cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33e8dfca-b3c3-47b7-ac35-6872307d5255"
   },
   "outputs": [],
   "source": [
    "# Place cursor below and insert the Pandas DataFrame for the Credit Risk data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb42a8c2-75e6-450c-a84a-82b78c37dada"
   },
   "source": [
    "We'll use the Pandas naming convention df for our DataFrame. Make sure that the cell below uses the name for the dataframe that was generated above. For the locally uploaded file it should look like df_data_1 or df_data_2 or df_data_x. For the virtualized data case it should look like data_df_1 or data_df_2 or data_df_x. The exact number you see in the variable name might be different.\n",
    "\n",
    "**<font color='red'><< UPDATE THE VARIABLE ASSIGNMENT TO THE VARIABLE GENERATED ABOVE. >></font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ef72340a-10d6-4cac-98cb-afbe82d43714"
   },
   "outputs": [],
   "source": [
    "# Replace data_df_1 with the variable name generated above.\n",
    "df = df_data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e4cf8fd-a443-4084-bdba-b14cdd4a98a3"
   },
   "source": [
    "### 2.1 Drop Some Features\n",
    "Some columns are data attributes that we will not want to use in the machine learning model. We can drop those columns / features:\n",
    "\n",
    "- CustomerID feature (column)\n",
    "- Personal Attributes: first_name,last_name,email,street_address,city,state,postal_code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd46e06a-bd3e-4234-a629-16d749ffdacf"
   },
   "outputs": [],
   "source": [
    "#Drop some columns, ignoring errors for missing keys in case we use different data sets.\n",
    "df = df.drop(columns=['CustomerID', 'FirstName', 'LastName', 'Email', 'StreetAddress', 'City', 'State', 'PostalCode', '_ID'], axis=1, errors='ignore')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "904ef4ab-83c8-4457-a45f-6133ec26163d"
   },
   "source": [
    "### 2.2 Examine the data types of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bea9163-edaa-4899-ae84-8df2e1c6606c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8c4e374-22ea-4671-80af-2833d7763e3b"
   },
   "outputs": [],
   "source": [
    "# Statistics for the columns (features). Set it to all, since default is to describe just the numeric features.\n",
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50991391-a319-4cca-bf74-5fa255d0df6f"
   },
   "source": [
    "We see that the loan amounts range from 250 to ~11,600. That the age range for applicants is between 19 and 74. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adc507d0-f598-40ff-bb0d-fffd27ddd5cf"
   },
   "source": [
    "### 2.3 Check for missing data\n",
    "\n",
    "We should check if there are missing values in our dataset. There are various ways we can address this issue:\n",
    "\n",
    "- Drop records with missing values \n",
    "- Fill in the missing value with one of the following strategies: Zero, Mean of the values for the column, Random value, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78b239ba-b501-4e28-92b2-bfb572ee10e4"
   },
   "outputs": [],
   "source": [
    "# Check if we have any NaN values and see which features have missing values that should be addressed\n",
    "print(df.isnull().values.any())\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fb91dcc-2e3b-4ffc-bb76-375d47995f45"
   },
   "source": [
    "Depending on which version of the dataset that you used, there may be no missing values. If there are any missing values from the output above, the sample below would be one approach to handle this issue by imputing the values for the column that reported missing data (i.e the `CurrentResidenceDuration` column in the code as an example):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbd9e275-ca7a-4637-b3ff-4b54c73391c8"
   },
   "outputs": [],
   "source": [
    "## Handle missing values for nan_column (CurrentResidenceDuration)\n",
    "#from sklearn.impute import SimpleImputer\n",
    "\n",
    "## Find the column number for TotalCharges (starting at 0).\n",
    "#target_idx = df.columns.get_loc(\"CurrentResidenceDuration\")\n",
    "#imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "#df.iloc[:, target_idx] = imputer.fit_transform(df.iloc[:, target_idx].values.reshape(-1, 1))\n",
    "#df.iloc[:, target_idx] = pd.Series(df.iloc[:, target_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e425f46e-1540-4004-b0a3-f71f91d0f1f7"
   },
   "source": [
    "### 2.4 Categorize Features\n",
    "\n",
    "We will categorize some of the columns / features based on wether they are categorical values or continuous (i.e numerical) values. We will use this in later sections to build visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "410db5a6-5e8a-48dd-b9f2-3ba66ae950a9"
   },
   "outputs": [],
   "source": [
    "TARGET_LABEL_COLUMN_NAME = 'Risk'\n",
    "columns_idx = np.s_[0:] # Slice of first row(header) with all columns.\n",
    "first_record_idx = np.s_[0] # Index of first record\n",
    "\n",
    "string_fields = [type(fld) is str for fld in df.iloc[first_record_idx, columns_idx]] # All string fields\n",
    "all_features = [x for x in df.columns if x != TARGET_LABEL_COLUMN_NAME]\n",
    "categorical_columns = list(np.array(df.columns)[columns_idx][string_fields])\n",
    "categorical_features = [x for x in categorical_columns if x != TARGET_LABEL_COLUMN_NAME]\n",
    "continuous_features = [x for x in all_features if x not in categorical_features]\n",
    "\n",
    "print('All Features: ', all_features)\n",
    "print('\\nCategorical Features: ', categorical_features)\n",
    "print('\\nContinuous Features: ', continuous_features)\n",
    "print('\\nAll Categorical Columns: ', categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf3578f3-7030-42c1-b62d-a40b750f43a7"
   },
   "source": [
    "### 2.5 Visualize data\n",
    "\n",
    "Data visualization can be used to find patterns, detect outliers, understand distribution and more. We can use graphs such as:\n",
    "\n",
    "- Histograms, boxplots, etc: To find distribution / spread of our continuous variables.\n",
    "- Bar charts: To show frequency in categorical values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf9c9372-0a16-487d-aa9a-1035236da188"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set_palette(\"hls\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0862d341-e815-4d8b-bd06-3b839f6bff4f"
   },
   "source": [
    "First, we get a high level view of the distribution of Risk. What percentage of applicants in our dataset represent Risk vs No Risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "139e81cf-5f80-4975-b15b-2e91f184bb9e"
   },
   "outputs": [],
   "source": [
    "print(df.groupby([TARGET_LABEL_COLUMN_NAME]).size())\n",
    "risk_plot = sns.countplot(data=df, x=TARGET_LABEL_COLUMN_NAME, order=df[TARGET_LABEL_COLUMN_NAME].value_counts().index)\n",
    "plt.ylabel('Count')\n",
    "for p in risk_plot.patches:\n",
    "    height = p.get_height()\n",
    "    risk_plot.text(p.get_x()+p.get_width()/2., height + 1,'{0:.0%}'.format(height/float(len(df))),ha=\"center\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a649fd6-0ad0-46dd-81b3-7e350c275f4e"
   },
   "source": [
    "We can get use frequency counts charts to get an understanding of the categorical features relative to Risk\n",
    "\n",
    "- We can see in the `CheckingStatus` visualization, loan applications with 'no_checking' have a higher occurence of Risk versus loans with other checking status values.\n",
    "- We can see in the `CreditHistory` visualization, the loans that have no credits (i.e. all credit has been paid back) have no occurences of Risk (at least in this dataset). There is a small count of Risk for those applicants that have paid back all credit to date. And there is a higher frequency or ratio of Risk for applicants that have existing credit (i.e outstanding credit).\n",
    "\n",
    "### NOTE: The creation of these plots can take several minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "220ac775-f732-4f08-a7f7-e5ffc91ba7d3"
   },
   "outputs": [],
   "source": [
    "# Categorical feature count plots\n",
    "f, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10), (ax11, ax12), (ax13, ax14)) = plt.subplots(7, 2, figsize=(25, 25))\n",
    "ax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12, ax13, ax14 ]\n",
    "\n",
    "for i in range(len(categorical_features)):\n",
    "    sns.countplot(x = categorical_features[i], hue=TARGET_LABEL_COLUMN_NAME, data=df, ax=ax[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4c81dae-d6d5-4f85-8b74-2462bd9ac0c2"
   },
   "source": [
    "We can get use histrogram and boxplots to get an understanding of the distribution of our continuous / numerical features relative to Risk.\n",
    "\n",
    "- We can see that for loans that have Risk, the `InstallmentPercent` tends to be higher (i.e. the loans with Risk tend to have loan amounts with higher percentage of the loan applicants disposable income).\n",
    "- We can see that those with 'No Risk' seem to be those with fewer existing credit loans at the bank (`ExistingCreditCount`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c60c229-2f25-43c3-bb1c-abb3a103f225"
   },
   "outputs": [],
   "source": [
    "# Continuous feature histograms.\n",
    "f, ((ax1, ax2),(ax3, ax4), (ax5, ax6), (ax7, ax8)) = plt.subplots(4, 2, figsize=(25, 25))\n",
    "ax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8]\n",
    "for i in range(len(continuous_features)):\n",
    "    #sns.distplot(df[continuous_features[i]], bins=20, color=\"blue\", hist=True, ax=ax[i])\n",
    "    sns.distplot(df[df.Risk == 'Risk'][continuous_features[i]], bins=20, color=\"Red\", hist=True, ax=ax[i])\n",
    "    sns.distplot(df[df.Risk == 'No Risk'][continuous_features[i]], bins=20, color=\"blue\", hist=True, ax=ax[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e70a9900-8e64-434a-bd6d-a24e5a73576a"
   },
   "outputs": [],
   "source": [
    "# Plot boxplots of numerical columns. More variation in the boxplot implies higher significance. \n",
    "f, ((ax1, ax2),(ax3, ax4), (ax5, ax6), (ax7, ax8)) = plt.subplots(4, 2, figsize=(25, 25))\n",
    "ax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8]\n",
    "for i in range(len(continuous_features)):\n",
    "    sns.boxplot(x = TARGET_LABEL_COLUMN_NAME, y = continuous_features[i], data=df, ax=ax[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c5d3abf-783b-43b8-8987-e100fe9af2f3"
   },
   "source": [
    "## 3.0 Create a model\n",
    "\n",
    "Now we can create our machine learning model. You could use the insights / intuition gained from the data visualization steps above to what kind of model to create or which features to use. We will create a simple classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cec99c9-7b7c-4381-b2a1-00a285da87c8"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_data = spark.createDataFrame(df)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbdbdd8d-0b23-40a7-898e-ec090d457874"
   },
   "source": [
    "### 3.1 Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fe084535-ab7c-456a-ac07-4e2c80c589a3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark_df = df_data\n",
    "(train_data, test_data) = spark_df.randomSplit([0.8, 0.2], 24)\n",
    "\n",
    "print(\"Number of records for training: \" + str(train_data.count()))\n",
    "print(\"Number of records for evaluation: \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b3ca50d-8045-4b7d-8e1b-23fb73361543"
   },
   "source": [
    "### 3.2 Examine the Spark DataFrame Schema\n",
    "Look at the data types to determine requirements for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5a0812d-d385-4cf0-96d6-9eb19648c01e"
   },
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9940ffe4-7831-474e-b850-f51cdfb54c0b"
   },
   "source": [
    "### 3.3 Use StringIndexer to encode a string column of labels to a column of label indices\n",
    "\n",
    "We are using the Pipeline package to build the development steps as pipeline. \n",
    "We are using StringIndexer to handle categorical / string features from the dataset. StringIndexer encodes a string column of labels to a column of label indices\n",
    "\n",
    "We then use VectorAssembler to asemble these features into a vector. Pipelines API requires that input variables are passed in  a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56c5111c-709f-4d50-b22d-1fab07c53c09"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler, SQLTransformer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model\n",
    "\n",
    "#Create StringIndexer columns whose names are same as the categorical column with an appended _IX.\n",
    "categorical_num_features = [x + '_IX' for x in categorical_features]\n",
    "si_list = [StringIndexer(inputCol=nm_in, outputCol=nm_out) for nm_in, nm_out in zip(categorical_features, categorical_num_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1c724e7-2806-42df-8c03-86e94946a278"
   },
   "outputs": [],
   "source": [
    "# Encode our target label column (i.e Risk or No Risk). \n",
    "# Also, creates an label convert which performs an inverse map to get back a 'Risk' or 'No Risk' label from the encoded prediction.\n",
    "si_label = StringIndexer(inputCol=TARGET_LABEL_COLUMN_NAME, outputCol=\"label\").fit(spark_df)\n",
    "label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_label.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ea782a63-24fd-4f8e-83ab-de771fd61749"
   },
   "outputs": [],
   "source": [
    "# Construct all encoded categorical features plus continuous features into a vector\n",
    "va_features = VectorAssembler(inputCols=categorical_num_features + continuous_features, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8c560a7-c0e7-4a1c-9202-8ac9c3300891"
   },
   "source": [
    "### 3.4 Create a pipeline, and fit a model using RandomForestClassifier \n",
    "Assemble all the stages into a pipeline. We don't expect a clean linear regression, so we'll use RandomForestClassifier to find the best decision tree for the data.\n",
    "\n",
    "The pipeline will consist of: the feature string indexing step, the label string indexing Step, vector sssembly of all features step, random forest classifier, label converter step, and ending with a feature filter step.\n",
    "\n",
    "**Note: If you want filter features from model output, you could use the feature filter by replacing `*` with feature names to be retained in SQLTransformer statement.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30ad2a5a-0654-4f1a-9e47-5c888421aaf7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(featuresCol=\"features\")\n",
    "feature_filter = SQLTransformer(statement=\"SELECT * FROM __THIS__\")\n",
    "pipeline = Pipeline(stages= si_list + [si_label, va_features, classifier, label_converter, feature_filter])\n",
    "\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3db59535-596c-4076-b9e6-dd4313d8589e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)\n",
    "evaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName='areaUnderROC')\n",
    "area_under_curve = evaluatorDT.evaluate(predictions)\n",
    "\n",
    "evaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName='areaUnderPR')\n",
    "area_under_PR = evaluatorDT.evaluate(predictions)\n",
    "#default evaluation is areaUnderROC\n",
    "print(\"areaUnderROC = %g\" % area_under_curve, \"areaUnderPR = %g\" % area_under_PR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55c92c05-71e6-4b29-aa77-dd1879ed9dba"
   },
   "source": [
    "### 3.5 evaluate more metrics by exporting them into pandas and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86ab7ac6-a6ba-4a83-924f-cd29bfe9152b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = predictions.toPandas()['prediction']\n",
    "y_pred = ['Risk' if pred == 1.0 else 'No Risk' for pred in y_pred]\n",
    "y_test = test_data.toPandas()[TARGET_LABEL_COLUMN_NAME]\n",
    "print(classification_report(y_test, y_pred, target_names=['Risk', 'No Risk']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9010fcd-2b5d-4ff7-82b4-25ade008f0f5"
   },
   "source": [
    "## 4.0 Save the model and test data\n",
    "\n",
    "Now the model can be saved for future deployment. The model will be saved using the Watson Machine Learning client, to a deployment space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25c4c839-ecd1-4386-8fb7-044fbc4c5797"
   },
   "source": [
    "### 4.1 Save the model to ICP4D local Watson Machine Learning\n",
    "\n",
    "<font color='red'>Replace the `username` and `password` values of `************` with your Cloud Pak for Data `username` and `password`. The value for `url` should match the `url` for your Cloud Pak for Data cluster, which you can get from the browser address bar (be sure to include the 'https://'.</font> The credentials should look something like this (these are example values, not the ones you will use):\n",
    "\n",
    "`\n",
    "wml_credentials = {\n",
    "                   \"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\",\n",
    "                   \"username\": \"cp4duser\",\n",
    "                   \"password\" : \"cp4dpass\",\n",
    "                   \"instance_id\": \"openshift\",\n",
    "                   \"version\" : \"3.5\"\n",
    "                  }\n",
    "`\n",
    "#### NOTE: Make sure that there is no trailing forward slash `/` in the `url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "867123dc-132b-4bc4-a6cf-30ec653b8da5"
   },
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient\n",
    "\n",
    "wml_credentials = {\n",
    "                   \"url\": \"************\",\n",
    "                   \"username\": \"************\",\n",
    "                   \"password\" : \"************\",\n",
    "                   \"instance_id\": \"openshift\",\n",
    "                   \"version\" : \"3.5\"\n",
    "                  }\n",
    "\n",
    "wml_client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebdc3dab-7433-4bba-a9d5-186fd39c5097"
   },
   "outputs": [],
   "source": [
    "wml_client.spaces.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bac1c3d2-7c4c-453d-a2c8-21e10c8a8ff5"
   },
   "source": [
    "### Use the desired space as the `default_space`\n",
    "\n",
    "The deployment space ID will be looked up based on the name specified above. If you do not receive a space GUID as an output to the next cell, do not proceed until you have created a deployment space.\n",
    "\n",
    "**<font color='red'><< UPDATE THE VARIABLE 'MODEL_NAME' TO A UNIQUE NAME>></font>**\n",
    "\n",
    "**<font color='red'><< UPDATE THE VARIABLE 'DEPLOYMENT_SPACE_NAME' TO THE NAME OF THE DEPLOYMENT SPACE CREATED PREVIOUSLY>></font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c705cc8d-9d4d-4f3c-b972-8a88f993606f"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"************\"\n",
    "DEPLOYMENT_SPACE_NAME = \"************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e03d0b48-6e29-4278-bd37-9e99f2b2eeed"
   },
   "outputs": [],
   "source": [
    "wml_client.spaces.list()\n",
    "all_spaces = wml_client.spaces.get_details()['resources']\n",
    "space_id = None\n",
    "for space in all_spaces:\n",
    "    if space['entity']['name'] == DEPLOYMENT_SPACE_NAME:\n",
    "        space_id = space[\"metadata\"][\"id\"]\n",
    "        print(\"\\nDeployment Space ID: \", space_id)\n",
    "\n",
    "if space_id is None:\n",
    "    print(\"WARNING: Your space does not exist. Create a deployment space before proceeding to the next cell.\")\n",
    "    #space_id = client.spaces.store(meta_props={client.spaces.ConfigurationMetaNames.NAME: space_name})[\"metadata\"][\"guid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e297d983-c5e7-4acf-9656-2d823888f104"
   },
   "outputs": [],
   "source": [
    "# Now set the default space to the ID for your deployment space. If this is successful, you will see a 'SUCCESS' message.\n",
    "wml_client.set.default_space(space_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06145288-350f-4618-8f83-5a0161299805"
   },
   "source": [
    "#### (Optional) Remove Existing Model and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0896a5aa-2a39-4dad-a8cf-b14b2ab97e75",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wml_models = wml_client.repository.get_model_details()\n",
    "#model_uid = None\n",
    "#for model_in in wml_models['resources']:\n",
    "#    if MODEL_NAME == model_in['entity']['name']:\n",
    "#        model_uid = model_in['metadata']['guid']\n",
    "#        print('Deleting model id', model_uid)\n",
    "#        wml_client.repository.delete(model_uid)\n",
    "#        break\n",
    "#\n",
    "# wml_client.repository.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccc45220-b392-4044-bf35-7235a07b9046"
   },
   "source": [
    "#### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7184603-d102-4575-a6ec-7ac93a354b01",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    wml_client.repository.ModelMetaNames.NAME: MODEL_NAME,\n",
    "    wml_client.repository.ModelMetaNames.TYPE: 'mllib_2.4',\n",
    "    wml_client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: wml_client.software_specifications.get_id_by_name('spark-mllib_2.4'),\n",
    "}\n",
    "\n",
    "\n",
    "published_model_details = wml_client.repository.store_model(model, metadata, training_data=df_data,  pipeline=pipeline)\n",
    "model_uid = wml_client.repository.get_model_uid(published_model_details)\n",
    "\n",
    "print(json.dumps(published_model_details, indent=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23899ae3-0c9a-4e1c-a5a7-887fc94af55c"
   },
   "outputs": [],
   "source": [
    "wml_client.repository.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1048845f-10ac-45ea-b3c9-8369f2100c32"
   },
   "source": [
    "## 5.0 Save Test Data\n",
    "\n",
    "We will save the test data we used to evaluate the model to our project. Although not required, this will make it easier to run batch tests later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7848aadc-22dd-48dd-a3e6-549ffe9ae57e"
   },
   "outputs": [],
   "source": [
    "write_score_CSV=test_data.toPandas().drop([TARGET_LABEL_COLUMN_NAME], axis=1)\n",
    "write_score_CSV.to_csv('/project_data/data_asset/GermanCreditRiskSparkMLBatchScore.csv', sep=',', index=False)\n",
    "#project.save_data('GermanCreditRiskSparkMLBatchScore.csv', write_score_CSV.to_csv())\n",
    "\n",
    "write_eval_CSV=test_data.toPandas()\n",
    "write_eval_CSV.to_csv('/project_data/data_asset/GermanCreditRiskSparkMLEval.csv', sep=',', index=False)\n",
    "#project.save_data('GermanCreditRiskSparkMLEval.csv', write_eval_CSV.to_csv())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
