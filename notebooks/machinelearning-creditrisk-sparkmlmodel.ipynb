{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Predicting Loan Risk using SparkML on IBM Cloud Pak for Data (ICP4D)"}, {"metadata": {}, "cell_type": "markdown", "source": "We'll use this notebook to create a machine learning model to predict customer churn. In this notebook we will build the prediction model using the SparkML library.\n\nThis notebook walks you through these steps:\n\n- Load and Visualize data set.\n- Build a predictive model with SparkML API\n- Save the model in the ML repository"}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.0 Install required packages\n\nThere are a couple of Python packages we will use in this notebook. First we make sure the Watson Machine Learning client v3 is removed (its not installed by default) and then install/upgrade the v4 version of the client (this package is installed by default on CP4D).\n\nWML Client: https://wml-api-pyclient-dev-v4.mybluemix.net/#repository"}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "### 1.1 Package Installation"}, {"metadata": {}, "cell_type": "code", "source": "import warnings\nwarnings.filterwarnings('ignore')", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "!pip uninstall watson-machine-learning-client -y | tail -n 1\n!pip install --user watson-machine-learning-client-v4==1.0.95 --upgrade | tail -n 1\n!pip install --user pyspark==2.3.3 --upgrade | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.2 Package Imports"}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nimport numpy as np\nimport json\nimport os\n\n# Import the Project Library to read/write project assets\nfrom project_lib import Project\nproject = Project.access()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.0 Load and Clean data\n\nWe'll load our data as a pandas data frame.\n\n**<font color='red'><< FOLLOW THE INSTRUCTIONS BELOW TO LOAD THE DATASET >></font>**\n\n* Highlight the cell below by clicking it.\n* Click the `10/01` \"Find data\" icon in the upper right of the notebook.\n* If you are using Virtualized data, begin by choosing the `Files` tab. Then choose your virtualized data (i.e. MYSCHEMA.APPLICANTFINANCIALPERSONALLOANDATA), click `Insert to code` and choose `Insert Pandas DataFrame`.\n* If you are using this notebook without virtualized data, add the locally uploaded file `german_credit_data.csv` by choosing the `Files` tab. Then choose the `german_credit_data.csv`. Click `Insert to code` and choose `Insert Pandas DataFrame`.\n* The code to bring the data into the notebook environment and create a Pandas DataFrame will be added to the cell below.\n* Run the cell\n"}, {"metadata": {}, "cell_type": "code", "source": "# Place cursor below and insert the Pandas DataFrame for the Credit Risk data\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We'll use the Pandas naming convention df for our DataFrame. Make sure that the cell below uses the name for the dataframe used above. For the locally uploaded file it should look like df_data_1 or df_data_2 or df_data_x. For the virtualized data case it should look like data_df_1 or data_df_2 or data_df_x.\n\n**<font color='red'><< UPDATE THE VARIABLE ASSIGNMENT TO THE VARIABLE GENERATED ABOVE. >></font>**"}, {"metadata": {}, "cell_type": "code", "source": "# Replace data_df_3 with the variable name generated above.\ndf = data_df_1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1 Drop Some Features\nSome columns are data attributes that we will not want to use in the machine learning model. We can drop those columns / features:\n\n- CustomerID feature (column)\n- Personal Attributes: first_name,last_name,email,street_address,city,state,postal_code "}, {"metadata": {}, "cell_type": "code", "source": "#Drop some columns, ignoring errors for missing keys in case we use different data sets.\ndf = df.drop(columns=['CustomerID', 'FirstName', 'LastName', 'Email', 'StreetAddress', 'City', 'State', 'PostalCode', '_ID'], axis=1, errors='ignore')\ndf.head(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2 Examine the data types of the features"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "df.info()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Statistics for the columns (features). Set it to all, since default is to describe just the numeric features.\ndf.describe(include = 'all')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We see that the loan amounts range from 250 to ~11,600. That the age range for applicants is between 19 and 74. etc."}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.3 Check for missing data\n\nWe should check if there are missing values in our dataset. There are various ways we can address this issue:\n\n- Drop records with missing values \n- Fill in the missing value with one of the following strategies: Zero, Mean of the values for the column, Random value, etc)."}, {"metadata": {}, "cell_type": "code", "source": "# Check if we have any NaN values and see which features have missing values that should be addressed\nprint(df.isnull().values.any())\ndf.isnull().sum()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Depending on which version of the dataset that you used, there may be no missing values. If there are any missing values from the output above, the sample below would be one approach to handle this issue by imputing the values for the column that reported missing data (i.e the `CurrentResidenceDuration` column in the code as an example):\n"}, {"metadata": {}, "cell_type": "code", "source": "## Handle missing values for nan_column (CurrentResidenceDuration)\n#from sklearn.impute import SimpleImputer\n\n## Find the column number for TotalCharges (starting at 0).\n#target_idx = df.columns.get_loc(\"CurrentResidenceDuration\")\n#imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n#df.iloc[:, target_idx] = imputer.fit_transform(df.iloc[:, target_idx].values.reshape(-1, 1))\n#df.iloc[:, target_idx] = pd.Series(df.iloc[:, target_idx])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.4 Categorize Features\n\nWe will categorize some of the columns / features based on wether they are categorical values or continuous (i.e numerical) values. We will use this in later sections to build visualizations."}, {"metadata": {}, "cell_type": "code", "source": "TARGET_LABEL_COLUMN_NAME = 'Risk'\ncolumns_idx = np.s_[0:] # Slice of first row(header) with all columns.\nfirst_record_idx = np.s_[0] # Index of first record\n\nstring_fields = [type(fld) is str for fld in df.iloc[first_record_idx, columns_idx]] # All string fields\nall_features = [x for x in df.columns if x != TARGET_LABEL_COLUMN_NAME]\ncategorical_columns = list(np.array(df.columns)[columns_idx][string_fields])\ncategorical_features = [x for x in categorical_columns if x != TARGET_LABEL_COLUMN_NAME]\ncontinuous_features = [x for x in all_features if x not in categorical_features]\n\nprint('All Features: ', all_features)\nprint('\\nCategorical Features: ', categorical_features)\nprint('\\nContinuous Features: ', continuous_features)\nprint('\\nAll Categorical Columns: ', categorical_columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.5 Visualize data\n\nData visualization can be used to find patterns, detect outliers, understand distribution and more. We can use graphs such as:\n\n- Histograms, boxplots, etc: To find distribution / spread of our continuous variables.\n- Bar charts: To show frequency in categorical values.\n"}, {"metadata": {}, "cell_type": "code", "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\n\n%matplotlib inline\nsns.set(style=\"darkgrid\")\nsns.set_palette(\"hls\", 3)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "First, we get a high level view of the distribution of Risk. What percentage of applicants in our dataset represent Risk vs No Risk."}, {"metadata": {}, "cell_type": "code", "source": "print(df.groupby([TARGET_LABEL_COLUMN_NAME]).size())\nrisk_plot = sns.countplot(data=df, x=TARGET_LABEL_COLUMN_NAME, order=df[TARGET_LABEL_COLUMN_NAME].value_counts().index)\nplt.ylabel('Count')\nfor p in risk_plot.patches:\n    height = p.get_height()\n    risk_plot.text(p.get_x()+p.get_width()/2., height + 1,'{0:.0%}'.format(height/float(len(df))),ha=\"center\") \nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We can get use frequency counts charts to get an understanding of the categorical features relative to Risk\n\n- We can see in the `CheckingStatus` visualization, loan applications with 'no_checking' have a higher occurence of Risk versus loans with other checking status values.\n- We can see in the `CreditHistory` visualization, the loans that have no credits (i.e. all credit has been paid back) have no occurences of Risk (at least in this dataset). There is a small count of Risk for those applicants that have paid back all credit to date. And there is a higher frequency or ratio of Risk for applicants that have existing credit (i.e outstanding credit).\n\n### NOTE: The creation of these plots can take several minutes"}, {"metadata": {}, "cell_type": "code", "source": "# Categorical feature count plots\nf, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10), (ax11, ax12), (ax13, ax14)) = plt.subplots(7, 2, figsize=(25, 25))\nax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12, ax13, ax14 ]\n\nfor i in range(len(categorical_features)):\n    sns.countplot(x = categorical_features[i], hue=TARGET_LABEL_COLUMN_NAME, data=df, ax=ax[i])\n    ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We can get use histrogram and boxplots to get an understanding of the distribution of our continuous / numerical features relative to Risk.\n\n- We can see that for loans that have Risk, the `InstallmentPercent` tends to be higher (i.e. the loans with Risk tend to have loan amounts with higher percentage of the loan applicants disposable income).\n- We can see that those with 'No Risk' seem to be those with fewer existing credit loans at the bank (`ExistingCreditCount`)\n"}, {"metadata": {}, "cell_type": "code", "source": "# Continuous feature histograms.\nf, ((ax1, ax2),(ax3, ax4), (ax5, ax6), (ax7, ax8)) = plt.subplots(4, 2, figsize=(25, 25))\nax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8]\nfor i in range(len(continuous_features)):\n    #sns.distplot(df[continuous_features[i]], bins=20, color=\"blue\", hist=True, ax=ax[i])\n    sns.distplot(df[df.Risk == 'Risk'][continuous_features[i]], bins=20, color=\"Red\", hist=True, ax=ax[i])\n    sns.distplot(df[df.Risk == 'No Risk'][continuous_features[i]], bins=20, color=\"blue\", hist=True, ax=ax[i])\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Plot boxplots of numerical columns. More variation in the boxplot implies higher significance. \nf, ((ax1, ax2),(ax3, ax4), (ax5, ax6), (ax7, ax8)) = plt.subplots(4, 2, figsize=(25, 25))\nax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8]\nfor i in range(len(continuous_features)):\n    sns.boxplot(x = TARGET_LABEL_COLUMN_NAME, y = continuous_features[i], data=df, ax=ax[i])\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3.0 Create a model\n\nNow we can create our machine learning model. You could use the insights / intuition gained from the data visualization steps above to what kind of model to create or which features to use. We will create a simple classification model."}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nimport pandas as pd\nimport json\n\nspark = SparkSession.builder.getOrCreate()\ndf_data = spark.createDataFrame(df)\ndf_data.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1 Split the data into training and test sets"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "spark_df = df_data\n(train_data, test_data) = spark_df.randomSplit([0.8, 0.2], 24)\n\nprint(\"Number of records for training: \" + str(train_data.count()))\nprint(\"Number of records for evaluation: \" + str(test_data.count()))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.2 Examine the Spark DataFrame Schema\nLook at the data types to determine requirements for feature engineering"}, {"metadata": {}, "cell_type": "code", "source": "spark_df.printSchema()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.3 Use StringIndexer to encode a string column of labels to a column of label indices\n\nWe are using the Pipeline package to build the development steps as pipeline. \nWe are using StringIndexer to handle categorical / string features from the dataset. StringIndexer encodes a string column of labels to a column of label indices\n\nWe then use VectorAssembler to asemble these features into a vector. Pipelines API requires that input variables are passed in  a vector"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler, SQLTransformer\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model\n\n#Create StringIndexer columns whose names are same as the categorical column with an appended _IX.\ncategorical_num_features = [x + '_IX' for x in categorical_features]\nsi_list = [StringIndexer(inputCol=nm_in, outputCol=nm_out) for nm_in, nm_out in zip(categorical_features, categorical_num_features)]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Encode our target label column (i.e Risk or No Risk). \n# Also, creates an label convert which performs an inverse map to get back a 'Risk' or 'No Risk' label from the encoded prediction.\nsi_label = StringIndexer(inputCol=TARGET_LABEL_COLUMN_NAME, outputCol=\"label\").fit(spark_df)\nlabel_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_label.labels)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Construct all encoded categorical features plus continuous features into a vector\nva_features = VectorAssembler(inputCols=categorical_num_features + continuous_features, outputCol=\"features\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.4 Create a pipeline, and fit a model using RandomForestClassifier \nAssemble all the stages into a pipeline. We don't expect a clean linear regression, so we'll use RandomForestClassifier to find the best decision tree for the data.\n\nThe pipeline will consist of: the feature string indexing step, the label string indexing Step, vector sssembly of all features step, random forest classifier, label converter step, and ending with a feature filter step.\n\n**Note: If you want filter features from model output, you could use the feature filter by replacing `*` with feature names to be retained in SQLTransformer statement.**"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "classifier = RandomForestClassifier(featuresCol=\"features\")\nfeature_filter = SQLTransformer(statement=\"SELECT * FROM __THIS__\")\npipeline = Pipeline(stages= si_list + [si_label, va_features, classifier, label_converter, feature_filter])\n\nmodel = pipeline.fit(train_data)", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "predictions = model.transform(test_data)\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName='areaUnderROC')\narea_under_curve = evaluatorDT.evaluate(predictions)\n\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName='areaUnderPR')\narea_under_PR = evaluatorDT.evaluate(predictions)\n#default evaluation is areaUnderROC\nprint(\"areaUnderROC = %g\" % area_under_curve, \"areaUnderPR = %g\" % area_under_PR)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.5 evaluate more metrics by exporting them into pandas and numpy"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics import classification_report\ny_pred = predictions.toPandas()['prediction']\ny_pred = ['Risk' if pred == 1.0 else 'No Risk' for pred in y_pred]\ny_test = test_data.toPandas()[TARGET_LABEL_COLUMN_NAME]\nprint(classification_report(y_test, y_pred, target_names=['Risk', 'No Risk']))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 4.0 Save the model and test data\n\nNow the model can be saved for future deployment. The model will be saved using the Watson Machine Learning client, to a deployment space."}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.1 Save the model to ICP4D local Watson Machine Learning\n\n<font color='red'>Replace the `username` and `password` values of `************` with your Cloud Pak for Data `username` and `password`. The value for `url` should match the `url` for your Cloud Pak for Data cluster, which you can get from the browser address bar (be sure to include the 'https://'.</font> The credentials should look something like this (these are example values, not the ones you will use):\n\n`\nwml_credentials = {\n                   \"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\",\n                   \"username\": \"cp4duser\",\n                   \"password\" : \"cp4dpass\",\n                   \"instance_id\": \"wml_local\",\n                   \"version\" : \"2.5.0\"\n                  }\n`\n#### NOTE: Make sure that there is no trailing forward slash `/` in the `url`"}, {"metadata": {}, "cell_type": "code", "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n\nwml_credentials = {\n                   \"url\": \"************\",\n                   \"username\": \"************\",\n                   \"password\" : \"************\",\n                   \"instance_id\": \"wml_local\",\n                   \"version\" : \"2.5.0\"\n                  }\n\nwml_client = WatsonMachineLearningAPIClient(wml_credentials)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_client.spaces.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Use the desired space as the `default_space`\n\nThe deployment space ID will be looked up based on the name specified above. If you do not receive a space GUID as an output to the next cell, do not proceed until you have created a deployment space.\n\n**<font color='red'><< UPDATE THE VARIABLE 'MODEL_NAME' TO A UNIQUE NAME>></font>**\n\n**<font color='red'><< UPDATE THE VARIABLE 'DEPLOYMENT_SPACE_NAME' TO THE NAME OF THE DEPLOYMENT SPACE CREATED PREVIOUSLY>></font>**"}, {"metadata": {}, "cell_type": "code", "source": "MODEL_NAME = \"************\"\nDEPLOYMENT_SPACE_NAME = \"************\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_client.spaces.list()\nall_spaces = wml_client.spaces.get_details()['resources']\nspace_id = None\nfor space in all_spaces:\n    if space['entity']['name'] == DEPLOYMENT_SPACE_NAME:\n        space_id = space[\"metadata\"][\"guid\"]\n        print(\"\\nDeployment Space GUID: \", space_id)\n\nif space_id is None:\n    print(\"WARNING: Your space does not exist. Create a deployment space before proceeding to the next cell.\")\n    #space_id = client.spaces.store(meta_props={client.spaces.ConfigurationMetaNames.NAME: space_name})[\"metadata\"][\"guid\"]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Now set the default space to the GUID for your deployment space. If this is successful, you will see a 'SUCCESS' message.\nwml_client.set.default_space(space_id)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### (Optional) Remove Existing Model and Deployment"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "#wml_models = wml_client.repository.get_model_details()\n#model_uid = None\n#for model_in in wml_models['resources']:\n#    if MODEL_NAME == model_in['entity']['name']:\n#        model_uid = model_in['metadata']['guid']\n#        print('Deleting model id', model_uid)\n#        wml_client.repository.delete(model_uid)\n#        break\n#\n# wml_client.repository.list_models()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Save the Model"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "metadata = {\n    wml_client.repository.ModelMetaNames.NAME: MODEL_NAME,\n    wml_client.repository.ModelMetaNames.TYPE: 'mllib_2.3',\n    wml_client.repository.ModelMetaNames.RUNTIME_UID: 'spark-mllib_2.3',\n}\n\npublished_model_details = wml_client.repository.store_model(model, metadata, training_data=df_data,  pipeline=pipeline)\nmodel_uid = wml_client.repository.get_model_uid(published_model_details)\n\nprint(json.dumps(published_model_details, indent=3))\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_client.repository.list_models()\nwml_client.deployments.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 5.0 Save Test Data\n\nWe will save the test data we used to evaluate the model to our project. Although not required, this will make it easier to run batch tests later on."}, {"metadata": {}, "cell_type": "code", "source": "write_score_CSV=test_data.toPandas().drop([TARGET_LABEL_COLUMN_NAME], axis=1)\nwrite_score_CSV.to_csv('/project_data/data_asset/GermanCreditRiskSparkMLBatchScore.csv', sep=',', index=False)\n#project.save_data('GermanCreditRiskSparkMLBatchScore.csv', write_score_CSV.to_csv())\n\nwrite_eval_CSV=test_data.toPandas()\nwrite_eval_CSV.to_csv('/project_data/data_asset/GermanCreditRiskSparkMLEval.csv', sep=',', index=False)\n#project.save_data('GermanCreditRiskSparkMLEval.csv', write_eval_CSV.to_csv())", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}